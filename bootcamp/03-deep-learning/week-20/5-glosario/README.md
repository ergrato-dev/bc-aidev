# üìñ Glosario - Semana 20

T√©rminos clave de TensorFlow y Keras.

---

## A

### Activation Function (Funci√≥n de Activaci√≥n)

Funci√≥n matem√°tica aplicada a la salida de cada neurona. Introduce no-linealidad permitiendo que la red aprenda patrones complejos. Ejemplos: ReLU, sigmoid, softmax.

```python
# En Keras
layers.Dense(64, activation='relu')
```

### Adam (Adaptive Moment Estimation)

Optimizador que combina las ventajas de AdaGrad y RMSprop. Calcula learning rates adaptativos para cada par√°metro. Es el optimizador por defecto m√°s recomendado.

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
```

---

## B

### Batch

Subconjunto de datos de entrenamiento procesados juntos antes de actualizar los pesos. Un batch_size de 32 significa que 32 muestras se procesan antes de cada actualizaci√≥n de gradiente.

### Batch Normalization

T√©cnica que normaliza las activaciones de cada capa durante el entrenamiento. Acelera el entrenamiento y act√∫a como regularizaci√≥n.

```python
layers.Dense(64)
layers.BatchNormalization()
layers.Activation('relu')
```

---

## C

### Callback

Objeto que ejecuta acciones en puntos espec√≠ficos del entrenamiento (inicio/fin de √©poca, batch, etc.). Permiten control avanzado del proceso de entrenamiento.

```python
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5),
    ModelCheckpoint('best_model.keras', save_best_only=True)
]
```

### Categorical Crossentropy

Funci√≥n de p√©rdida para clasificaci√≥n multiclase. Mide la diferencia entre la distribuci√≥n predicha y la real.

$$L = -\sum_{i} y_i \log(\hat{y}_i)$$

### Compile

M√©todo que configura el modelo para entrenamiento, especificando optimizer, loss y m√©tricas.

```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## D

### Dense Layer (Capa Densa)

Capa completamente conectada donde cada neurona recibe entrada de todas las neuronas de la capa anterior. Tambi√©n llamada "fully connected".

```python
layers.Dense(units=64, activation='relu')
```

### Dropout

T√©cnica de regularizaci√≥n que desactiva aleatoriamente un porcentaje de neuronas durante el entrenamiento para prevenir overfitting.

```python
layers.Dropout(0.3)  # 30% de neuronas desactivadas
```

---

## E

### EarlyStopping

Callback que detiene el entrenamiento cuando una m√©trica monitoreada deja de mejorar.

```python
EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)
```

### Epoch (√âpoca)

Una pasada completa por todo el conjunto de entrenamiento. 10 epochs significa que cada muestra fue vista 10 veces.

### Eager Execution

Modo de ejecuci√≥n de TensorFlow 2.x donde las operaciones se eval√∫an inmediatamente, sin necesidad de sesiones.

---

## F

### Fit

M√©todo que entrena el modelo con los datos proporcionados.

```python
history = model.fit(
    X_train, y_train,
    epochs=10,
    validation_data=(X_val, y_val)
)
```

### Flatten

Capa que convierte un tensor multidimensional a 1D. T√≠picamente usado antes de capas Dense.

```python
layers.Flatten(input_shape=(28, 28, 1))  # (28, 28, 1) ‚Üí (784,)
```

---

## G

### Glorot Initialization (Xavier)

M√©todo de inicializaci√≥n de pesos que mantiene la varianza de las activaciones. Recomendado para tanh y sigmoid.

```python
kernel_initializer='glorot_uniform'
```

### GradientTape

Contexto de TensorFlow para calcular gradientes autom√°ticamente durante operaciones.

```python
with tf.GradientTape() as tape:
    y = model(x)
    loss = loss_fn(y, y_true)
gradients = tape.gradient(loss, model.trainable_variables)
```

---

## H

### He Initialization

M√©todo de inicializaci√≥n de pesos optimizado para funciones de activaci√≥n ReLU.

```python
kernel_initializer='he_normal'
```

### History

Objeto retornado por `model.fit()` que contiene las m√©tricas de entrenamiento por √©poca.

```python
history.history['loss']      # Lista de loss por √©poca
history.history['val_accuracy']  # Accuracy de validaci√≥n
```

---

## I

### Input Shape

Forma de los datos de entrada que el modelo espera. Se especifica en la primera capa.

```python
layers.Dense(64, input_shape=(784,))  # 784 features de entrada
```

---

## K

### Keras

API de alto nivel para construir y entrenar modelos de deep learning. Integrada en TensorFlow desde la versi√≥n 2.0.

### Kernel

En el contexto de capas Dense, los "kernels" son los pesos (weights) de las conexiones.

```python
kernel_initializer='he_normal'
kernel_regularizer=tf.keras.regularizers.l2(0.01)
```

---

## L

### Learning Rate

Hiperpar√°metro que controla cu√°nto se ajustan los pesos en cada paso de optimizaci√≥n. Valores t√≠picos: 0.001, 0.01.

### Loss Function

Funci√≥n que mide qu√© tan lejos est√°n las predicciones del modelo de los valores reales. El objetivo del entrenamiento es minimizarla.

---

## M

### Metrics

Medidas de rendimiento del modelo que se calculan durante entrenamiento y evaluaci√≥n, pero no afectan el proceso de optimizaci√≥n.

```python
metrics=['accuracy', 'precision', 'recall']
```

### ModelCheckpoint

Callback que guarda el modelo durante el entrenamiento.

```python
ModelCheckpoint(
    'best_model.keras',
    monitor='val_accuracy',
    save_best_only=True
)
```

---

## O

### Optimizer

Algoritmo que actualiza los pesos del modelo usando los gradientes calculados. Ejemplos: Adam, SGD, RMSprop.

### Overfitting

Cuando el modelo aprende demasiado bien los datos de entrenamiento pero no generaliza a datos nuevos. Se detecta cuando train_accuracy >> val_accuracy.

---

## P

### Predict

M√©todo que genera predicciones para datos de entrada.

```python
predictions = model.predict(X_test)
```

---

## R

### ReduceLROnPlateau

Callback que reduce el learning rate cuando una m√©trica deja de mejorar.

```python
ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3
)
```

### ReLU (Rectified Linear Unit)

Funci√≥n de activaci√≥n que retorna max(0, x). La m√°s usada en capas ocultas.

$$f(x) = \max(0, x)$$

---

## S

### Sequential

API de Keras para crear modelos como una pila lineal de capas.

```python
model = Sequential([
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])
```

### Softmax

Funci√≥n de activaci√≥n que convierte logits en probabilidades (suman 1). Usada en la capa de salida para clasificaci√≥n multiclase.

$$\sigma(z)_i = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$$

### Sparse Categorical Crossentropy

Versi√≥n de categorical crossentropy para etiquetas como enteros (no one-hot encoded).

```python
loss='sparse_categorical_crossentropy'
```

---

## T

### TensorBoard

Herramienta de visualizaci√≥n de TensorFlow para monitorear el entrenamiento.

```bash
tensorboard --logdir logs/fit
```

### TensorFlow

Framework de c√≥digo abierto de Google para computaci√≥n num√©rica y machine learning.

### Tensor

Array n-dimensional, estructura de datos fundamental en TensorFlow.

```python
tensor = tf.constant([[1, 2], [3, 4]])
```

### Trainable

Propiedad que indica si los pesos de una capa se actualizan durante el entrenamiento.

---

## V

### Validation Split

Porcentaje de datos de entrenamiento reservados para validaci√≥n.

```python
model.fit(X, y, validation_split=0.2)  # 20% para validaci√≥n
```

### Variable

Tensor mutable en TensorFlow, usado para almacenar pesos entrenables.

```python
weights = tf.Variable(tf.random.normal([3, 2]))
```

---

_Glosario Semana 20 | TensorFlow y Keras | Bootcamp IA: Zero to Hero_
