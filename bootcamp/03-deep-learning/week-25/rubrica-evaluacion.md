# ğŸ“‹ RÃºbrica de EvaluaciÃ³n - Semana 25

## ğŸ¯ Transformers: Attention is All You Need

---

## ğŸ“Š DistribuciÃ³n de PuntuaciÃ³n

| Componente | Peso | DescripciÃ³n |
|------------|------|-------------|
| ğŸ§  Conocimiento | 30% | ComprensiÃ³n teÃ³rica de attention y transformers |
| ğŸ’ª DesempeÃ±o | 35% | Ejercicios prÃ¡cticos completados |
| ğŸ“¦ Producto | 35% | Proyecto clasificador de texto |

---

## ğŸ§  Conocimiento (30%)

### Conceptos Evaluados

| Concepto | Puntos | Criterio |
|----------|--------|----------|
| Mecanismo de AtenciÃ³n | 8 | Explica Query, Key, Value |
| Self-Attention | 8 | Comprende cÃ¡lculo de attention scores |
| Multi-Head Attention | 7 | Entiende propÃ³sito de mÃºltiples heads |
| Positional Encoding | 7 | Explica necesidad y funcionamiento |

### Niveles de Logro

| Nivel | Rango | DescripciÃ³n |
|-------|-------|-------------|
| Excelente | 90-100% | Explica transformer completo con fÃ³rmulas |
| Bueno | 75-89% | Comprende attention y arquitectura |
| Suficiente | 60-74% | Entiende conceptos bÃ¡sicos |
| Insuficiente | <60% | No comprende el mecanismo de atenciÃ³n |

---

## ğŸ’ª DesempeÃ±o (35%)

### Ejercicios PrÃ¡cticos

| Ejercicio | Puntos | Criterios |
|-----------|--------|-----------|
| Attention BÃ¡sico | 12 | Implementa scaled dot-product attention |
| Multi-Head Attention | 12 | MÃºltiples heads con concatenaciÃ³n |
| Transformer Encoder | 11 | Encoder layer completo funcionando |

### Criterios por Ejercicio

#### Ejercicio 1: Attention BÃ¡sico
- [ ] Calcula attention scores correctamente (4 pts)
- [ ] Aplica softmax (4 pts)
- [ ] Multiplica por Values (4 pts)

#### Ejercicio 2: Multi-Head Attention
- [ ] Proyecta Q, K, V por cada head (4 pts)
- [ ] Ejecuta attention en paralelo (4 pts)
- [ ] Concatena y proyecta salida (4 pts)

#### Ejercicio 3: Transformer Encoder
- [ ] Self-attention + Add & Norm (4 pts)
- [ ] Feed-forward network (4 pts)
- [ ] Residual connections (3 pts)

---

## ğŸ“¦ Producto (35%)

### Proyecto: Clasificador de Texto con Transformer

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| Funcionalidad | 12 | Modelo entrena y predice |
| Accuracy | 10 | > 85% en test set |
| Arquitectura | 8 | Transformer encoder bien estructurado |
| CÃ³digo | 5 | Limpio y documentado |

### Niveles de Logro del Proyecto

| Nivel | Accuracy | Puntos |
|-------|----------|--------|
| Excelente | > 90% | 35/35 |
| Bueno | 85-90% | 30/35 |
| Suficiente | 75-85% | 25/35 |
| Insuficiente | < 75% | 15/35 |

---

## ğŸ“ˆ Escala de CalificaciÃ³n

| CalificaciÃ³n | Rango | Significado |
|--------------|-------|-------------|
| A | 90-100% | Sobresaliente |
| B | 80-89% | Notable |
| C | 70-79% | Aprobado |
| D | 60-69% | Suficiente |
| F | <60% | No aprobado |

---

## âœ… Checklist de AutoevaluaciÃ³n

### Conocimiento
- [ ] Puedo explicar Q, K, V y su propÃ³sito
- [ ] Entiendo por quÃ© se escala por âˆšd_k
- [ ] SÃ© quÃ© es multi-head attention y por quÃ© es Ãºtil
- [ ] Comprendo el rol del positional encoding

### DesempeÃ±o
- [ ] ImplementÃ© attention desde cero
- [ ] CreÃ© multi-head attention funcional
- [ ] ConstruÃ­ un transformer encoder layer

### Producto
- [ ] Mi clasificador alcanza accuracy > 85%
- [ ] El cÃ³digo estÃ¡ documentado
- [ ] Puedo explicar cada componente

---

## ğŸ¯ FÃ³rmulas Clave

**Scaled Dot-Product Attention:**
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Multi-Head Attention:**
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

---

_RÃºbrica Semana 25 | MÃ³dulo Deep Learning_
