# üìù Proyecto: Clasificador de Texto con Transformer

## üéØ Objetivo

Construir un **clasificador de sentimientos** usando un Transformer Encoder desde cero. El modelo debe alcanzar **> 85% de accuracy** en el dataset de prueba.

---

## üìã Descripci√≥n

En este proyecto integrador aplicar√°s todos los conceptos de la semana:

1. **Scaled Dot-Product Attention**
2. **Multi-Head Attention**
3. **Positional Encoding**
4. **Transformer Encoder**
5. **Classification Head**

### Dataset

Usaremos un dataset de rese√±as de pel√≠culas (sentimiento positivo/negativo).

---

## üèóÔ∏è Arquitectura

```
Input Tokens
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Embedding  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Positional  ‚îÇ
‚îÇ  Encoding   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Transformer ‚îÇ √ó N layers
‚îÇ   Encoder   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   [CLS]     ‚îÇ  ‚Üê Tomar primer token
‚îÇ   Pooling   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Linear    ‚îÇ
‚îÇ  Classifier ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚ñº
  Prediction
(Positive/Negative)
```

---

## üìù Requisitos

### Modelo

- [ ] Transformer Encoder con al menos 2 capas
- [ ] Multi-Head Attention con 4+ heads
- [ ] Positional Encoding (sinusoidal o aprendido)
- [ ] Classification head con dropout

### Entrenamiento

- [ ] Funci√≥n de p√©rdida: CrossEntropyLoss
- [ ] Optimizador: Adam con learning rate scheduling
- [ ] Early stopping para evitar overfitting

### M√©tricas

- [ ] **Accuracy > 85%** en test set
- [ ] Reportar precision, recall, F1-score
- [ ] Visualizar curvas de entrenamiento

---

## üìÅ Estructura

```
clasificador-texto-transformer/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ starter/
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # C√≥digo inicial con TODOs
‚îî‚îÄ‚îÄ solution/
    ‚îî‚îÄ‚îÄ main.py          # Soluci√≥n completa
```

---

## üöÄ Instrucciones

### 1. Preparar Datos

```python
# Cargar dataset (usaremos datos sint√©ticos o IMDB)
train_loader, test_loader, vocab = prepare_data()
```

### 2. Implementar Modelo

Completa los TODOs en `starter/main.py`:

1. `PositionalEncoding`: A√±adir informaci√≥n de posici√≥n
2. `TransformerClassifier`: Encoder + Classification head
3. `train_epoch`: Loop de entrenamiento
4. `evaluate`: Evaluaci√≥n en test set

### 3. Entrenar

```bash
python starter/main.py
```

### 4. Evaluar

El script mostrar√°:
- Accuracy por √©poca
- M√©tricas finales
- Gr√°ficas de p√©rdida

---

## ‚úÖ Criterios de Evaluaci√≥n

| Criterio | Puntos |
|----------|--------|
| Modelo compila sin errores | 20% |
| Arquitectura correcta | 25% |
| Entrenamiento funciona | 25% |
| Accuracy > 85% | 20% |
| C√≥digo documentado | 10% |

---

## üí° Tips

1. **Empieza simple**: 2 capas, 4 heads, d_model=128
2. **Normaliza**: Usa Layer Normalization
3. **Regulariza**: Dropout 0.1-0.3
4. **Learning rate**: Empieza con 1e-4
5. **Batch size**: 32-64 funciona bien

---

## üîó Recursos

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [BERT for Classification](https://arxiv.org/abs/1810.04805)
- [torchtext datasets](https://pytorch.org/text/stable/datasets.html)

---

## ‚è±Ô∏è Tiempo Estimado

2 horas
