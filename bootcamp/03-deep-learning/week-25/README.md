# ðŸ¤– Semana 25: Transformers - Attention is All You Need

## ðŸŽ¯ Objetivos de Aprendizaje

Al finalizar esta semana, serÃ¡s capaz de:

- âœ… Comprender el mecanismo de atenciÃ³n (attention)
- âœ… Entender la arquitectura Transformer completa
- âœ… Implementar Self-Attention desde cero
- âœ… Aplicar Positional Encoding
- âœ… Usar Transformers pre-entrenados

---

## ðŸ“š Requisitos Previos

- Semana 24: RNNs completada
- Ãlgebra lineal (multiplicaciÃ³n de matrices)
- ComprensiÃ³n de secuencias

---

## ðŸ—‚ï¸ Estructura de la Semana

```
week-25/
â”œâ”€â”€ README.md
â”œâ”€â”€ rubrica-evaluacion.md
â”œâ”€â”€ 0-assets/
â”‚   â”œâ”€â”€ 01-attention-mechanism.svg
â”‚   â”œâ”€â”€ 02-transformer-architecture.svg
â”‚   â”œâ”€â”€ 03-multi-head-attention.svg
â”‚   â””â”€â”€ 04-positional-encoding.svg
â”œâ”€â”€ 1-teoria/
â”‚   â”œâ”€â”€ 01-atencion-mecanismo.md
â”‚   â”œâ”€â”€ 02-self-attention.md
â”‚   â”œâ”€â”€ 03-arquitectura-transformer.md
â”‚   â””â”€â”€ 04-positional-encoding.md
â”œâ”€â”€ 2-practicas/
â”‚   â”œâ”€â”€ ejercicio-01-attention-basico/
â”‚   â”œâ”€â”€ ejercicio-02-multi-head-attention/
â”‚   â””â”€â”€ ejercicio-03-transformer-encoder/
â”œâ”€â”€ 3-proyecto/
â”‚   â””â”€â”€ clasificador-texto-transformer/
â”œâ”€â”€ 4-recursos/
â”‚   â””â”€â”€ README.md
â””â”€â”€ 5-glosario/
    â””â”€â”€ README.md
```

---

## ðŸ“ Contenidos

### ðŸ“– TeorÃ­a (1.5 horas)

| # | Tema | Archivo | DuraciÃ³n |
|---|------|---------|----------|
| 1 | Mecanismo de AtenciÃ³n | [01-atencion-mecanismo.md](1-teoria/01-atencion-mecanismo.md) | 25 min |
| 2 | Self-Attention | [02-self-attention.md](1-teoria/02-self-attention.md) | 25 min |
| 3 | Arquitectura Transformer | [03-arquitectura-transformer.md](1-teoria/03-arquitectura-transformer.md) | 25 min |
| 4 | Positional Encoding | [04-positional-encoding.md](1-teoria/04-positional-encoding.md) | 15 min |

### ðŸ’» PrÃ¡cticas (2.5 horas)

| # | Ejercicio | Carpeta | DuraciÃ³n |
|---|-----------|---------|----------|
| 1 | Attention BÃ¡sico | [ejercicio-01-attention-basico/](2-practicas/ejercicio-01-attention-basico/) | 45 min |
| 2 | Multi-Head Attention | [ejercicio-02-multi-head-attention/](2-practicas/ejercicio-02-multi-head-attention/) | 50 min |
| 3 | Transformer Encoder | [ejercicio-03-transformer-encoder/](2-practicas/ejercicio-03-transformer-encoder/) | 55 min |

### ðŸ“¦ Proyecto (2 horas)

| Proyecto | DescripciÃ³n | Carpeta |
|----------|-------------|---------|
| Clasificador de Texto | ClasificaciÃ³n con Transformer Encoder | [clasificador-texto-transformer/](3-proyecto/clasificador-texto-transformer/) |

---

## â±ï¸ DistribuciÃ³n del Tiempo

```
Total: 6 horas

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ“– TeorÃ­a      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  1.5h (25%)  â”‚
â”‚  ðŸ’» PrÃ¡cticas   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.5h (42%)  â”‚
â”‚  ðŸ“¦ Proyecto    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.0h (33%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Œ Entregables

1. **Ejercicios completados** (2-practicas/)
   - [ ] ejercicio-01: Attention bÃ¡sico funcionando
   - [ ] ejercicio-02: Multi-Head Attention implementado
   - [ ] ejercicio-03: Transformer Encoder operativo

2. **Proyecto semanal** (3-proyecto/)
   - [ ] Clasificador de texto con Transformer
   - [ ] Accuracy > 85% en test set
   - [ ] CÃ³digo documentado

---

## ðŸ”— NavegaciÃ³n

| â¬…ï¸ Anterior | ðŸ  Inicio | Siguiente âž¡ï¸ |
|-------------|-----------|---------------|
| [Semana 24](../week-24/README.md) | [MÃ³dulo 3](../README.md) | [Semana 26](../week-26/README.md) |

---

## ðŸ’¡ Contexto HistÃ³rico

> ðŸŽ¯ **"Attention Is All You Need"** (Vaswani et al., 2017) revolucionÃ³ el campo del NLP y posteriormente toda la IA, dando origen a BERT, GPT, y los LLMs actuales.

---

_Semana 25 de 36 | MÃ³dulo: Deep Learning | Bootcamp IA: Zero to Hero_
