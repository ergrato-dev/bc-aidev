# ğŸ—ï¸ Ejercicio 03: Transformer Encoder

## ğŸ¯ Objetivo

Construir un **Transformer Encoder** completo combinando Multi-Head Attention, Feed-Forward Network, y Layer Normalization.

---

## ğŸ“‹ Conceptos Clave

### Arquitectura de una Encoder Layer

```
Input
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â–¼                  â”‚
Multi-Head Attention â”‚
  â”‚                  â”‚
  â–¼                  â”‚
  + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Residual Connection)
  â”‚
  â–¼
Layer Norm
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â–¼                  â”‚
Feed-Forward         â”‚
  â”‚                  â”‚
  â–¼                  â”‚
  + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Residual Connection)
  â”‚
  â–¼
Layer Norm
  â”‚
Output
```

### Componentes

1. **Multi-Head Attention**: Captura relaciones entre tokens
2. **Feed-Forward Network**: MLP con expansiÃ³n (4x tÃ­picamente)
3. **Add & Norm**: ConexiÃ³n residual + Layer Normalization

---

## ğŸ“ Instrucciones

### Paso 1: Feed-Forward Network

Red neuronal simple: Linear â†’ ReLU â†’ Dropout â†’ Linear

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
```

### Paso 2: Encoder Layer

Combina atenciÃ³n + feed-forward con residuales:

```python
# Attention con residual
x = x + self.dropout(self.attention(x, x, x))
x = self.norm1(x)

# FFN con residual
x = x + self.dropout(self.ffn(x))
x = self.norm2(x)
```

### Paso 3: Positional Encoding

Los Transformers no tienen nociÃ³n de orden. AÃ±adimos posiciÃ³n:

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### Paso 4: Transformer Encoder Completo

Apila N encoder layers con embedding + positional encoding.

### Paso 5: Probar con Secuencias

Procesa secuencias de tokens y observa las representaciones.

---

## âœ… Criterios de Ã‰xito

- [ ] FeedForward expande y contrae correctamente
- [ ] EncoderLayer aplica residuales y normalizaciÃ³n
- [ ] PositionalEncoding aÃ±ade informaciÃ³n de posiciÃ³n
- [ ] TransformerEncoder procesa secuencias correctamente

---

## ğŸ”— Recursos

- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Residual Connections](https://arxiv.org/abs/1512.03385)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)

---

## â±ï¸ Tiempo Estimado

60 minutos
