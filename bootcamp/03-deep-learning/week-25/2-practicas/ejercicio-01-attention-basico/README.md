# üëÅÔ∏è Ejercicio 01: Atenci√≥n B√°sica

## üéØ Objetivo

Implementar el mecanismo de **Scaled Dot-Product Attention** desde cero para comprender c√≥mo Query, Key y Value interact√∫an.

---

## üìã Conceptos Clave

### Scaled Dot-Product Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Donde:
- **Q** (Query): Lo que estamos buscando
- **K** (Key): √çndices contra los que comparamos
- **V** (Value): Informaci√≥n a recuperar
- **d_k**: Dimensi√≥n de las keys (para estabilidad num√©rica)

---

## üìù Instrucciones

### Paso 1: Importaciones y Setup

Abre `starter/main.py` y descomenta las importaciones b√°sicas:

```python
import torch
import torch.nn.functional as F
```

### Paso 2: Producto Punto Q¬∑K^T

El primer paso es calcular la similitud entre queries y keys:

```python
# scores tiene forma (batch, seq_len_q, seq_len_k)
scores = torch.matmul(Q, K.transpose(-2, -1))
```

Descomenta la secci√≥n correspondiente en `starter/main.py`.

### Paso 3: Escalado por ‚àöd_k

Sin escalar, los valores pueden ser muy grandes y softmax satura:

```python
d_k = K.size(-1)
scaled_scores = scores / (d_k ** 0.5)
```

### Paso 4: Softmax

Convertimos scores a probabilidades (suman 1 por fila):

```python
attention_weights = F.softmax(scaled_scores, dim=-1)
```

### Paso 5: Multiplicar por Values

Finalmente, usamos los pesos para combinar los values:

```python
output = torch.matmul(attention_weights, V)
```

### Paso 6: Funci√≥n Completa

Une todo en una funci√≥n `scaled_dot_product_attention`.

### Paso 7: Visualizaci√≥n

Visualiza los pesos de atenci√≥n con matplotlib para entender qu√© tokens "atienden" a cu√°les.

---

## ‚úÖ Criterios de √âxito

- [ ] La funci√≥n `scaled_dot_product_attention` retorna output y weights
- [ ] El output tiene la misma forma que V
- [ ] Los weights suman 1 en la √∫ltima dimensi√≥n
- [ ] Puedes visualizar el patr√≥n de atenci√≥n

---

## üîó Recursos

- [Attention Is All You Need (Paper)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

---

## ‚è±Ô∏è Tiempo Estimado

30 minutos
