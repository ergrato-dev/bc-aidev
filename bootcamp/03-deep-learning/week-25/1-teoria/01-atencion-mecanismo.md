# üëÅÔ∏è Mecanismo de Atenci√≥n

## üéØ Objetivos

- Comprender el problema que resuelve la atenci√≥n
- Entender Query, Key, Value
- Implementar atenci√≥n b√°sica

---

![Mecanismo de Atenci√≥n](../0-assets/01-attention-mechanism.svg)

---

## 1. El Problema de las RNNs

### Cuello de Botella

En arquitecturas seq2seq tradicionales:

```
Encoder: "El gato negro" ‚Üí [h_final]
                              ‚Üì
Decoder: [h_final] ‚Üí "The black cat"
```

**Problema**: Toda la informaci√≥n debe comprimirse en un √∫nico vector.

### Limitaciones

- Secuencias largas pierden informaci√≥n inicial
- No hay acceso directo a tokens espec√≠ficos
- Dificultad con dependencias de largo alcance

---

## 2. La Idea de Atenci√≥n

### Intuici√≥n

En lugar de un √∫nico vector, el decoder puede "mirar" todos los estados del encoder:

```
Encoder states: [h1, h2, h3, h4]
                  ‚Üì   ‚Üì   ‚Üì   ‚Üì
Attention:      [0.1, 0.7, 0.1, 0.1]  ‚Üê "¬øA cu√°l presto atenci√≥n?"
                  ‚Üì   ‚Üì   ‚Üì   ‚Üì
Context:        Œ£(attention √ó states)
```

### Analog√≠a

Como cuando lees un libro y buscas informaci√≥n espec√≠fica:
- **Query**: "¬øQu√© busco?" (lo que necesito saber)
- **Keys**: "¬øQu√© hay disponible?" (√≠ndice del libro)
- **Values**: "¬øQu√© contiene?" (el contenido real)

---

## 3. Query, Key, Value

### Definiciones

| Componente | S√≠mbolo | Descripci√≥n |
|------------|---------|-------------|
| Query | Q | Lo que estoy buscando |
| Key | K | Contra qu√© comparo |
| Value | V | Lo que obtengo si hay match |

### Proceso

1. **Comparar** Query con todas las Keys ‚Üí scores
2. **Normalizar** scores con softmax ‚Üí weights
3. **Combinar** Values ponderados ‚Üí output

---

## 4. Scaled Dot-Product Attention

### F√≥rmula

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### Por qu√© escalar por ‚àöd_k

Sin escalar, para dimensiones altas los productos punto son muy grandes, causando:
- Gradientes de softmax cercanos a 0
- Saturaci√≥n y problemas de entrenamiento

### Implementaci√≥n

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    """
    Scaled Dot-Product Attention.
    
    Args:
        Q: (batch, seq_q, d_k)
        K: (batch, seq_k, d_k)
        V: (batch, seq_k, d_v)
    
    Returns:
        output: (batch, seq_q, d_v)
        attention_weights: (batch, seq_q, seq_k)
    """
    d_k = Q.size(-1)
    
    # 1. Calcular scores: QK^T
    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_q, seq_k)
    
    # 2. Escalar
    scores = scores / (d_k ** 0.5)
    
    # 3. Softmax para obtener pesos
    attention_weights = F.softmax(scores, dim=-1)
    
    # 4. Multiplicar por Values
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

---

## 5. Ejemplo Paso a Paso

```python
# Dimensiones peque√±as para visualizar
d_k = 4
seq_len = 3

# Crear Q, K, V (batch=1)
Q = torch.randn(1, seq_len, d_k)
K = torch.randn(1, seq_len, d_k)
V = torch.randn(1, seq_len, d_k)

print('Q shape:', Q.shape)
print('K shape:', K.shape)
print('V shape:', V.shape)

# Paso 1: QK^T
scores = torch.matmul(Q, K.transpose(-2, -1))
print('\nScores (QK^T):')
print(scores)

# Paso 2: Escalar
scaled_scores = scores / (d_k ** 0.5)
print('\nScaled scores:')
print(scaled_scores)

# Paso 3: Softmax
weights = F.softmax(scaled_scores, dim=-1)
print('\nAttention weights:')
print(weights)
print('Sum per row:', weights.sum(dim=-1))  # Debe ser 1.0

# Paso 4: Weighted sum de Values
output = torch.matmul(weights, V)
print('\nOutput shape:', output.shape)
```

---

## 6. Visualizando la Atenci√≥n

```python
import matplotlib.pyplot as plt

def plot_attention(weights, src_tokens, tgt_tokens):
    """Visualizar matriz de atenci√≥n."""
    plt.figure(figsize=(8, 6))
    plt.imshow(weights, cmap='Blues')
    plt.colorbar()
    plt.xticks(range(len(src_tokens)), src_tokens)
    plt.yticks(range(len(tgt_tokens)), tgt_tokens)
    plt.xlabel('Source')
    plt.ylabel('Target')
    plt.title('Attention Weights')
    plt.show()

# Ejemplo
src = ['El', 'gato', 'negro']
tgt = ['The', 'black', 'cat']
# weights de shape (3, 3)
```

---

## 7. Tipos de Atenci√≥n

### Por Relaci√≥n Q-K-V

| Tipo | Query | Key/Value | Uso |
|------|-------|-----------|-----|
| Self-Attention | mismo input | mismo input | Encoder |
| Cross-Attention | decoder | encoder | Decoder |
| Masked Self-Attention | mismo input | mismo (causal) | Decoder autoregresivo |

### Por C√°lculo

| Tipo | F√≥rmula | Complejidad |
|------|---------|-------------|
| Dot-Product | QK^T | O(n¬≤d) |
| Additive | v^T¬∑tanh(W_q¬∑Q + W_k¬∑K) | O(n¬≤d) |
| Multiplicative | Q¬∑W¬∑K^T | O(n¬≤d¬≤) |

---

## ‚úÖ Checklist de Comprensi√≥n

- [ ] Entiendo el problema del cuello de botella en seq2seq
- [ ] Puedo explicar Query, Key, Value
- [ ] S√© por qu√© se escala por ‚àöd_k
- [ ] Implement√© attention desde cero

---

## üìö Recursos

- [Attention Is All You Need (Paper)](https://arxiv.org/abs/1706.03762)
- [Visualizing Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
