# üìñ Glosario - Semana 25: Transformers

T√©rminos clave de la arquitectura Transformer ordenados alfab√©ticamente.

---

## A

### Add & Norm
Combinaci√≥n de **conexi√≥n residual** (Add) y **Layer Normalization** (Norm). Estabiliza el entrenamiento y permite gradientes m√°s fuertes.

```python
output = LayerNorm(x + Sublayer(x))
```

### Attention
Mecanismo que permite a un modelo enfocarse en partes relevantes de la entrada. Calcula pesos de importancia entre elementos.

### Attention Head
Una instancia del mecanismo de atenci√≥n con sus propias matrices de proyecci√≥n (W_Q, W_K, W_V).

### Attention Weights
Pesos que indican cu√°nta "atenci√≥n" presta cada posici√≥n a las dem√°s. Resultado de softmax sobre los scores.

---

## B

### BERT
**Bidirectional Encoder Representations from Transformers**. Modelo que usa solo el encoder del Transformer con pre-entrenamiento bidireccional.

---

## C

### Causal Mask
M√°scara triangular que impide que posiciones futuras sean visibles. Esencial para decoders autoregresivos.

```python
mask = torch.triu(torch.ones(n, n), diagonal=1) == 0
```

### Cross-Attention
Atenci√≥n donde Q viene de una secuencia y K, V de otra. Usado en decoders para atender al encoder.

---

## D

### Decoder
Parte del Transformer que genera la salida. Usa masked self-attention + cross-attention + feed-forward.

### d_k (d_key)
Dimensi√≥n de las Keys (y Queries). T√≠picamente `d_model / num_heads`.

### d_model
Dimensi√≥n del modelo (tama√±o de embeddings y representaciones). Valores comunes: 256, 512, 768, 1024.

### d_v (d_value)
Dimensi√≥n de los Values. Generalmente igual a d_k.

---

## E

### Encoder
Parte del Transformer que procesa la entrada. Stack de capas con self-attention + feed-forward.

### Encoder Layer
Una capa del encoder: Multi-Head Attention ‚Üí Add & Norm ‚Üí Feed-Forward ‚Üí Add & Norm.

---

## F

### Feed-Forward Network (FFN)
Red neuronal posici√≥n-wise: dos capas lineales con activaci√≥n ReLU/GELU.

$$\text{FFN}(x) = \text{Linear}_2(\text{ReLU}(\text{Linear}_1(x)))$$

T√≠picamente d_ff = 4 √ó d_model.

---

## G

### GPT
**Generative Pre-trained Transformer**. Modelo decoder-only para generaci√≥n de texto.

---

## K

### Key (K)
En atenci√≥n, representa "contra qu√© comparamos". Se usa para calcular similitud con Query.

---

## L

### Layer Normalization
Normalizaci√≥n que opera sobre la dimensi√≥n de features (no batch). M√°s estable que Batch Norm para secuencias.

```python
LayerNorm(x) = Œ≥ * (x - Œº) / œÉ + Œ≤
```

---

## M

### Masked Self-Attention
Self-attention con m√°scara causal. Cada posici√≥n solo puede atender a posiciones anteriores.

### Multi-Head Attention
M√∫ltiples heads de atenci√≥n en paralelo, concatenados y proyectados. Captura diferentes tipos de relaciones.

$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O$$

---

## P

### Padding Mask
M√°scara que indica qu√© posiciones son padding y deben ignorarse en atenci√≥n.

### Positional Encoding
Informaci√≥n de posici√≥n a√±adida a embeddings. El Transformer original usa funciones sinusoidales:

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

---

## Q

### Query (Q)
En atenci√≥n, representa "qu√© estamos buscando". Define qu√© informaci√≥n queremos extraer.

---

## R

### Residual Connection
Conexi√≥n que suma la entrada a la salida de una capa: `output = x + layer(x)`. Facilita el flujo de gradientes.

### RoPE
**Rotary Position Embedding**. Encoding posicional que codifica posici√≥n relativa rotando vectores.

---

## S

### Scaled Dot-Product Attention
Mecanismo de atenci√≥n base del Transformer:

$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### Self-Attention
Atenci√≥n donde Q, K, V provienen de la misma secuencia. Cada token atiende a todos los dem√°s.

### Softmax
Funci√≥n que convierte scores a probabilidades (valores positivos que suman 1).

---

## T

### Transformer
Arquitectura de red neuronal basada enteramente en atenci√≥n, sin recurrencia ni convoluciones. Introducida en "Attention Is All You Need" (2017).

---

## V

### Value (V)
En atenci√≥n, representa "qu√© informaci√≥n extraer". Los pesos de atenci√≥n ponderan los values.

---

## F√≥rmulas Clave

| Concepto | F√≥rmula |
|----------|---------|
| Attention | $\text{softmax}(QK^T/\sqrt{d_k})V$ |
| Multi-Head | $\text{Concat}(\text{head}_1,...,\text{head}_h)W^O$ |
| PE (sin) | $\sin(pos/10000^{2i/d})$ |
| PE (cos) | $\cos(pos/10000^{2i/d})$ |
| FFN | $W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2$ |

---

_Semana 25 | M√≥dulo: Deep Learning | Bootcamp IA: Zero to Hero_
