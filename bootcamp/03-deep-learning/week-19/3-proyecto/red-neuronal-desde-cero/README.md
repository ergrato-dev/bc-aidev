# ğŸ§  Proyecto: Red Neuronal desde Cero

## ğŸ¯ Objetivo

Implementar una red neuronal multicapa (MLP) completa usando **solo NumPy**, incluyendo forward propagation, backpropagation y entrenamiento.

---

## ğŸ“‹ DescripciÃ³n

ConstruirÃ¡s una red neuronal que pueda:

1. Resolver el problema XOR (imposible para un perceptrÃ³n simple)
2. Clasificar datos sintÃ©ticos no linealmente separables
3. Aprender mediante gradient descent con backpropagation

**âš ï¸ IMPORTANTE**: No uses TensorFlow, PyTorch, Keras ni ningÃºn framework de Deep Learning. Solo NumPy.

---

## ğŸ—ï¸ Arquitectura a Implementar

```
Input Layer    Hidden Layer    Output Layer
    (2)            (4)             (1)
    
   [xâ‚]â”€â”€â”     â”Œâ”€â”€[hâ‚]â”€â”€â”
         â”œâ”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€[Å·]
   [xâ‚‚]â”€â”€â”˜     â””â”€â”€[hâ‚„]â”€â”€â”˜
```

---

## ğŸ“ Estructura

```
red-neuronal-desde-cero/
â”œâ”€â”€ README.md           # Este archivo
â”œâ”€â”€ starter/
â”‚   â””â”€â”€ main.py         # CÃ³digo inicial con TODOs
â””â”€â”€ solution/
    â””â”€â”€ main.py         # SoluciÃ³n completa
```

---

## âœ… Requisitos

### Funcionalidades

- [ ] InicializaciÃ³n de pesos (He initialization)
- [ ] Forward propagation con cache
- [ ] FunciÃ³n de pÃ©rdida (Binary Cross-Entropy)
- [ ] Backward propagation (gradientes)
- [ ] ActualizaciÃ³n de pesos (Gradient Descent)
- [ ] Loop de entrenamiento completo
- [ ] VisualizaciÃ³n de pÃ©rdida y frontera de decisiÃ³n

### MÃ©tricas MÃ­nimas

- Accuracy en XOR: **100%** (es posible)
- Accuracy en datos sintÃ©ticos: **> 90%**

---

## ğŸ”§ Funciones a Implementar

```python
# En starter/main.py encontrarÃ¡s:

def sigmoid(z): ...
def sigmoid_derivative(z): ...
def relu(z): ...
def relu_derivative(z): ...

def initialize_parameters(layer_dims): ...
def forward_propagation(X, parameters): ...
def compute_loss(Y_hat, Y): ...
def backward_propagation(Y_hat, Y, cache, parameters): ...
def update_parameters(parameters, gradients, learning_rate): ...
def train(X, Y, layer_dims, epochs, learning_rate): ...
```

---

## ğŸ“Š Dataset

### XOR (Principal)

```python
X = np.array([[0, 0, 1, 1],
              [0, 1, 0, 1]])
Y = np.array([[0, 1, 1, 0]])
```

### Moons (Opcional)

```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.2)
```

---

## â±ï¸ Tiempo Estimado

2 horas

---

## ğŸ’¡ Tips

1. **Shapes**: Presta atenciÃ³n a las dimensiones de las matrices
2. **Debugging**: Imprime shapes en cada paso
3. **Gradientes**: Usa gradient checking para verificar
4. **Learning Rate**: Empieza con 0.1, ajusta si no converge
5. **Ã‰pocas**: XOR converge en ~1000-5000 Ã©pocas

---

## ğŸ“š Recursos

- [Neural Networks and Deep Learning (Nielsen)](http://neuralnetworksanddeeplearning.com/)
- [CS231n Backpropagation](https://cs231n.github.io/optimization-2/)
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

---

## ğŸ¯ Entregables

1. **CÃ³digo funcional** que entrene la red en XOR
2. **GrÃ¡fica de pÃ©rdida** vs Ã©pocas
3. **GrÃ¡fica de frontera de decisiÃ³n** mostrando clasificaciÃ³n correcta
4. **Accuracy final** impreso en consola

---

## âœ… Criterios de EvaluaciÃ³n

| Criterio | Puntos |
|----------|--------|
| Forward propagation correcto | 20 |
| Backpropagation correcto | 25 |
| Loop de entrenamiento funcional | 15 |
| XOR resuelto (100% accuracy) | 20 |
| Visualizaciones | 10 |
| CÃ³digo limpio y documentado | 10 |
| **Total** | **100** |

---

## ğŸš€ Bonus

- Implementar momentum
- Agregar regularizaciÃ³n L2
- Probar con dataset make_moons
- Arquitectura configurable (mÃ¡s capas)
