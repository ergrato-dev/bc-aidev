#  Glosario - Semana 19

T茅rminos clave de Fundamentos de Redes Neuronales.

---

## A

### Activation Function (Funci贸n de Activaci贸n)

Funci贸n no lineal aplicada a la salida de una neurona. Introduce no-linealidad en la red, permitiendo aprender patrones complejos.

```python
# Ejemplo: ReLU
def relu(z):
    return np.maximum(0, z)
```

---

## B

### Backpropagation (Retropropagaci贸n)

Algoritmo para calcular gradientes de la funci贸n de p茅rdida respecto a los pesos. Usa la regla de la cadena para propagar el error desde la salida hacia las capas anteriores.

### Bias (Sesgo)

Par谩metro adicional en cada neurona que permite desplazar la funci贸n de activaci贸n. An谩logo al t茅rmino independiente en una ecuaci贸n lineal.

$$z = w \cdot x + b$$

---

## C

### Chain Rule (Regla de la Cadena)

Regla de c谩lculo para derivar funciones compuestas. Fundamento matem谩tico de backpropagation.

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}$$

### Computation Graph (Grafo Computacional)

Representaci贸n visual de las operaciones matem谩ticas como un grafo dirigido. Facilita el c谩lculo de gradientes.

---

## D

### Deep Learning (Aprendizaje Profundo)

Subcampo del Machine Learning que usa redes neuronales con m煤ltiples capas (profundas) para aprender representaciones jer谩rquicas.

### Dense Layer (Capa Densa)

Capa donde cada neurona est谩 conectada a todas las neuronas de la capa anterior. Tambi茅n llamada "fully connected".

### Derivative (Derivada)

Medida de la tasa de cambio de una funci贸n. En redes neuronales, usada para determinar c贸mo ajustar pesos.

---

## E

### Epoch (poca)

Una pasada completa por todo el dataset de entrenamiento durante el proceso de aprendizaje.

### Exploding Gradient

Problema donde los gradientes crecen exponencialmente durante backpropagation, causando inestabilidad num茅rica.

---

## F

### Forward Propagation (Propagaci贸n Hacia Adelante)

Proceso de calcular la salida de la red pasando la entrada a trav茅s de todas las capas.

---

## G

### Gradient (Gradiente)

Vector de derivadas parciales. Indica la direcci贸n de m谩ximo crecimiento de una funci贸n.

### Gradient Descent (Descenso de Gradiente)

Algoritmo de optimizaci贸n que actualiza par谩metros en direcci贸n opuesta al gradiente para minimizar la funci贸n de p茅rdida.

$$w_{nuevo} = w_{viejo} - \eta \cdot \nabla L$$

---

## H

### He Initialization

T茅cnica de inicializaci贸n de pesos dise帽ada para redes con ReLU.

$$W \sim \mathcal{N}(0, \sqrt{2/n_{in}})$$

### Hidden Layer (Capa Oculta)

Capas entre la entrada y la salida. Extraen representaciones intermedias de los datos.

---

## L

### Learning Rate (Tasa de Aprendizaje)

Hiperpar谩metro que controla el tama帽o de los pasos en gradient descent. Notaci贸n: 畏 (eta) o 伪 (alpha).

### Leaky ReLU

Variante de ReLU que permite un peque帽o gradiente para valores negativos, evitando "neuronas muertas".

$$f(x) = \max(\alpha x, x), \quad \alpha \approx 0.01$$

### Loss Function (Funci贸n de P茅rdida)

Mide qu茅 tan lejos est谩n las predicciones de los valores reales. El objetivo del entrenamiento es minimizarla.

---

## M

### MLP (Multi-Layer Perceptron)

Red neuronal feedforward con una o m谩s capas ocultas. Arquitectura m谩s b谩sica de Deep Learning.

---

## N

### Neural Network (Red Neuronal)

Modelo computacional inspirado en el cerebro, compuesto por capas de neuronas artificiales interconectadas.

### Neuron (Neurona)

Unidad b谩sica de una red neuronal. Recibe entradas, aplica pesos, suma, a帽ade bias y pasa por activaci贸n.

---

## P

### Perceptron (Perceptr贸n)

Red neuronal m谩s simple, con una sola capa. Solo puede clasificar datos linealmente separables.

---

## R

### ReLU (Rectified Linear Unit)

Funci贸n de activaci贸n m谩s usada en capas ocultas: $f(x) = \max(0, x)$

---

## S

### Sigmoid

Funci贸n de activaci贸n que mapea valores al rango (0, 1). Usada en salidas binarias.

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

### Softmax

Funci贸n que convierte un vector de valores en probabilidades que suman 1. Usada en clasificaci贸n multiclase.

---

## T

### Tanh (Tangente Hiperb贸lica)

Funci贸n de activaci贸n que mapea valores al rango (-1, 1). Centrada en cero.

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

---

## V

### Vanishing Gradient

Problema donde los gradientes se vuelven muy peque帽os en capas profundas, impidiendo el aprendizaje efectivo.

---

## W

### Weight (Peso)

Par谩metro que determina la importancia de una conexi贸n entre neuronas. Se ajusta durante el entrenamiento.

---

## X

### Xavier Initialization

T茅cnica de inicializaci贸n de pesos para mantener varianza estable entre capas.

$$W \sim \mathcal{N}(0, \sqrt{1/n_{in}})$$

### XOR Problem

Problema cl谩sico que un perceptr贸n simple no puede resolver, pero un MLP con capa oculta s铆.

---

## S铆mbolos Comunes

| S铆mbolo  | Significado                                     |
| -------- | ----------------------------------------------- |
| $W$      | Matriz de pesos                                 |
| $b$      | Vector de bias                                  |
| $z$      | Pre-activaci贸n (antes de funci贸n de activaci贸n) |
| $a$      | Activaci贸n (despu茅s de funci贸n de activaci贸n)   |
| $L$      | P茅rdida (loss)                                  |
| $\eta$   | Learning rate                                   |
| $\nabla$ | Gradiente                                       |
| $\sigma$ | Sigmoid                                         |
