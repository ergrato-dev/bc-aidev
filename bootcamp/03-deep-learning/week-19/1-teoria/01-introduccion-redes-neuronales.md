# üß† Introducci√≥n a las Redes Neuronales

## üéØ Objetivos

- Comprender la inspiraci√≥n biol√≥gica de las redes neuronales
- Entender los componentes b√°sicos de una neurona artificial
- Conocer la historia y evoluci√≥n del deep learning

---

## üìö Contenido

### 1. De las Neuronas Biol√≥gicas a las Artificiales

El cerebro humano contiene aproximadamente **86 mil millones de neuronas**, cada una conectada a miles de otras. Las redes neuronales artificiales se inspiran en esta arquitectura.

![Neurona Biol√≥gica vs Artificial](../0-assets/01-neurona-biologica-vs-artificial.svg)

#### Neurona Biol√≥gica

```
Dendritas ‚Üí Soma ‚Üí Ax√≥n ‚Üí Terminales sin√°pticas
 (inputs)   (proceso)  (output)    (conexiones)
```

- **Dendritas**: Reciben se√±ales de otras neuronas
- **Soma**: Cuerpo celular que procesa las se√±ales
- **Ax√≥n**: Transmite la se√±al de salida
- **Sinapsis**: Conexiones con otras neuronas

#### Neurona Artificial

```python
# Analog√≠a en c√≥digo
def artificial_neuron(inputs, weights, bias, activation_fn):
    """
    inputs: se√±ales de entrada (como dendritas)
    weights: importancia de cada input (como sinapsis)
    bias: umbral de activaci√≥n
    activation_fn: decisi√≥n de activarse o no (como soma)
    """
    weighted_sum = sum(x * w for x, w in zip(inputs, weights)) + bias
    output = activation_fn(weighted_sum)
    return output
```

---

### 2. Componentes de una Neurona Artificial

#### 2.1 Inputs (x)

Los datos de entrada que la neurona recibe:

```python
import numpy as np

# Ejemplo: caracter√≠sticas de una imagen de 3 p√≠xeles
inputs = np.array([0.5, 0.3, 0.8])  # x‚ÇÅ, x‚ÇÇ, x‚ÇÉ
```

#### 2.2 Pesos (w)

Determinan la importancia de cada input:

```python
# Cada input tiene un peso asociado
weights = np.array([0.4, -0.2, 0.6])  # w‚ÇÅ, w‚ÇÇ, w‚ÇÉ

# Pesos grandes = input importante
# Pesos negativos = input inhibe la salida
```

#### 2.3 Bias (b)

Permite ajustar el umbral de activaci√≥n:

```python
bias = 0.1  # Desplaza la funci√≥n de activaci√≥n
```

#### 2.4 Suma Ponderada (z)

Combinaci√≥n lineal de inputs y pesos:

```python
# z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ + b
z = np.dot(inputs, weights) + bias
print(f"Suma ponderada: {z}")  # z = 0.5*0.4 + 0.3*(-0.2) + 0.8*0.6 + 0.1 = 0.72
```

#### 2.5 Funci√≥n de Activaci√≥n (f)

Introduce no-linealidad:

```python
def sigmoid(z):
    """Funci√≥n de activaci√≥n sigmoid"""
    return 1 / (1 + np.exp(-z))

# Aplicar activaci√≥n
output = sigmoid(z)
print(f"Output: {output}")  # ‚âà 0.67
```

---

### 3. ¬øPor Qu√© Necesitamos No-Linealidad?

Sin funciones de activaci√≥n no-lineales, una red multicapa ser√≠a equivalente a una sola capa:

```python
# Sin activaci√≥n (solo lineal)
# Capa 1: h = W‚ÇÅx + b‚ÇÅ
# Capa 2: y = W‚ÇÇh + b‚ÇÇ = W‚ÇÇ(W‚ÇÅx + b‚ÇÅ) + b‚ÇÇ = (W‚ÇÇW‚ÇÅ)x + (W‚ÇÇb‚ÇÅ + b‚ÇÇ)
#                      = W'x + b' ‚Üê ¬°Equivalente a una sola capa!

# Con activaci√≥n no-lineal
# h = f(W‚ÇÅx + b‚ÇÅ)
# y = f(W‚ÇÇh + b‚ÇÇ) ‚Üê ¬°No se puede simplificar!
```

Las funciones de activaci√≥n permiten a las redes aprender **patrones complejos y no-lineales**.

---

### 4. Historia del Deep Learning

| A√±o  | Hito                                    | Investigadores     |
| ---- | --------------------------------------- | ------------------ |
| 1943 | Modelo de neurona McCulloch-Pitts       | McCulloch, Pitts   |
| 1958 | Perceptr√≥n                              | Frank Rosenblatt   |
| 1969 | Limitaciones del Perceptr√≥n (XOR)       | Minsky, Papert     |
| 1986 | Backpropagation popularizado            | Rumelhart, Hinton  |
| 2006 | Deep Belief Networks                    | Hinton             |
| 2012 | AlexNet gana ImageNet                   | Krizhevsky, Hinton |
| 2017 | Transformer "Attention is All You Need" | Vaswani et al.     |
| 2022 | ChatGPT / GPT-4                         | OpenAI             |

---

### 5. Tipos de Arquitecturas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REDES NEURONALES                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Feedforward   ‚îÇ   Recurrentes   ‚îÇ    Convolucionales      ‚îÇ
‚îÇ      (MLP)      ‚îÇ   (RNN/LSTM)    ‚îÇ        (CNN)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Datos tabular ‚îÇ ‚Ä¢ Secuencias    ‚îÇ ‚Ä¢ Im√°genes              ‚îÇ
‚îÇ ‚Ä¢ Clasificaci√≥n ‚îÇ ‚Ä¢ Texto/NLP     ‚îÇ ‚Ä¢ Video                 ‚îÇ
‚îÇ ‚Ä¢ Regresi√≥n     ‚îÇ ‚Ä¢ Series temp.  ‚îÇ ‚Ä¢ Audio (espectro)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 6. Neurona Completa en C√≥digo

```python
import numpy as np

class Neuron:
    """Implementaci√≥n de una neurona artificial."""

    def __init__(self, n_inputs: int, activation: str = 'sigmoid'):
        # Inicializaci√≥n aleatoria de pesos
        self.weights = np.random.randn(n_inputs) * 0.01
        self.bias = 0.0
        self.activation = activation

    def _activate(self, z: float) -> float:
        """Aplica la funci√≥n de activaci√≥n."""
        if self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-z))
        elif self.activation == 'relu':
            return max(0, z)
        elif self.activation == 'tanh':
            return np.tanh(z)
        else:
            return z  # Lineal

    def forward(self, inputs: np.ndarray) -> float:
        """Forward pass: calcula la salida de la neurona."""
        z = np.dot(inputs, self.weights) + self.bias
        return self._activate(z)


# Uso
neuron = Neuron(n_inputs=3, activation='sigmoid')
inputs = np.array([0.5, 0.3, 0.8])
output = neuron.forward(inputs)
print(f"Output: {output:.4f}")
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la analog√≠a entre neuronas biol√≥gicas y artificiales
- [ ] Puedo explicar el papel de pesos, bias y activaci√≥n
- [ ] Comprendo por qu√© la no-linealidad es esencial
- [ ] Conozco los hitos principales de la historia del DL

---

## üîó Referencias

- [Neural Networks and Deep Learning - Ch. 1](http://neuralnetworksanddeeplearning.com/chap1.html)
- [3Blue1Brown - But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)
