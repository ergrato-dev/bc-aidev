# ‚ö° Ejercicio 02: Autograd y Gradientes

## üéØ Objetivo

Comprender el sistema de diferenciaci√≥n autom√°tica de PyTorch para calcular gradientes necesarios en el entrenamiento de redes neuronales.

---

## üìã Instrucciones

Este ejercicio te guiar√° a trav√©s del uso de autograd. Abre `starter/main.py` y descomenta cada secci√≥n seg√∫n avances.

---

## Paso 1: requires_grad B√°sico

El flag `requires_grad` indica que queremos calcular gradientes:

```python
import torch

# Tensor con gradientes
x = torch.tensor([2.0], requires_grad=True)

# Operaci√≥n
y = x ** 2  # y = x¬≤

# Calcular gradiente
y.backward()

print(x.grad)  # dy/dx = 2x = 4
```

**Abre `starter/main.py`** y descomenta la secci√≥n del Paso 1.

---

## Paso 2: Grafo Computacional

PyTorch construye un grafo de operaciones autom√°ticamente:

```python
a = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([3.0], requires_grad=True)

c = a * b    # c = 6
d = c + a    # d = 8
e = d ** 2   # e = 64

e.backward()

print(a.grad)  # de/da
print(b.grad)  # de/db
```

**Descomenta** la secci√≥n del Paso 2.

---

## Paso 3: Acumulaci√≥n de Gradientes

Los gradientes se acumulan por defecto. Hay que limpiarlos:

```python
x = torch.tensor([2.0], requires_grad=True)

# Primera backward
y1 = x ** 2
y1.backward()
print(x.grad)  # 4

# Segunda backward (se acumula!)
y2 = x ** 3
y2.backward()
print(x.grad)  # 4 + 12 = 16

# Limpiar gradientes
x.grad.zero_()
```

**Descomenta** la secci√≥n del Paso 3.

---

## Paso 4: torch.no_grad()

Desactiva el c√°lculo de gradientes para inferencia:

```python
x = torch.tensor([2.0], requires_grad=True)

# Con gradientes
y = x ** 2
print(y.requires_grad)  # True

# Sin gradientes (inferencia)
with torch.no_grad():
    z = x ** 2
    print(z.requires_grad)  # False
```

**Descomenta** la secci√≥n del Paso 4.

---

## Paso 5: detach()

Desconecta un tensor del grafo computacional:

```python
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2

# y est√° conectado al grafo
z = y.detach()  # z NO est√° conectado

# √ötil para:
# - Pasar a NumPy: y.detach().numpy()
# - Congelar parte de la red
```

**Descomenta** la secci√≥n del Paso 5.

---

## Paso 6: Gradientes de Funciones Comunes

Verifica que autograd calcula gradientes correctamente:

```python
# Lineal: y = 3x + 2 ‚Üí dy/dx = 3
# Cuadr√°tica: y = x¬≤ ‚Üí dy/dx = 2x
# Exponencial: y = e^x ‚Üí dy/dx = e^x
# Sigmoid: y = œÉ(x) ‚Üí dy/dx = œÉ(x)(1-œÉ(x))
```

**Descomenta** la secci√≥n del Paso 6.

---

## Paso 7: Regresi√≥n Lineal Manual

Implementa regresi√≥n lineal usando solo autograd:

```python
# y = wx + b
# Aprende w y b usando gradientes
```

**Descomenta** la secci√≥n del Paso 7.

---

## ‚úÖ Verificaci√≥n

Al completar todos los pasos, deber√≠as:
- Entender c√≥mo funciona `requires_grad`
- Ver el grafo computacional en acci√≥n
- Saber cu√°ndo usar `no_grad()` y `detach()`
- Implementar regresi√≥n lineal con gradientes manuales

---

## üìö Recursos

- [Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
