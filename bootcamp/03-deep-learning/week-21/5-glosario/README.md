# üìñ Glosario - Semana 21: PyTorch

T√©rminos clave de PyTorch ordenados alfab√©ticamente.

---

## A

### Autograd
Sistema de diferenciaci√≥n autom√°tica de PyTorch. Calcula gradientes autom√°ticamente registrando operaciones en un grafo computacional din√°mico.

```python
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()  # Autograd calcula dy/dx = 4
```

---

## B

### Backward
M√©todo que calcula gradientes propagando hacia atr√°s a trav√©s del grafo computacional.

```python
loss.backward()  # Calcula gradientes de todos los tensores con requires_grad=True
```

### Batch
Conjunto de muestras procesadas juntas en una iteraci√≥n de entrenamiento.

```python
dataloader = DataLoader(dataset, batch_size=32)
```

---

## C

### Computational Graph
Estructura de datos que representa las operaciones matem√°ticas y sus dependencias. En PyTorch es din√°mico (define-by-run).

### CrossEntropyLoss
Funci√≥n de p√©rdida para clasificaci√≥n multiclase. Combina LogSoftmax y NLLLoss.

```python
criterion = nn.CrossEntropyLoss()
loss = criterion(outputs, targets)  # targets son √≠ndices de clase
```

### CUDA
Plataforma de NVIDIA para computaci√≥n en GPU. PyTorch usa CUDA para acelerar operaciones.

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---

## D

### DataLoader
Clase que proporciona iteraci√≥n eficiente sobre datasets con batching, shuffling y carga paralela.

```python
loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
```

### Dataset
Clase abstracta que representa un conjunto de datos. Debe implementar `__len__` y `__getitem__`.

### Define-by-Run
Paradigma donde el grafo computacional se construye din√°micamente durante la ejecuci√≥n, no antes.

### Detach
M√©todo que crea una copia del tensor sin conexi√≥n al grafo computacional.

```python
frozen = tensor.detach()  # No propaga gradientes
```

### Dropout
T√©cnica de regularizaci√≥n que desactiva neuronas aleatoriamente durante entrenamiento.

```python
dropout = nn.Dropout(p=0.5)  # 50% de probabilidad de desactivar
```

### dtype
Tipo de dato de un tensor (float32, int64, etc.).

```python
tensor = torch.tensor([1, 2, 3], dtype=torch.float32)
```

---

## E

### Epoch
Una pasada completa por todo el dataset de entrenamiento.

### eval()
M√©todo que pone el modelo en modo evaluaci√≥n. Desactiva Dropout y usa estad√≠sticas guardadas en BatchNorm.

```python
model.eval()
with torch.no_grad():
    predictions = model(test_data)
```

---

## F

### Forward
M√©todo que define c√≥mo los datos fluyen a trav√©s del modelo.

```python
def forward(self, x):
    return self.fc(x)
```

### Functional API (F)
M√≥dulo `torch.nn.functional` con funciones sin estado (activaciones, p√©rdidas).

```python
import torch.nn.functional as F
x = F.relu(x)
```

---

## G

### Gradient
Derivada parcial de una funci√≥n respecto a sus par√°metros. Indica la direcci√≥n de m√°ximo crecimiento.

### grad
Atributo de un tensor donde se almacenan los gradientes despu√©s de `backward()`.

```python
x.grad  # Gradiente de la p√©rdida respecto a x
```

### GPU
Graphics Processing Unit. Acelera operaciones matriciales del deep learning.

---

## L

### Learning Rate
Hiperpar√°metro que controla el tama√±o del paso en la actualizaci√≥n de par√°metros.

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

### Linear (nn.Linear)
Capa completamente conectada. Aplica transformaci√≥n $y = xW^T + b$.

```python
fc = nn.Linear(in_features=784, out_features=128)
```

### Loss Function
Funci√≥n que mide el error entre predicciones y valores reales.

---

## M

### Module (nn.Module)
Clase base para todos los componentes de redes neuronales en PyTorch.

```python
class MiRed(nn.Module):
    def __init__(self):
        super().__init__()
```

---

## N

### no_grad
Context manager que desactiva el c√°lculo de gradientes. Usado para inferencia.

```python
with torch.no_grad():
    output = model(data)
```

### numel
M√©todo que retorna el n√∫mero total de elementos en un tensor.

```python
tensor.numel()  # Total de elementos
```

---

## O

### Optimizer
Algoritmo que actualiza los par√°metros del modelo usando gradientes.

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer.step()  # Actualiza par√°metros
```

---

## P

### Parameters
Tensores entrenables de un modelo. Accesibles via `model.parameters()`.

### PyTorch
Framework de deep learning desarrollado por Meta AI. Conocido por su grafo din√°mico y sintaxis pyth√≥nica.

---

## R

### ReLU
Rectified Linear Unit. Funci√≥n de activaci√≥n $f(x) = \max(0, x)$.

```python
relu = nn.ReLU()
# o
x = F.relu(x)
```

### requires_grad
Flag que indica si un tensor necesita gradientes calculados.

```python
x = torch.tensor([1.0], requires_grad=True)
```

### Reshape
Cambiar la forma de un tensor sin modificar sus datos.

```python
tensor.reshape(3, 4)
tensor.view(3, 4)
```

---

## S

### Squeeze
Elimina dimensiones de tama√±o 1 de un tensor.

```python
t = torch.rand(1, 3, 1)
t.squeeze()  # Shape: [3]
```

### state_dict
Diccionario que mapea nombres de par√°metros a tensores. Usado para guardar/cargar modelos.

```python
torch.save(model.state_dict(), 'model.pth')
model.load_state_dict(torch.load('model.pth'))
```

### Step
M√©todo del optimizador que actualiza los par√°metros usando los gradientes calculados.

```python
optimizer.step()
```

---

## T

### Tensor
Array multidimensional, estructura de datos fundamental en PyTorch.

```python
tensor = torch.tensor([[1, 2], [3, 4]])
```

### to()
M√©todo para mover tensores o modelos a un dispositivo espec√≠fico.

```python
tensor.to('cuda')
model.to(device)
```

### train()
M√©todo que pone el modelo en modo entrenamiento. Activa Dropout y BatchNorm usa estad√≠sticas del batch.

```python
model.train()
```

---

## U

### Unsqueeze
A√±ade una dimensi√≥n de tama√±o 1 en la posici√≥n especificada.

```python
t = torch.rand(3)
t.unsqueeze(0)  # Shape: [1, 3]
```

---

## V

### View
Retorna un tensor con diferente shape pero compartiendo memoria con el original.

```python
tensor.view(3, 4)  # Requiere memoria contigua
```

---

## Z

### zero_grad
M√©todo que resetea los gradientes a cero. Necesario antes de cada backward.

```python
optimizer.zero_grad()
```

---

_Semana 21 | M√≥dulo 3: Deep Learning | Bootcamp IA: Zero to Hero_
