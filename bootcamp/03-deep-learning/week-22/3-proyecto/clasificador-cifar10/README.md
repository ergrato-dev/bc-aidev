# ğŸ¯ Proyecto: Clasificador de ImÃ¡genes CIFAR-10

## ğŸ“‹ DescripciÃ³n

ConstruirÃ¡s una CNN desde cero para clasificar imÃ¡genes del dataset CIFAR-10, aplicando todos los conceptos de convoluciÃ³n, pooling y arquitecturas aprendidos esta semana.

**CIFAR-10** contiene 60,000 imÃ¡genes a color (32Ã—32Ã—3) en 10 clases:
- âœˆï¸ AviÃ³n (airplane)
- ğŸš— AutomÃ³vil (automobile)
- ğŸ¦ PÃ¡jaro (bird)
- ğŸ± Gato (cat)
- ğŸ¦Œ Ciervo (deer)
- ğŸ• Perro (dog)
- ğŸ¸ Rana (frog)
- ğŸ´ Caballo (horse)
- ğŸš¢ Barco (ship)
- ğŸšš CamiÃ³n (truck)

---

## ğŸ¯ Objetivos

1. **DiseÃ±ar** una arquitectura CNN apropiada para CIFAR-10
2. **Implementar** la red usando PyTorch
3. **Entrenar** el modelo con tÃ©cnicas de regularizaciÃ³n
4. **Evaluar** y analizar el rendimiento
5. **Visualizar** filtros y predicciones

---

## ğŸ“Š Requisitos del Modelo

### Arquitectura MÃ­nima

Tu CNN debe incluir:

- [ ] Al menos **3 bloques convolucionales**
- [ ] **Batch Normalization** despuÃ©s de cada convoluciÃ³n
- [ ] **MaxPooling** o stride > 1 para reducir dimensiones
- [ ] **Dropout** para regularizaciÃ³n
- [ ] **Clasificador** con al menos una capa oculta

### MÃ©tricas Objetivo

| MÃ©trica | MÃ­nimo | Deseable |
|---------|--------|----------|
| Test Accuracy | â‰¥ 70% | â‰¥ 75% |
| Training completo | â‰¤ 20 Ã©pocas | â‰¤ 15 Ã©pocas |

---

## ğŸ—ï¸ Arquitectura Sugerida

```
Input: 32Ã—32Ã—3 (imagen RGB)
â”‚
â”œâ”€â”€ Bloque 1
â”‚   â”œâ”€â”€ Conv2d(3, 32, 3, padding=1)
â”‚   â”œâ”€â”€ BatchNorm2d(32)
â”‚   â”œâ”€â”€ ReLU
â”‚   â””â”€â”€ MaxPool2d(2)  â†’ 16Ã—16Ã—32
â”‚
â”œâ”€â”€ Bloque 2
â”‚   â”œâ”€â”€ Conv2d(32, 64, 3, padding=1)
â”‚   â”œâ”€â”€ BatchNorm2d(64)
â”‚   â”œâ”€â”€ ReLU
â”‚   â””â”€â”€ MaxPool2d(2)  â†’ 8Ã—8Ã—64
â”‚
â”œâ”€â”€ Bloque 3
â”‚   â”œâ”€â”€ Conv2d(64, 128, 3, padding=1)
â”‚   â”œâ”€â”€ BatchNorm2d(128)
â”‚   â”œâ”€â”€ ReLU
â”‚   â””â”€â”€ MaxPool2d(2)  â†’ 4Ã—4Ã—128
â”‚
â”œâ”€â”€ Flatten â†’ 2048
â”‚
â”œâ”€â”€ Clasificador
â”‚   â”œâ”€â”€ Linear(2048, 256)
â”‚   â”œâ”€â”€ ReLU
â”‚   â”œâ”€â”€ Dropout(0.5)
â”‚   â””â”€â”€ Linear(256, 10)
â”‚
â””â”€â”€ Output: 10 clases
```

---

## ğŸ“ Entregables

### 1. CÃ³digo (70%)

**Archivo**: `starter/main.py`

Debe contener:

```python
# Imports necesarios
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. DefiniciÃ³n del modelo
class CIFAR10CNN(nn.Module):
    # TODO: Implementar arquitectura

# 2. Data augmentation y loaders
# TODO: Implementar transformaciones

# 3. Funciones de entrenamiento
# TODO: train_epoch, evaluate

# 4. Loop de entrenamiento
# TODO: Entrenar el modelo

# 5. EvaluaciÃ³n final
# TODO: MÃ©tricas y visualizaciones
```

### 2. Resultados (20%)

Generar los siguientes archivos:

- `training_curves.png` - Curvas de loss y accuracy
- `confusion_matrix.png` - Matriz de confusiÃ³n
- `sample_predictions.png` - Ejemplos de predicciones
- `model_cifar10.pth` - Modelo entrenado

### 3. AnÃ¡lisis (10%)

Responder en comentarios del cÃ³digo:

1. Â¿Por quÃ© CIFAR-10 es mÃ¡s difÃ­cil que MNIST?
2. Â¿QuÃ© efecto tiene el data augmentation?
3. Â¿QuÃ© clases confunde mÃ¡s el modelo?

---

## ğŸ”§ ConfiguraciÃ³n Recomendada

```python
# HiperparÃ¡metros sugeridos
BATCH_SIZE = 128
LEARNING_RATE = 0.001
NUM_EPOCHS = 15
WEIGHT_DECAY = 1e-4

# Data Augmentation
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2470, 0.2435, 0.2616]
    )
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2470, 0.2435, 0.2616]
    )
])
```

---

## ğŸ“ˆ RÃºbrica de EvaluaciÃ³n

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| **Arquitectura** | 25 | CNN con â‰¥3 bloques, BatchNorm, Dropout |
| **Data Augmentation** | 15 | Flip, Crop, NormalizaciÃ³n correcta |
| **Entrenamiento** | 20 | Loop correcto, scheduler, early stopping |
| **Accuracy â‰¥70%** | 20 | Alcanzar mÃ©trica objetivo |
| **Visualizaciones** | 10 | Curvas, matriz confusiÃ³n, predicciones |
| **CÃ³digo limpio** | 10 | Comentarios, organizaciÃ³n, type hints |
| **Total** | 100 | |

---

## ğŸ’¡ Tips

1. **Empieza simple**: Primero haz que funcione, luego optimiza
2. **Data augmentation es clave**: Sin Ã©l, accuracy baja ~5-10%
3. **Learning rate scheduling**: Reduce LR cuando loss se estanca
4. **Monitorea overfitting**: Si train_acc >> test_acc, agrega regularizaciÃ³n
5. **GPU**: Usa CUDA si estÃ¡ disponible (`torch.cuda.is_available()`)

---

## ğŸ”— Recursos

- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- [PyTorch CIFAR-10 Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)
- [Data Augmentation en PyTorch](https://pytorch.org/vision/stable/transforms.html)

---

## ğŸ“… Tiempo Estimado

| Fase | Tiempo |
|------|--------|
| Setup y datos | 15 min |
| DiseÃ±o arquitectura | 30 min |
| ImplementaciÃ³n | 45 min |
| Entrenamiento | 20 min |
| EvaluaciÃ³n y visualizaciÃ³n | 10 min |
| **Total** | ~2 horas |

---

## ğŸš€ Â¡Comienza!

1. Abre `starter/main.py`
2. Implementa cada secciÃ³n marcada con TODO
3. Ejecuta y entrena tu modelo
4. Genera las visualizaciones requeridas
5. Â¡Alcanza el 70% de accuracy!

**Â¡Buena suerte! ğŸ€**
