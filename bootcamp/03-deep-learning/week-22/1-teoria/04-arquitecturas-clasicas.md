# ðŸ›ï¸ Arquitecturas ClÃ¡sicas: LeNet y VGG

## ðŸŽ¯ Objetivos

- Conocer la historia y evoluciÃ³n de las arquitecturas CNN
- Entender LeNet-5 como la primera CNN prÃ¡ctica
- Dominar la filosofÃ­a de diseÃ±o de VGG
- Implementar ambas arquitecturas en PyTorch

---

## ðŸ“‹ Contenido

### 1. LeNet-5 (1998)

La primera red convolucional exitosa, diseÃ±ada por Yann LeCun para reconocimiento de dÃ­gitos escritos a mano.

#### Historia

- **Creador**: Yann LeCun (AT&T Bell Labs)
- **AÃ±o**: 1998
- **AplicaciÃ³n**: Lectura automÃ¡tica de cheques bancarios
- **Dataset**: MNIST (dÃ­gitos 0-9)
- **Impacto**: DemostrÃ³ que CNNs podÃ­an superar mÃ©todos tradicionales

#### Arquitectura

```
ENTRADA â†’ C1 â†’ S2 â†’ C3 â†’ S4 â†’ C5 â†’ F6 â†’ SALIDA

Capa    | Tipo          | Output    | ParÃ¡metros
--------|---------------|-----------|------------
Input   | -             | 32Ã—32Ã—1   | -
C1      | Conv 5Ã—5, 6   | 28Ã—28Ã—6   | 156
S2      | AvgPool 2Ã—2   | 14Ã—14Ã—6   | 0
C3      | Conv 5Ã—5, 16  | 10Ã—10Ã—16  | 2,416
S4      | AvgPool 2Ã—2   | 5Ã—5Ã—16    | 0
C5      | Conv 5Ã—5, 120 | 1Ã—1Ã—120   | 48,120
F6      | FC 84         | 84        | 10,164
Output  | FC 10         | 10        | 850
--------|---------------|-----------|------------
Total   |               |           | ~61,706
```

#### CaracterÃ­sticas Clave

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **TamaÃ±o entrada** | 32Ã—32 (MNIST se rellena de 28Ã—28) |
| **ActivaciÃ³n** | Tanh (en el paper original) |
| **Pooling** | Average pooling (subsampling) |
| **Conexiones C3** | No todas las conexiones (optimizaciÃ³n) |

#### ImplementaciÃ³n en PyTorch

```python
import torch
import torch.nn as nn

class LeNet5(nn.Module):
    """
    LeNet-5 modernizado para MNIST.
    
    Cambios del original:
    - ReLU en lugar de Tanh
    - Max pooling en lugar de average
    - Todas las conexiones en C3
    """
    
    def __init__(self, num_classes: int = 10):
        super().__init__()
        
        # C1: 1@32Ã—32 â†’ 6@28Ã—28
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        
        # S2: 6@28Ã—28 â†’ 6@14Ã—14
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # C3: 6@14Ã—14 â†’ 16@10Ã—10
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        
        # S4: 16@10Ã—10 â†’ 16@5Ã—5
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # C5: 16@5Ã—5 â†’ 120@1Ã—1
        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)
        
        # F6: 120 â†’ 84
        self.fc1 = nn.Linear(120, 84)
        
        # Output: 84 â†’ 10
        self.fc2 = nn.Linear(84, num_classes)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Feature extraction
        x = torch.relu(self.conv1(x))  # 32â†’28
        x = self.pool1(x)               # 28â†’14
        x = torch.relu(self.conv2(x))  # 14â†’10
        x = self.pool2(x)               # 10â†’5
        x = torch.relu(self.conv3(x))  # 5â†’1
        
        # Classification
        x = x.view(x.size(0), -1)      # Flatten
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Crear y probar
model = LeNet5()
x = torch.randn(1, 1, 32, 32)
output = model(x)
print(f"Input: {x.shape}, Output: {output.shape}")

# Contar parÃ¡metros
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parÃ¡metros: {total_params:,}")  # ~61,706
```

---

### 2. VGG (2014)

VGGNet demostrÃ³ que la **profundidad** es crucial. GanÃ³ el segundo lugar en ImageNet 2014.

#### Historia

- **Creadores**: Visual Geometry Group (Oxford)
- **AÃ±o**: 2014
- **Paper**: "Very Deep Convolutional Networks for Large-Scale Image Recognition"
- **InnovaciÃ³n**: Usar solo filtros 3Ã—3

#### La FilosofÃ­a 3Ã—3

Â¿Por quÃ© solo filtros 3Ã—3?

```
Dos capas 3Ã—3 equivalen a una capa 5Ã—5:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3Ã—3 â”‚ 3Ã—3 â”‚  =  â”‚ 5Ã—5 â”‚
â”‚  â†’  â”‚  â†’  â”‚     â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Campo receptivo: 3 + (3-1) = 5

Pero con menos parÃ¡metros:
- 2 Ã— (3Ã—3Ã—CÃ—C) = 18CÂ²
- 1 Ã— (5Ã—5Ã—CÃ—C) = 25CÂ²

Â¡Y mÃ¡s no-linealidades (2 ReLUs vs 1)!
```

#### Arquitecturas VGG

| Modelo | Capas | DescripciÃ³n |
|--------|-------|-------------|
| VGG-11 | 11 | 8 conv + 3 FC |
| VGG-13 | 13 | 10 conv + 3 FC |
| VGG-16 | 16 | 13 conv + 3 FC |
| VGG-19 | 19 | 16 conv + 3 FC |

#### VGG-16 Detallado

```
Bloque 1: 2Ã— Conv 64  + MaxPool    [224â†’112]
Bloque 2: 2Ã— Conv 128 + MaxPool    [112â†’56]
Bloque 3: 3Ã— Conv 256 + MaxPool    [56â†’28]
Bloque 4: 3Ã— Conv 512 + MaxPool    [28â†’14]
Bloque 5: 3Ã— Conv 512 + MaxPool    [14â†’7]
FC: 4096 â†’ 4096 â†’ 1000

Total parÃ¡metros: ~138 millones
```

#### ImplementaciÃ³n VGG-16

```python
import torch
import torch.nn as nn

def make_vgg_block(in_channels: int, out_channels: int, num_convs: int) -> nn.Sequential:
    """Crea un bloque VGG: N convs + maxpool."""
    layers = []
    for i in range(num_convs):
        layers.append(nn.Conv2d(
            in_channels if i == 0 else out_channels,
            out_channels,
            kernel_size=3,
            padding=1
        ))
        layers.append(nn.ReLU(inplace=True))
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)


class VGG16(nn.Module):
    """
    VGG-16 para ImageNet.
    
    Arquitectura:
    - 5 bloques convolucionales
    - 3 capas fully connected
    - 138M parÃ¡metros
    """
    
    def __init__(self, num_classes: int = 1000):
        super().__init__()
        
        # Feature extractor
        self.features = nn.Sequential(
            # Bloque 1: 3â†’64, 224â†’112
            make_vgg_block(3, 64, 2),
            # Bloque 2: 64â†’128, 112â†’56
            make_vgg_block(64, 128, 2),
            # Bloque 3: 128â†’256, 56â†’28
            make_vgg_block(128, 256, 3),
            # Bloque 4: 256â†’512, 28â†’14
            make_vgg_block(256, 512, 3),
            # Bloque 5: 512â†’512, 14â†’7
            make_vgg_block(512, 512, 3),
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Crear modelo
model = VGG16(num_classes=10)

# Verificar con imagen pequeÃ±a (adaptar para CIFAR)
x = torch.randn(1, 3, 224, 224)
output = model(x)
print(f"Input: {x.shape}, Output: {output.shape}")
```

#### VGG para CIFAR-10 (adaptado)

```python
class VGG_CIFAR(nn.Module):
    """VGG simplificado para CIFAR-10 (32Ã—32)."""
    
    def __init__(self, num_classes: int = 10):
        super().__init__()
        
        self.features = nn.Sequential(
            # Bloque 1: 32â†’16
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Bloque 2: 16â†’8
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Bloque 3: 8â†’4
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Bloque 4: 4â†’2
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 2 * 2, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Para CIFAR-10
model_cifar = VGG_CIFAR(num_classes=10)
x = torch.randn(1, 3, 32, 32)
print(f"VGG-CIFAR output: {model_cifar(x).shape}")
```

---

### 3. ComparaciÃ³n LeNet vs VGG

| Aspecto | LeNet-5 | VGG-16 |
|---------|---------|--------|
| **AÃ±o** | 1998 | 2014 |
| **Entrada** | 32Ã—32Ã—1 | 224Ã—224Ã—3 |
| **Profundidad** | 7 capas | 16 capas |
| **ParÃ¡metros** | ~60K | ~138M |
| **Filtros** | 5Ã—5 | 3Ã—3 |
| **ActivaciÃ³n** | Tanh | ReLU |
| **Pooling** | Average | Max |
| **Dataset** | MNIST | ImageNet |

---

### 4. Principios de DiseÃ±o

#### PatrÃ³n ComÃºn en CNNs

```
INICIO                              FIN
Espacial grande, pocos canales  â†’   Espacial pequeÃ±o, muchos canales

[224Ã—224Ã—3] â†’ [112Ã—112Ã—64] â†’ [56Ã—56Ã—128] â†’ [28Ã—28Ã—256] â†’ [14Ã—14Ã—512] â†’ [7Ã—7Ã—512]

Canales:   3  â†’   64   â†’   128   â†’   256   â†’   512   â†’   512
Espacial: 224 â†’  112   â†’    56   â†’    28   â†’    14   â†’     7
```

#### Reglas de DiseÃ±o

1. **Duplicar canales al reducir espacialidad**
   ```
   Conv 64 â†’ Pool â†’ Conv 128 â†’ Pool â†’ Conv 256
   ```

2. **Usar filtros pequeÃ±os y profundidad**
   ```
   3Ã—3 + 3Ã—3 > 5Ã—5 (mismo campo receptivo, menos params)
   ```

3. **Batch Normalization** (post-VGG)
   ```python
   Conv â†’ BatchNorm â†’ ReLU
   ```

4. **Global Average Pooling** (post-VGG)
   ```python
   # En lugar de Flatten + FC grandes
   nn.AdaptiveAvgPool2d(1)  # Reduce a 1Ã—1
   ```

---

### 5. Usando Modelos Pre-entrenados

```python
import torch
from torchvision import models

# VGG-16 pre-entrenado en ImageNet
vgg16 = models.vgg16(weights='IMAGENET1K_V1')

# Ver arquitectura
print(vgg16)

# Usar solo features (transfer learning)
vgg_features = vgg16.features

# Congelar pesos
for param in vgg_features.parameters():
    param.requires_grad = False

# Adaptar para nueva tarea
class CustomVGG(nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()
        self.features = vgg16.features
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(512, num_classes)
    
    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Para 10 clases
model = CustomVGG(num_classes=10)
```

---

### 6. EvoluciÃ³n Post-VGG

```
VGG (2014)      ResNet (2015)     DenseNet (2016)
    â”‚               â”‚                  â”‚
    â”‚           Skip Connections   Dense Connections
    â”‚               â”‚                  â”‚
    â–¼               â–¼                  â–¼
[Conv-Conv]     [Conv + Input]    [Conv concat all]
[Pool]          [Pool]            [Pool]

Problema:       SoluciÃ³n:         Beneficio:
DegradaciÃ³n     Residual Learning Feature Reuse
con profundidad permite 152+      eficiencia params
```

---

## ðŸ“Š Resumen

| Arquitectura | InnovaciÃ³n Principal | LecciÃ³n |
|--------------|---------------------|---------|
| **LeNet** | Primera CNN funcional | Convoluciones funcionan |
| **AlexNet** | GPU, ReLU, Dropout | Escala con hardware |
| **VGG** | Profundidad con 3Ã—3 | MÃ¡s profundo = mejor |
| **ResNet** | Skip connections | Entrenar redes muy profundas |

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Conozco la arquitectura de LeNet-5
- [ ] Entiendo por quÃ© VGG usa solo filtros 3Ã—3
- [ ] Puedo implementar ambas arquitecturas en PyTorch
- [ ] SÃ© usar modelos pre-entrenados de torchvision
- [ ] Entiendo el patrÃ³n "mÃ¡s canales, menos espacial"

---

_Siguiente: [PrÃ¡ctica - ConvoluciÃ³n Manual](../2-practicas/ejercicio-01-convolucion-manual/)_
