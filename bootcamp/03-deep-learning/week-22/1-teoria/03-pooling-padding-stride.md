# üìê Pooling, Padding y Stride

## üéØ Objetivos

- Entender el prop√≥sito y tipos de pooling
- Dominar el concepto de padding y sus modos
- Comprender el efecto del stride
- Calcular dimensiones de salida en cualquier configuraci√≥n

---

## üìã Contenido

### 1. Pooling (Submuestreo)

El pooling **reduce la dimensionalidad espacial** manteniendo la informaci√≥n m√°s relevante.

#### ¬øPor qu√© Pooling?

| Beneficio | Descripci√≥n |
|-----------|-------------|
| **Reduce c√≥mputo** | Menos p√≠xeles = menos operaciones |
| **Reduce par√°metros** | Capas siguientes m√°s peque√±as |
| **Invarianza espacial** | Peque√±os desplazamientos no afectan |
| **Reduce overfitting** | Menos par√°metros = mejor generalizaci√≥n |

---

### 2. Tipos de Pooling

#### 2.1 Max Pooling

Toma el **valor m√°ximo** de cada regi√≥n:

```
Entrada (4√ó4):           Max Pool 2√ó2, stride=2:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1 ‚îÇ 3 ‚îÇ 2 ‚îÇ 1 ‚îÇ        ‚îÇ 4 ‚îÇ 4 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§   ‚Üí    ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2 ‚îÇ 4 ‚îÇ 1 ‚îÇ 4 ‚îÇ        ‚îÇ 6 ‚îÇ 5 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§        ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
‚îÇ 6 ‚îÇ 2 ‚îÇ 3 ‚îÇ 1 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1 ‚îÇ 4 ‚îÇ 2 ‚îÇ 5 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

Regi√≥n [0:2, 0:2]:       Regi√≥n [0:2, 2:4]:
[1, 3]                   [2, 1]
[2, 4] ‚Üí max = 4         [1, 4] ‚Üí max = 4
```

**Uso**: El m√°s com√∫n en CNNs. Preserva features m√°s prominentes.

```python
import torch.nn as nn

max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
# Reduce dimensiones a la mitad
```

#### 2.2 Average Pooling

Calcula el **promedio** de cada regi√≥n:

```
Entrada:                 Avg Pool 2√ó2:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1 ‚îÇ 3 ‚îÇ 2 ‚îÇ 2 ‚îÇ        ‚îÇ 2.5 ‚îÇ 2.0 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§   ‚Üí    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2 ‚îÇ 4 ‚îÇ 2 ‚îÇ 2 ‚îÇ        ‚îÇ 3.25‚îÇ 2.75‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ 4 ‚îÇ 2 ‚îÇ 3 ‚îÇ 1 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 5 ‚îÇ 2 ‚îÇ 4 ‚îÇ 3 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

(1+3+2+4)/4 = 2.5
```

**Uso**: √ötil en algunas arquitecturas, menos agresivo que max pooling.

```python
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)
```

#### 2.3 Global Pooling

Reduce **todo el feature map a un solo valor** por canal:

```
Entrada (C√óH√óW):         Global Pool:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Canal 1     ‚îÇ          ‚îÇv1 ‚îÇ
‚îÇ  H √ó W      ‚îÇ    ‚Üí     ‚îú‚îÄ‚îÄ‚îÄ‚î§
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇv2 ‚îÇ
‚îÇ Canal 2     ‚îÇ          ‚îú‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  H √ó W      ‚îÇ          ‚îÇ...‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îò
                         (C√ó1√ó1)
```

```python
# Global Average Pooling
gap = nn.AdaptiveAvgPool2d(1)  # Output: (batch, channels, 1, 1)

# Global Max Pooling
gmp = nn.AdaptiveMaxPool2d(1)
```

**Uso**: Reemplaza capas FC finales en arquitecturas modernas.

---

### 3. Comparaci√≥n de Pooling

```python
import torch
import torch.nn as nn

# Crear tensor de ejemplo
x = torch.tensor([
    [1., 3., 2., 1.],
    [2., 4., 1., 4.],
    [6., 2., 3., 1.],
    [1., 4., 2., 5.]
]).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 4, 4)

# Aplicar diferentes poolings
max_pool = nn.MaxPool2d(2, 2)
avg_pool = nn.AvgPool2d(2, 2)
global_avg = nn.AdaptiveAvgPool2d(1)
global_max = nn.AdaptiveMaxPool2d(1)

print(f"Input:\n{x.squeeze()}")
print(f"\nMax Pool 2√ó2:\n{max_pool(x).squeeze()}")
print(f"\nAvg Pool 2√ó2:\n{avg_pool(x).squeeze()}")
print(f"\nGlobal Avg: {global_avg(x).item():.2f}")
print(f"Global Max: {global_max(x).item():.2f}")
```

---

### 4. Padding

El padding **a√±ade valores alrededor de la imagen** para controlar el tama√±o de salida.

#### 4.1 Sin Padding (Valid)

```
Entrada 5√ó5:             Kernel 3√ó3:        Salida 3√ó3:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ x x x x x     ‚îÇ        ‚îÇ k k k   ‚îÇ        ‚îÇ o o o   ‚îÇ
‚îÇ x x x x x     ‚îÇ   *    ‚îÇ k k k   ‚îÇ   =    ‚îÇ o o o   ‚îÇ
‚îÇ x x x x x     ‚îÇ        ‚îÇ k k k   ‚îÇ        ‚îÇ o o o   ‚îÇ
‚îÇ x x x x x     ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ x x x x x     ‚îÇ        
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        5 - 3 + 1 = 3
```

#### 4.2 Same Padding

A√±ade padding para que **salida = entrada**:

```
Entrada 5√ó5 + pad=1:     Kernel 3√ó3:        Salida 5√ó5:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 0 0 0 0 0 0 0   ‚îÇ      ‚îÇ k k k   ‚îÇ        ‚îÇ o o o o o ‚îÇ
‚îÇ 0 x x x x x 0   ‚îÇ  *   ‚îÇ k k k   ‚îÇ   =    ‚îÇ o o o o o ‚îÇ
‚îÇ 0 x x x x x 0   ‚îÇ      ‚îÇ k k k   ‚îÇ        ‚îÇ o o o o o ‚îÇ
‚îÇ 0 x x x x x 0   ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ o o o o o ‚îÇ
‚îÇ 0 x x x x x 0   ‚îÇ                         ‚îÇ o o o o o ‚îÇ
‚îÇ 0 x x x x x 0   ‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ 0 0 0 0 0 0 0   ‚îÇ      
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      (5 - 3 + 2√ó1)/1 + 1 = 5
```

#### Calcular Padding para Same

```python
def same_padding(kernel_size: int) -> int:
    """Calcula padding para mantener dimensiones."""
    return kernel_size // 2

# Ejemplos
print(same_padding(3))  # 1 (para kernel 3√ó3)
print(same_padding(5))  # 2 (para kernel 5√ó5)
print(same_padding(7))  # 3 (para kernel 7√ó7)
```

#### En PyTorch

```python
import torch.nn as nn

# Padding expl√≠cito
conv_valid = nn.Conv2d(1, 32, kernel_size=3, padding=0)  # Valid
conv_same = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # Same

# Padding autom√°tico (PyTorch 1.9+)
conv_auto = nn.Conv2d(1, 32, kernel_size=3, padding='same')
```

---

### 5. Stride

El stride **controla el salto del kernel** entre posiciones.

#### Stride = 1 (Default)

```
Posici√≥n 1:    Posici√≥n 2:    Posici√≥n 3:
[x x x]‚óã ‚óã    ‚óã[x x x]‚óã    ‚óã ‚óã[x x x]
[x x x]‚óã ‚óã    ‚óã[x x x]‚óã    ‚óã ‚óã[x x x]
[x x x]‚óã ‚óã    ‚óã[x x x]‚óã    ‚óã ‚óã[x x x]
 ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã
 ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã

Se mueve 1 p√≠xel a la vez
```

#### Stride = 2

```
Posici√≥n 1:    Posici√≥n 2:    (Sin posici√≥n 3)
[x x x]‚óã ‚óã    ‚óã ‚óã[x x x]
[x x x]‚óã ‚óã    ‚óã ‚óã[x x x]
[x x x]‚óã ‚óã    ‚óã ‚óã[x x x]
 ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã
 ‚óã ‚óã ‚óã ‚óã ‚óã     ‚óã ‚óã ‚óã ‚óã ‚óã

Se mueve 2 p√≠xeles a la vez ‚Üí reduce a la mitad
```

#### Efecto en Dimensiones

```python
def output_with_stride(W: int, K: int, P: int, S: int) -> int:
    """Calcula dimensi√≥n de salida."""
    return (W - K + 2 * P) // S + 1

# Imagen 32√ó32, kernel 3√ó3
print(output_with_stride(32, 3, 0, 1))  # 30 (stride=1)
print(output_with_stride(32, 3, 0, 2))  # 15 (stride=2)
print(output_with_stride(32, 3, 1, 1))  # 32 (same padding)
print(output_with_stride(32, 3, 1, 2))  # 16 (same + stride=2)
```

---

### 6. Stride vs Pooling para Reducir Dimensiones

Ambos reducen dimensionalidad, pero de forma diferente:

| Aspecto | Max Pooling | Stride > 1 |
|---------|-------------|------------|
| **Operaci√≥n** | Selecciona m√°ximo | Salta posiciones |
| **Par√°metros** | 0 | Mismos que conv normal |
| **Informaci√≥n** | Preserva m√°ximos | Puede perder info |
| **Uso moderno** | Tradicional | ResNet, arquitecturas recientes |

```python
# Reducir de 32√ó32 a 16√ó16

# Opci√≥n 1: Max Pooling
model_pool = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),  # 32√ó32
    nn.ReLU(),
    nn.MaxPool2d(2, 2)               # 16√ó16
)

# Opci√≥n 2: Strided Convolution
model_stride = nn.Sequential(
    nn.Conv2d(3, 64, 3, stride=2, padding=1),  # 16√ó16 directamente
    nn.ReLU()
)
```

---

### 7. Ejemplos Pr√°cticos

#### Calcular Dimensiones de una CNN

```python
import torch
import torch.nn as nn

class ExampleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Entrada: 1√ó32√ó32
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # ‚Üí 32√ó32√ó32
        self.pool1 = nn.MaxPool2d(2, 2)               # ‚Üí 32√ó16√ó16
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # ‚Üí 64√ó16√ó16
        self.pool2 = nn.MaxPool2d(2, 2)               # ‚Üí 64√ó8√ó8
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # ‚Üí 128√ó8√ó8
        self.pool3 = nn.MaxPool2d(2, 2)               # ‚Üí 128√ó4√ó4
        
        # 128 √ó 4 √ó 4 = 2048
        self.fc = nn.Linear(128 * 4 * 4, 10)
    
    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = self.pool3(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc(x)
        return x

# Verificar dimensiones
model = ExampleCNN()
x = torch.randn(1, 1, 32, 32)
print(f"Input: {x.shape}")

# Ver cada paso
x1 = model.pool1(torch.relu(model.conv1(x)))
print(f"After conv1+pool1: {x1.shape}")

x2 = model.pool2(torch.relu(model.conv2(x1)))
print(f"After conv2+pool2: {x2.shape}")

x3 = model.pool3(torch.relu(model.conv3(x2)))
print(f"After conv3+pool3: {x3.shape}")
```

#### Tabla de Dimensiones

```
Capa            | Output Shape | Params
----------------|--------------|--------
Input           | 1√ó32√ó32      | -
Conv1 (32, 3√ó3) | 32√ó32√ó32     | 320
Pool1 (2√ó2)     | 32√ó16√ó16     | 0
Conv2 (64, 3√ó3) | 64√ó16√ó16     | 18,496
Pool2 (2√ó2)     | 64√ó8√ó8       | 0
Conv3 (128,3√ó3) | 128√ó8√ó8      | 73,856
Pool3 (2√ó2)     | 128√ó4√ó4      | 0
Flatten         | 2048         | 0
FC (10)         | 10           | 20,490
----------------|--------------|--------
Total           |              | 113,162
```

---

### 8. Padding Modes en PyTorch

```python
import torch.nn.functional as F

# Zeros (default)
x_zeros = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)

# Reflect
x_reflect = F.pad(x, (1, 1, 1, 1), mode='reflect')

# Replicate
x_replicate = F.pad(x, (1, 1, 1, 1), mode='replicate')

# Circular
x_circular = F.pad(x, (1, 1, 1, 1), mode='circular')
```

```
Original:        Zeros:           Reflect:         Replicate:
[a b c]          [0 a b c 0]      [b a b c b]      [a a b c c]
[d e f]    ‚Üí     [0 d e f 0]      [e d e f e]      [d d e f f]
[g h i]          [0 g h i 0]      [h g h i h]      [g g h i i]
```

---

## üìä Resumen de F√≥rmulas

### Tama√±o de Salida

$$O = \left\lfloor \frac{W - K + 2P}{S} \right\rfloor + 1$$

### Padding para Same (stride=1)

$$P = \frac{K - 1}{2}$$

### Reducci√≥n por Pooling

$$O = \frac{W}{k}$$ (con stride = kernel_size)

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la diferencia entre max y average pooling
- [ ] S√© cu√°ndo usar global pooling
- [ ] Puedo calcular padding para mantener dimensiones
- [ ] Entiendo el efecto del stride
- [ ] Puedo calcular las dimensiones de salida de cualquier capa

---

_Siguiente: [Arquitecturas Cl√°sicas (LeNet, VGG)](04-arquitecturas-clasicas.md)_
