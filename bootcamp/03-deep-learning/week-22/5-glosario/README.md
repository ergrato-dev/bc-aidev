# üìñ Glosario - Semana 22: CNNs I

T√©rminos clave ordenados alfab√©ticamente.

---

## A

### Activation Map
Ver **Feature Map**.

### Average Pooling
Operaci√≥n de pooling que calcula el promedio de valores en una ventana. Menos com√∫n que Max Pooling pero √∫til para ciertas aplicaciones.

```python
nn.AvgPool2d(kernel_size=2, stride=2)
```

---

## B

### Batch Normalization (BatchNorm)
T√©cnica de normalizaci√≥n que estandariza las activaciones de cada capa durante el entrenamiento. Acelera convergencia y permite learning rates m√°s altos.

```python
# Despu√©s de Conv2d
nn.BatchNorm2d(num_features)  # num_features = canales
```

**F√≥rmula:**
$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta$$

---

## C

### Canal (Channel)
Dimensi√≥n de profundidad en una imagen o feature map. Im√°genes RGB tienen 3 canales; feature maps pueden tener cientos.

### Convoluci√≥n
Operaci√≥n matem√°tica que aplica un filtro/kernel sobre una imagen, produciendo un mapa de caracter√≠sticas.

**F√≥rmula 2D:**
$$(I * K)[i,j] = \sum_m \sum_n I[i+m, j+n] \cdot K[m, n]$$

### Conv2d
Capa convolucional 2D en PyTorch.

```python
nn.Conv2d(
    in_channels,   # Canales entrada
    out_channels,  # N√∫mero de filtros
    kernel_size,   # Tama√±o del kernel
    stride=1,      # Paso
    padding=0      # Relleno
)
```

---

## D

### Dilation
Separaci√≥n entre elementos del kernel. Dilation > 1 aumenta el campo receptivo sin aumentar par√°metros.

### Downsampling
Reducci√≥n de las dimensiones espaciales de un tensor, t√≠picamente mediante pooling o stride > 1.

### Dropout
T√©cnica de regularizaci√≥n que desactiva neuronas aleatoriamente durante entrenamiento.

```python
nn.Dropout(p=0.5)      # Para capas FC
nn.Dropout2d(p=0.25)   # Para feature maps
```

---

## F

### Feature Map
Salida de una capa convolucional. Representa caracter√≠sticas detectadas (bordes, texturas, formas).

### Filtro
Ver **Kernel**.

### Flatten
Operaci√≥n que convierte un tensor multidimensional en un vector 1D.

```python
nn.Flatten()  # (B, C, H, W) -> (B, C*H*W)
```

---

## G

### Global Average Pooling (GAP)
Pooling que reduce cada feature map a un √∫nico valor promediando todos los elementos. Elimina la necesidad de capas FC grandes.

```python
nn.AdaptiveAvgPool2d(1)  # Output: (B, C, 1, 1)
```

---

## K

### Kernel (Filtro)
Matriz de pesos que se desliza sobre la imagen en una convoluci√≥n. Detecta patrones espec√≠ficos como bordes o texturas.

**Ejemplos comunes (3√ó3):**
- Sobel (bordes verticales)
- Laplaciano (detecci√≥n de bordes)
- Gaussian (suavizado)

---

## L

### LeNet-5
Primera CNN exitosa (LeCun, 1998). Arquitectura:
- Input 32√ó32
- 2 capas conv + pool
- 3 capas FC
- ~61,000 par√°metros

---

## M

### Max Pooling
Operaci√≥n que selecciona el valor m√°ximo de una ventana. Reduce dimensiones y proporciona invariancia a peque√±as traslaciones.

```python
nn.MaxPool2d(kernel_size=2, stride=2)
```

---

## O

### Output Size (Tama√±o de Salida)
F√≥rmula para calcular dimensiones despu√©s de convoluci√≥n:

$$O = \frac{W - K + 2P}{S} + 1$$

Donde:
- $W$ = tama√±o de entrada
- $K$ = tama√±o del kernel
- $P$ = padding
- $S$ = stride

---

## P

### Padding
Relleno a√±adido alrededor de la imagen antes de convoluci√≥n.

| Tipo | Descripci√≥n |
|------|-------------|
| `valid` | Sin padding (output m√°s peque√±o) |
| `same` | Padding para mantener tama√±o |

```python
nn.Conv2d(..., padding='same')  # Mantiene dimensiones
nn.Conv2d(..., padding=1)       # Padding expl√≠cito
```

### Pooling
Operaci√≥n de reducci√≥n de dimensionalidad que resume regiones del feature map.

---

## R

### Receptive Field (Campo Receptivo)
Regi√≥n de la imagen de entrada que influye en una neurona espec√≠fica de una capa posterior. Crece con la profundidad de la red.

### ReLU (Rectified Linear Unit)
Funci√≥n de activaci√≥n: $f(x) = \max(0, x)$

```python
nn.ReLU(inplace=True)
```

---

## S

### Stride
Paso o desplazamiento del kernel en cada movimiento. Stride > 1 reduce dimensiones.

```python
nn.Conv2d(..., stride=2)  # Reduce tama√±o a la mitad
```

### Subsampling
Ver **Downsampling** o **Pooling**.

---

## T

### Transfer Learning
T√©cnica de usar modelos pre-entrenados (ej: VGG en ImageNet) y adaptarlos a nuevas tareas.

---

## V

### VGG-16
Arquitectura profunda (Simonyan & Zisserman, 2014) con:
- 13 capas conv (3√ó3)
- 3 capas FC
- ~138 millones de par√°metros

Demostr√≥ que la profundidad mejora el rendimiento.

---

## W

### Weight Sharing
Caracter√≠stica de CNNs donde el mismo kernel se aplica en todas las posiciones de la imagen, reduciendo dr√°sticamente el n√∫mero de par√°metros.

---

## F√≥rmulas Clave

| Operaci√≥n | F√≥rmula |
|-----------|---------|
| Output size | $O = \frac{W - K + 2P}{S} + 1$ |
| Par√°metros Conv | $K^2 \times C_{in} \times C_{out} + C_{out}$ |
| BatchNorm | $\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$ |

---

## üîó Navegaci√≥n

[‚Üê Volver a la Semana](../README.md)
