# üìä Batch Normalization

## üéØ Objetivos

- Comprender el problema de Internal Covariate Shift
- Entender c√≥mo funciona Batch Normalization
- Conocer par√°metros Œ≥ y Œ≤
- Diferenciar comportamiento en train vs eval

---

## 1. El Problema

### Internal Covariate Shift

Durante el entrenamiento, la distribuci√≥n de las activaciones cambia constantemente:

```
√âpoca 1: activaciones ‚àà [-1, 1]
√âpoca 5: activaciones ‚àà [-5, 10]
√âpoca 10: activaciones ‚àà [0, 100]

‚Üí Cada capa debe re-adaptarse constantemente
‚Üí Entrenamiento lento e inestable
```

### S√≠ntomas

- Necesidad de learning rates muy peque√±os
- Inicializaci√≥n cuidadosa de pesos
- Saturaci√≥n de funciones de activaci√≥n
- Entrenamiento lento

---

## 2. Soluci√≥n: Batch Normalization

### Idea Principal

Normalizar las activaciones a media 0 y varianza 1 **dentro de cada mini-batch**:

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

Donde:
- $\mu_B = \frac{1}{m}\sum_{i=1}^{m} x_i$ (media del batch)
- $\sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu_B)^2$ (varianza del batch)
- $\epsilon$ = peque√±o valor para estabilidad num√©rica

### Par√°metros Aprendibles (Œ≥, Œ≤)

La red puede "desnormalizar" si es √∫til:

$$y_i = \gamma \hat{x}_i + \beta$$

- **Œ≥ (gamma)**: escala - puede revertir la normalizaci√≥n
- **Œ≤ (beta)**: desplazamiento - ajusta la media

```python
# Si Œ≥ = œÉ_B y Œ≤ = Œº_B, se recupera la entrada original
```

---

## 3. Algoritmo Completo

![Batch Normalization Process](../0-assets/03-batch-normalization.svg)

---

## 4. Implementaci√≥n desde Cero

```python
import torch
import torch.nn as nn

class MyBatchNorm1d(nn.Module):
    """Implementaci√≥n manual de Batch Normalization."""
    
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.eps = eps
        self.momentum = momentum
        
        # Par√°metros aprendibles
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        
        # Running statistics (para inferencia)
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        if self.training:
            # Calcular estad√≠sticas del batch
            mean = x.mean(dim=0)
            var = x.var(dim=0, unbiased=False)
            
            # Actualizar running statistics
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var
        else:
            # Usar running statistics en inferencia
            mean = self.running_mean
            var = self.running_var
        
        # Normalizar
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        # Escalar y desplazar
        return self.gamma * x_norm + self.beta
```

---

## 5. Batch Norm en PyTorch

### Tipos de Batch Norm

```python
import torch.nn as nn

# Para capas fully connected (normaliza sobre features)
nn.BatchNorm1d(num_features)

# Para CNNs (normaliza sobre canales)
nn.BatchNorm2d(num_features)  # Input: (N, C, H, W)

# Para datos 3D (videos, 3D convs)
nn.BatchNorm3d(num_features)
```

### Uso en Redes

```python
# Patr√≥n com√∫n: Linear/Conv ‚Üí BatchNorm ‚Üí Activation
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    
    nn.Linear(256, 128),
    nn.BatchNorm1d(128),
    nn.ReLU(),
    
    nn.Linear(128, 10)
)
```

### Para CNNs

```python
class CNNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        
        self.fc = nn.Linear(64 * 7 * 7, 10)
    
    def forward(self, x):
        # Conv ‚Üí BN ‚Üí ReLU ‚Üí Pool
        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)
        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)
        
        x = x.view(-1, 64 * 7 * 7)
        return self.fc(x)
```

---

## 6. Train vs Eval Mode

### Diferencia Cr√≠tica

```python
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.ReLU()
)

# ENTRENAMIENTO: usa estad√≠sticas del batch actual
model.train()
output_train = model(batch_data)

# INFERENCIA: usa running statistics acumuladas
model.eval()
output_eval = model(single_sample)  # ¬°No necesita batch!
```

### ‚ö†Ô∏è Error Com√∫n

```python
# ‚ùå INCORRECTO: Olvidar model.eval()
model.train()  # BN usa estad√≠sticas del batch
prediction = model(test_data)  # Resultados incorrectos

# ‚úÖ CORRECTO
model.eval()  # BN usa running statistics
with torch.no_grad():
    prediction = model(test_data)
```

---

## 7. Beneficios de Batch Norm

### 1. Entrenamiento M√°s R√°pido

```python
# Sin BN: lr peque√±o, muchas √©pocas
optimizer = Adam(model.parameters(), lr=0.0001)

# Con BN: lr m√°s grande, converge antes
optimizer = Adam(model_bn.parameters(), lr=0.001)
```

### 2. Menos Sensible a Inicializaci√≥n

```python
# Sin BN: inicializaci√≥n cr√≠tica
nn.init.xavier_uniform_(layer.weight)  # Necesario

# Con BN: m√°s robusto a inicializaci√≥n
# La normalizaci√≥n "arregla" distribuciones malas
```

### 3. Efecto Regularizador

El ruido de las estad√≠sticas del mini-batch act√∫a como regularizaci√≥n ligera.

### 4. Permite Learning Rates Mayores

```
Sin BN:  lr_max ‚âà 0.001
Con BN:  lr_max ‚âà 0.01 - 0.1
```

---

## 8. D√≥nde Colocar Batch Norm

### Debate: Antes o Despu√©s de Activaci√≥n

```python
# Opci√≥n 1: Despu√©s de Linear/Conv, antes de activaci√≥n (original)
x = self.linear(x)
x = self.bn(x)
x = F.relu(x)

# Opci√≥n 2: Despu√©s de activaci√≥n
x = self.linear(x)
x = F.relu(x)
x = self.bn(x)

# En la pr√°ctica, Opci√≥n 1 es m√°s com√∫n
```

### Con Dropout

```python
# Orden t√≠pico: Linear ‚Üí BN ‚Üí ReLU ‚Üí Dropout
x = self.linear(x)
x = self.bn(x)
x = F.relu(x)
x = self.dropout(x)
```

---

## 9. Variantes de Normalizaci√≥n

| T√©cnica | Normaliza sobre | Uso principal |
|---------|-----------------|---------------|
| **Batch Norm** | Mini-batch | CNNs, MLPs |
| **Layer Norm** | Features | Transformers, RNNs |
| **Instance Norm** | Instancia individual | Style Transfer |
| **Group Norm** | Grupos de canales | Batch peque√±os |

```python
# Layer Norm (no depende del batch)
nn.LayerNorm(normalized_shape)

# Instance Norm
nn.InstanceNorm2d(num_features)

# Group Norm
nn.GroupNorm(num_groups, num_channels)
```

---

## 10. Ejemplo Comparativo

```python
def compare_with_without_bn():
    """Compara convergencia con y sin Batch Norm."""
    
    # Sin Batch Norm
    model_no_bn = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    
    # Con Batch Norm
    model_with_bn = nn.Sequential(
        nn.Linear(784, 256),
        nn.BatchNorm1d(256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.BatchNorm1d(128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    
    # El modelo con BN t√≠picamente:
    # - Converge m√°s r√°pido
    # - Permite lr m√°s alto
    # - Es m√°s estable
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo el problema de Internal Covariate Shift
- [ ] Comprendo la f√≥rmula de normalizaci√≥n
- [ ] S√© qu√© hacen Œ≥ y Œ≤
- [ ] Distingo el comportamiento en train vs eval
- [ ] Conozco d√≥nde colocar BatchNorm en una red

---

## üìö Recursos Adicionales

- [Batch Normalization Paper (2015)](https://arxiv.org/abs/1502.03167)
- [How Does Batch Norm Help Optimization?](https://arxiv.org/abs/1805.11604)
- [PyTorch BatchNorm Documentation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)

---

_Siguiente: [04-data-augmentation.md](04-data-augmentation.md) - Aumentar datos artificialmente_
