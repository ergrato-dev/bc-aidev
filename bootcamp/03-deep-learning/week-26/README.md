# üõ°Ô∏è Semana 26: Regularizaci√≥n en Deep Learning

## üéØ Objetivos de Aprendizaje

Al finalizar esta semana, ser√°s capaz de:

- ‚úÖ Comprender el problema de overfitting en redes neuronales
- ‚úÖ Implementar Dropout y entender su efecto
- ‚úÖ Aplicar Batch Normalization correctamente
- ‚úÖ Usar Data Augmentation para aumentar datos
- ‚úÖ Configurar Early Stopping y Weight Decay
- ‚úÖ Combinar t√©cnicas de regularizaci√≥n efectivamente

---

## üìö Requisitos Previos

- Semana 25: Transformers completada
- Comprensi√≥n de redes neuronales y backpropagation
- Experiencia con PyTorch o TensorFlow

---

## üóÇÔ∏è Estructura de la Semana

```
week-26/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ rubrica-evaluacion.md
‚îú‚îÄ‚îÄ 0-assets/
‚îÇ   ‚îú‚îÄ‚îÄ 01-overfitting-underfitting.svg
‚îÇ   ‚îú‚îÄ‚îÄ 02-dropout-visualization.svg
‚îÇ   ‚îú‚îÄ‚îÄ 03-batch-normalization.svg
‚îÇ   ‚îî‚îÄ‚îÄ 04-data-augmentation.svg
‚îú‚îÄ‚îÄ 1-teoria/
‚îÇ   ‚îú‚îÄ‚îÄ 01-overfitting-problema.md
‚îÇ   ‚îú‚îÄ‚îÄ 02-dropout.md
‚îÇ   ‚îú‚îÄ‚îÄ 03-batch-normalization.md
‚îÇ   ‚îî‚îÄ‚îÄ 04-data-augmentation.md
‚îú‚îÄ‚îÄ 2-practicas/
‚îÇ   ‚îú‚îÄ‚îÄ ejercicio-01-dropout/
‚îÇ   ‚îú‚îÄ‚îÄ ejercicio-02-batch-norm/
‚îÇ   ‚îî‚îÄ‚îÄ ejercicio-03-augmentation/
‚îú‚îÄ‚îÄ 3-proyecto/
‚îÇ   ‚îî‚îÄ‚îÄ clasificador-regularizado/
‚îú‚îÄ‚îÄ 4-recursos/
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ 5-glosario/
    ‚îî‚îÄ‚îÄ README.md
```

---

## üìù Contenidos

### üìñ Teor√≠a (1.5 horas)

| # | Tema | Archivo | Duraci√≥n |
|---|------|---------|----------|
| 1 | Overfitting y Underfitting | [01-overfitting-problema.md](1-teoria/01-overfitting-problema.md) | 20 min |
| 2 | Dropout | [02-dropout.md](1-teoria/02-dropout.md) | 25 min |
| 3 | Batch Normalization | [03-batch-normalization.md](1-teoria/03-batch-normalization.md) | 25 min |
| 4 | Data Augmentation | [04-data-augmentation.md](1-teoria/04-data-augmentation.md) | 20 min |

### üíª Pr√°cticas (2.5 horas)

| # | Ejercicio | Carpeta | Duraci√≥n |
|---|-----------|---------|----------|
| 1 | Dropout en CNNs | [ejercicio-01-dropout/](2-practicas/ejercicio-01-dropout/) | 45 min |
| 2 | Batch Normalization | [ejercicio-02-batch-norm/](2-practicas/ejercicio-02-batch-norm/) | 45 min |
| 3 | Data Augmentation | [ejercicio-03-augmentation/](2-practicas/ejercicio-03-augmentation/) | 60 min |

### üì¶ Proyecto (2 horas)

| Proyecto | Descripci√≥n | Carpeta |
|----------|-------------|---------|
| Clasificador Regularizado | CNN con todas las t√©cnicas de regularizaci√≥n | [clasificador-regularizado/](3-proyecto/clasificador-regularizado/) |

---

## ‚è±Ô∏è Distribuci√≥n del Tiempo

```
Total: 6 horas

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìñ Teor√≠a      ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  1.5h (25%)  ‚îÇ
‚îÇ  üíª Pr√°cticas   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  2.5h (42%)  ‚îÇ
‚îÇ  üì¶ Proyecto    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  2.0h (33%)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìå Entregables

1. **Ejercicios completados** (2-practicas/)
   - [ ] Dropout aplicado a CNN
   - [ ] Batch Normalization implementado
   - [ ] Data Augmentation configurado

2. **Proyecto semanal** (3-proyecto/)
   - [ ] Modelo sin regularizaci√≥n (baseline)
   - [ ] Modelo con todas las t√©cnicas
   - [ ] Comparaci√≥n de m√©tricas
   - [ ] Gr√°ficas de overfitting vs regularizado

---

## üîë Conceptos Clave

### El Problema del Overfitting

```
Training Accuracy: 99%  ‚Üí  "¬°Excelente!"
Test Accuracy: 60%      ‚Üí  "Houston, tenemos un problema"
```

### T√©cnicas de Regularizaci√≥n

| T√©cnica | Qu√© hace | Cu√°ndo usar |
|---------|----------|-------------|
| **Dropout** | Apaga neuronas aleatoriamente | Capas fully-connected |
| **Batch Norm** | Normaliza activaciones | Entre capas (CNNs, MLPs) |
| **Data Augmentation** | Genera variaciones de datos | Cuando hay pocos datos |
| **Weight Decay** | Penaliza pesos grandes | Siempre (L2 regularization) |
| **Early Stopping** | Para antes de overfitting | Durante entrenamiento |

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior | üè† M√≥dulo | Siguiente ‚û°Ô∏è |
|-------------|-----------|--------------|
| [Semana 25: Transformers](../week-25/README.md) | [Deep Learning](../README.md) | [Semana 27: Optimizaci√≥n](../week-27/README.md) |

---

## üí° Tips para esta Semana

> üéØ **Consejo**: La regularizaci√≥n es un arte. No apliques todo a la vez. Empieza con una t√©cnica, mide su efecto, y luego a√±ade m√°s.

- **Dropout**: Empieza con 0.2-0.3, ajusta seg√∫n validaci√≥n
- **Batch Norm**: Col√≥calo despu√©s de la capa lineal, antes de activaci√≥n
- **Augmentation**: Que las transformaciones tengan sentido para tu dominio
- **Early Stopping**: Paciencia de 5-10 √©pocas suele funcionar

---

_Semana 26 de 36 | M√≥dulo: Deep Learning | Bootcamp IA: Zero to Hero_
