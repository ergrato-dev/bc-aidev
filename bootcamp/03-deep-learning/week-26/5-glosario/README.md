# üìñ Glosario - Semana 26: Regularizaci√≥n

## B

### Batch Normalization
T√©cnica que normaliza las activaciones de cada capa a media 0 y varianza 1 dentro de cada mini-batch. Acelera entrenamiento y permite learning rates mayores.

### Bias-Variance Trade-off
Balance entre error por suposiciones simplificadas (bias/underfitting) y error por sensibilidad a datos (variance/overfitting).

## C

### Co-adaptaci√≥n
Fen√≥meno donde neuronas dependen excesivamente unas de otras, reducido por Dropout.

### ColorJitter
Transformaci√≥n que modifica aleatoriamente brillo, contraste, saturaci√≥n y tono de im√°genes.

### CutMix
T√©cnica de augmentation que corta y pega regiones entre im√°genes, mezclando tambi√©n las etiquetas.

### Cutout
T√©cnica que elimina regiones rectangulares aleatorias de im√°genes durante entrenamiento.

## D

### Data Augmentation
T√©cnicas para crear variaciones artificiales de datos de entrenamiento, aumentando efectivamente el tama√±o del dataset.

### Dropout
T√©cnica de regularizaci√≥n que apaga neuronas aleatoriamente durante entrenamiento con probabilidad p.

### Dropout2d
Variante de Dropout para CNNs que apaga canales completos en lugar de activaciones individuales.

## E

### Early Stopping
T√©cnica que detiene el entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar.

### Ensemble
Combinaci√≥n de m√∫ltiples modelos. Dropout puede verse como entrenamiento impl√≠cito de un ensemble.

## G

### Gamma (Œ≥)
Par√°metro aprendible de escala en Batch Normalization.

### Gap Train-Test
Diferencia entre accuracy de entrenamiento y test. Indicador de overfitting.

### Generalizaci√≥n
Capacidad del modelo de funcionar bien en datos no vistos durante entrenamiento.

## I

### Internal Covariate Shift
Cambio en la distribuci√≥n de activaciones durante entrenamiento, problema que BatchNorm mitiga.

### Inverted Dropout
Implementaci√≥n de Dropout que escala activaciones por 1/(1-p) durante entrenamiento.

## L

### L1 Regularization (Lasso)
Penalizaci√≥n que suma valores absolutos de pesos: ŒªŒ£|w|.

### L2 Regularization (Ridge)
Penalizaci√≥n que suma cuadrados de pesos: ŒªŒ£w¬≤. Tambi√©n llamada Weight Decay.

### Layer Normalization
Normalizaci√≥n sobre features en lugar de batch. Usada en Transformers.

## M

### Mixup
T√©cnica que mezcla pares de im√°genes y etiquetas: x' = Œªx‚ÇÅ + (1-Œª)x‚ÇÇ.

### model.eval()
Modo de evaluaci√≥n en PyTorch. Desactiva Dropout y usa running statistics en BatchNorm.

### model.train()
Modo de entrenamiento en PyTorch. Activa Dropout y usa batch statistics en BatchNorm.

## O

### Overfitting
Cuando el modelo memoriza datos de entrenamiento sin generalizar. Alta accuracy en train, baja en test.

## R

### RandomCrop
Transformaci√≥n que recorta una regi√≥n aleatoria de la imagen.

### RandomHorizontalFlip
Transformaci√≥n que voltea la imagen horizontalmente con cierta probabilidad.

### RandomRotation
Transformaci√≥n que rota la imagen un √°ngulo aleatorio.

### Running Statistics
Media y varianza acumuladas en BatchNorm, usadas durante inferencia.

## U

### Underfitting
Cuando el modelo es muy simple para capturar patrones. Baja accuracy en train y test.

## W

### Weight Decay
T√©cnica que reduce magnitud de pesos multiplic√°ndolos por factor < 1 en cada paso. Equivalente a L2 con SGD.
