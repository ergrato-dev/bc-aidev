# ‚ö° Optimizadores Modernos

![Comparaci√≥n de Optimizadores](../0-assets/01-optimizers-comparison.svg)

## üéØ Objetivos

- Comprender el descenso de gradiente y sus variantes
- Conocer SGD, Momentum, Adam, AdamW
- Saber cu√°ndo usar cada optimizador

---

## 1. Descenso de Gradiente B√°sico

### F√≥rmula

$$\theta_{t+1} = \theta_t - \eta \cdot \nabla L(\theta_t)$$

Donde:
- $\theta$ = par√°metros del modelo
- $\eta$ = learning rate
- $\nabla L$ = gradiente de la funci√≥n de p√©rdida

### Problema

El gradiente b√°sico puede ser lento y oscilar en valles estrechos.

---

## 2. SGD con Momentum

### Idea

Acumular "velocidad" en la direcci√≥n consistente del gradiente.

### F√≥rmula

$$v_t = \beta \cdot v_{t-1} + \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta \cdot v_t$$

### PyTorch

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9  # Valor t√≠pico
)
```

### Beneficios

- Acelera convergencia en direcciones consistentes
- Reduce oscilaciones
- Puede escapar m√≠nimos locales superficiales

---

## 3. Adam (Adaptive Moment Estimation)

### Idea

Combina momentum con learning rates adaptativos por par√°metro.

### F√≥rmulas

```
m_t = Œ≤‚ÇÅ ¬∑ m_{t-1} + (1 - Œ≤‚ÇÅ) ¬∑ g_t     # Primer momento (media)
v_t = Œ≤‚ÇÇ ¬∑ v_{t-1} + (1 - Œ≤‚ÇÇ) ¬∑ g_t¬≤    # Segundo momento (varianza)

mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)                  # Correcci√≥n de sesgo
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ^t)

Œ∏_{t+1} = Œ∏_t - Œ∑ ¬∑ mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

### PyTorch

```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,        # LR t√≠pico para Adam
    betas=(0.9, 0.999),
    eps=1e-8
)
```

### Caracter√≠sticas

- Learning rate adaptativo por par√°metro
- Funciona bien con gradientes ruidosos
- Buen default para la mayor√≠a de casos

---

## 4. AdamW (Adam + Weight Decay Correcto)

### Problema de Adam

En Adam original, weight decay se implementa como L2 en el gradiente, lo cual no es equivalente a weight decay real.

### Soluci√≥n: Decoupled Weight Decay

```python
# AdamW separa weight decay de la actualizaci√≥n de Adam
Œ∏_{t+1} = Œ∏_t - Œ∑ ¬∑ (mÃÇ_t / (‚àövÃÇ_t + Œµ) + Œª ¬∑ Œ∏_t)
```

### PyTorch

```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # Weight decay correcto
)
```

### ¬øCu√°ndo usar AdamW?

- **Siempre que uses Adam con regularizaci√≥n**
- Transfer learning
- Transformers y modelos grandes

---

## 5. Comparaci√≥n de Optimizadores

| Optimizador | LR t√≠pico | Uso recomendado |
|-------------|-----------|-----------------|
| **SGD** | 0.1 - 0.01 | Baseline, investigaci√≥n |
| **SGD+Momentum** | 0.1 - 0.01 | CNNs, cuando hay tiempo |
| **Adam** | 0.001 - 0.0001 | Default r√°pido |
| **AdamW** | 0.001 - 0.0001 | Con regularizaci√≥n |

### Velocidad vs Generalizaci√≥n

```
SGD+Momentum: M√°s lento, mejor generalizaci√≥n final
Adam/AdamW:   M√°s r√°pido, puede sobreajustar
```

---

## 6. Ejemplo Comparativo

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# Diferentes optimizadores
optimizers = {
    'SGD': torch.optim.SGD(model.parameters(), lr=0.01),
    'SGD+Momentum': torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9),
    'Adam': torch.optim.Adam(model.parameters(), lr=0.001),
    'AdamW': torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01),
}

# Loop de entrenamiento est√°ndar
def train_step(model, optimizer, x, y, criterion):
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()
    return loss.item()
```

---

## 7. Tips Pr√°cticos

### Empezar con Adam

```python
# Buen punto de partida
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

### Cambiar a SGD para Fine-tuning

```python
# Despu√©s de Adam, SGD puede mejorar generalizaci√≥n
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)
```

### AdamW para Modelos Grandes

```python
# Transformers, modelos preentrenados
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=2e-5,
    weight_decay=0.01
)
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la diferencia entre SGD y Momentum
- [ ] Comprendo c√≥mo Adam adapta learning rates
- [ ] S√© por qu√© AdamW es mejor que Adam con L2
- [ ] Puedo elegir el optimizador seg√∫n el caso

---

## üìö Recursos Adicionales

- [Adam Paper (Kingma & Ba 2014)](https://arxiv.org/abs/1412.6980)
- [AdamW Paper (Loshchilov 2017)](https://arxiv.org/abs/1711.05101)
- [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html)

---

_Siguiente: [02-learning-rate-schedules.md](02-learning-rate-schedules.md)_
