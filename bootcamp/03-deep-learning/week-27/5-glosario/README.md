# üìñ Glosario - Semana 27: Optimizaci√≥n en Deep Learning

T√©rminos clave ordenados alfab√©ticamente.

---

## A

### Adam (Adaptive Moment Estimation)

Optimizador que combina momentum (primer momento) con RMSprop (segundo momento). Adapta el learning rate por par√°metro.

```python
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

**Hiperpar√°metros**: Œ≤‚ÇÅ=0.9 (momentum), Œ≤‚ÇÇ=0.999 (escala), Œµ=1e-8

### AdamW

Variante de Adam con **weight decay desacoplado**. Aplica regularizaci√≥n L2 directamente a los pesos, no al gradiente.

```python
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

**Uso**: Recomendado sobre Adam cuando se necesita regularizaci√≥n.

---

## B

### Batch Size

N√∫mero de muestras procesadas antes de actualizar los pesos. Afecta la estabilidad del gradiente y el uso de memoria.

- **Peque√±o** (16-32): M√°s ruido, mejor generalizaci√≥n
- **Grande** (256+): M√°s estable, puede requerir ajuste de LR

### Bias Correction

Correcci√≥n aplicada en Adam para compensar la inicializaci√≥n en cero de los momentos m y v.

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$

---

## C

### Callback

Funci√≥n ejecutada en puntos espec√≠ficos del entrenamiento (inicio/fin de √©poca, batch, etc.).

```python
# Ejemplo: Early Stopping es un callback
if early_stopping(val_loss):
    break
```

### Checkpoint

Archivo que guarda el estado del modelo, optimizador y scheduler para poder resumir entrenamiento.

```python
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, 'checkpoint.pth')
```

### Cosine Annealing

Scheduler que reduce el LR siguiendo una curva coseno, permitiendo decay suave.

$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))$$

---

## E

### Early Stopping

T√©cnica que detiene el entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar por N √©pocas (patience).

**Prop√≥sito**: Prevenir overfitting y ahorrar tiempo de c√≥mputo.

### Exploding Gradients

Problema donde los gradientes crecen exponencialmente durante backpropagation, causando actualizaciones inestables.

**Soluci√≥n**: Gradient clipping, mejor inicializaci√≥n.

---

## G

### Glorot Initialization

Ver **Xavier Initialization**.

### Gradient Clipping

T√©cnica que limita la magnitud de los gradientes para prevenir exploding gradients.

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Tipos**: Por norma (recomendado) o por valor.

---

## H

### He Initialization (Kaiming)

Inicializaci√≥n de pesos dise√±ada para activaciones ReLU. Escala varianza considerando que ReLU "mata" valores negativos.

$$W \sim N(0, \sqrt{\frac{2}{n_{in}}})$$

```python
nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
```

---

## L

### Learning Rate (LR)

Hiperpar√°metro que controla el tama√±o del paso en la actualizaci√≥n de pesos. Crucial para convergencia.

- **Muy alto**: No converge, oscila
- **Muy bajo**: Convergencia lenta, m√≠nimos locales

### Learning Rate Schedule

Estrategia para modificar el LR durante el entrenamiento.

**Tipos comunes**: StepLR, CosineAnnealing, OneCycleLR, ReduceOnPlateau

### L2 Regularization

Penalizaci√≥n a√±adida a la loss proporcional a la norma L2 de los pesos.

$$L_{total} = L_{original} + \lambda \sum w_i^2$$

---

## M

### Momentum

T√©cnica que acumula gradientes pasados para acelerar convergencia y suavizar oscilaciones.

$$v_t = \beta v_{t-1} + \nabla L$$
$$w = w - \eta v_t$$

**Œ≤ t√≠pico**: 0.9

### Model State Dict

Diccionario de PyTorch que contiene todos los par√°metros (pesos y biases) del modelo.

```python
model.state_dict()  # Obtener
model.load_state_dict(state_dict)  # Cargar
```

---

## O

### OneCycleLR

Scheduler que implementa la pol√≠tica 1cycle: warmup hasta max_lr, luego decay hasta min_lr.

```python
scheduler = OneCycleLR(optimizer, max_lr=0.1, epochs=10, steps_per_epoch=len(loader))
```

**Importante**: Hacer `scheduler.step()` por batch, no por √©poca.

### Optimizer State Dict

Estado interno del optimizador (momentos, contadores) necesario para resumir entrenamiento.

---

## P

### Patience

N√∫mero de √©pocas sin mejora que Early Stopping espera antes de detener el entrenamiento.

**Valor t√≠pico**: 5-10 √©pocas

---

## R

### ReduceLROnPlateau

Scheduler que reduce el LR cuando una m√©trica deja de mejorar.

```python
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)
scheduler.step(val_loss)  # Necesita la m√©trica
```

---

## S

### SGD (Stochastic Gradient Descent)

Optimizador b√°sico que actualiza pesos en direcci√≥n opuesta al gradiente.

$$w = w - \eta \nabla L$$

```python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

### StepLR

Scheduler que reduce el LR por un factor cada N √©pocas.

```python
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # LR √ó 0.1 cada 30 √©pocas
```

---

## V

### Vanishing Gradients

Problema donde los gradientes se vuelven muy peque√±os durante backpropagation, impidiendo que las capas iniciales aprendan.

**Causas**: Inicializaci√≥n incorrecta, activaciones saturantes (sigmoid).

**Soluciones**: He initialization, ReLU, BatchNorm, skip connections.

---

## W

### Warmup

Per√≠odo inicial de entrenamiento donde el LR aumenta gradualmente desde un valor peque√±o.

**Prop√≥sito**: Estabilizar entrenamiento inicial, especialmente con batch sizes grandes.

### Weight Decay

T√©cnica de regularizaci√≥n que penaliza pesos grandes. En AdamW, se aplica directamente:

$$w = w - \lambda w$$

**Diferencia con L2**: Weight decay es independiente del gradiente.

---

## X

### Xavier Initialization (Glorot)

Inicializaci√≥n de pesos para activaciones lineales, tanh o sigmoid. Mantiene varianza considerando entrada y salida.

$$W \sim N(0, \sqrt{\frac{2}{n_{in} + n_{out}}})$$

```python
nn.init.xavier_normal_(layer.weight)
```

---

## F√≥rmulas Clave

### Update Rules

| Optimizador | F√≥rmula |
|-------------|---------|
| SGD | $w = w - \eta \nabla L$ |
| Momentum | $v = \beta v + \nabla L$; $w = w - \eta v$ |
| Adam | $m = \beta_1 m + (1-\beta_1)\nabla L$; $v = \beta_2 v + (1-\beta_2)(\nabla L)^2$ |

### Inicializaci√≥n

| M√©todo | Varianza |
|--------|----------|
| Xavier | $\frac{2}{n_{in} + n_{out}}$ |
| He | $\frac{2}{n_{in}}$ |

---

_Semana 27 - Optimizaci√≥n en Deep Learning_
