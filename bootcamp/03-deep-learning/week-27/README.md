# âš¡ Semana 27: OptimizaciÃ³n en Deep Learning

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar esta semana, serÃ¡s capaz de:

- âœ… Comprender cÃ³mo funcionan los optimizadores modernos
- âœ… Implementar learning rate schedules efectivos
- âœ… Usar callbacks para monitorear y controlar entrenamiento
- âœ… Aplicar tÃ©cnicas de inicializaciÃ³n de pesos
- âœ… Implementar gradient clipping para estabilidad

---

## ğŸ“š Requisitos Previos

- Semana 26: RegularizaciÃ³n completada
- Backpropagation y gradientes
- PyTorch bÃ¡sico

---

## ğŸ—‚ï¸ Estructura de la Semana

```
week-27/
â”œâ”€â”€ README.md
â”œâ”€â”€ rubrica-evaluacion.md
â”œâ”€â”€ 0-assets/
â”‚   â”œâ”€â”€ 01-optimizers-comparison.svg
â”‚   â”œâ”€â”€ 02-learning-rate-schedules.svg
â”‚   â”œâ”€â”€ 03-gradient-flow.svg
â”‚   â””â”€â”€ 04-callbacks-workflow.svg
â”œâ”€â”€ 1-teoria/
â”‚   â”œâ”€â”€ 01-optimizadores.md
â”‚   â”œâ”€â”€ 02-learning-rate-schedules.md
â”‚   â”œâ”€â”€ 03-inicializacion-pesos.md
â”‚   â””â”€â”€ 04-callbacks-checkpoints.md
â”œâ”€â”€ 2-practicas/
â”‚   â”œâ”€â”€ ejercicio-01-optimizers/
â”‚   â”œâ”€â”€ ejercicio-02-lr-schedules/
â”‚   â””â”€â”€ ejercicio-03-callbacks/
â”œâ”€â”€ 3-proyecto/
â”‚   â””â”€â”€ entrenador-optimizado/
â”œâ”€â”€ 4-recursos/
â”‚   â””â”€â”€ README.md
â””â”€â”€ 5-glosario/
    â””â”€â”€ README.md
```

---

## ğŸ“ Contenidos

### ğŸ“– TeorÃ­a (1.5 horas)

| # | Tema | Archivo | DuraciÃ³n |
|---|------|---------|----------|
| 1 | Optimizadores Modernos | [01-optimizadores.md](1-teoria/01-optimizadores.md) | 25 min |
| 2 | Learning Rate Schedules | [02-learning-rate-schedules.md](1-teoria/02-learning-rate-schedules.md) | 25 min |
| 3 | InicializaciÃ³n de Pesos | [03-inicializacion-pesos.md](1-teoria/03-inicializacion-pesos.md) | 20 min |
| 4 | Callbacks y Checkpoints | [04-callbacks-checkpoints.md](1-teoria/04-callbacks-checkpoints.md) | 20 min |

### ğŸ’» PrÃ¡cticas (2.5 horas)

| # | Ejercicio | Carpeta | DuraciÃ³n |
|---|-----------|---------|----------|
| 1 | Comparar Optimizadores | [ejercicio-01-optimizers/](2-practicas/ejercicio-01-optimizers/) | 50 min |
| 2 | LR Schedules | [ejercicio-02-lr-schedules/](2-practicas/ejercicio-02-lr-schedules/) | 50 min |
| 3 | Callbacks Custom | [ejercicio-03-callbacks/](2-practicas/ejercicio-03-callbacks/) | 50 min |

### ğŸ“¦ Proyecto (2 horas)

| Proyecto | DescripciÃ³n | Carpeta |
|----------|-------------|---------|
| Entrenador Optimizado | Pipeline completo con mejores prÃ¡cticas | [entrenador-optimizado/](3-proyecto/entrenador-optimizado/) |

---

## â±ï¸ DistribuciÃ³n del Tiempo

```
Total: 6 horas

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“– TeorÃ­a      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  1.5h (25%)  â”‚
â”‚  ğŸ’» PrÃ¡cticas   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.5h (42%)  â”‚
â”‚  ğŸ“¦ Proyecto    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.0h (33%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Conceptos Clave

| Concepto | DescripciÃ³n |
|----------|-------------|
| **SGD + Momentum** | Optimizador clÃ¡sico con aceleraciÃ³n |
| **Adam** | Adaptive moments, el mÃ¡s popular |
| **AdamW** | Adam con weight decay correcto |
| **StepLR** | Reduce LR cada N Ã©pocas |
| **CosineAnnealing** | LR decrece siguiendo coseno |
| **OneCycleLR** | Ciclo Ãºnico, warmup + decay |
| **Gradient Clipping** | Limita magnitud de gradientes |
| **Xavier/He Init** | InicializaciÃ³n inteligente de pesos |

---

## ğŸ“Œ Entregables

1. **Ejercicios completados** (2-practicas/)
   - [ ] ComparaciÃ³n de optimizadores con mÃ©tricas
   - [ ] LR schedules visualizados y comparados
   - [ ] Callbacks personalizados funcionando

2. **Proyecto** (3-proyecto/)
   - [ ] Pipeline de entrenamiento completo
   - [ ] Early stopping + checkpointing
   - [ ] LR schedule optimizado
   - [ ] Logging de mÃ©tricas

---

## ğŸ”— NavegaciÃ³n

| â¬…ï¸ Anterior | ğŸ  MÃ³dulo | Siguiente â¡ï¸ |
|-------------|-----------|--------------|
| [Semana 26](../week-26/README.md) | [Deep Learning](../README.md) | [Semana 28](../week-28/README.md) |

---

_Semana 27 de 36 | MÃ³dulo: Deep Learning | Bootcamp IA: Zero to Hero_
