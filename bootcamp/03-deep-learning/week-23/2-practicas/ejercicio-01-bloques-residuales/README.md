# Ejercicio 01: Implementaci√≥n de Bloques Residuales

## üéØ Objetivo

Implementar desde cero los bloques BasicBlock y Bottleneck de ResNet para comprender c√≥mo funcionan las conexiones residuales.

---

## üìã Conceptos Clave

- **Skip Connection**: Conexi√≥n que suma la entrada directamente a la salida
- **BasicBlock**: 2 convoluciones 3√ó3 (ResNet-18/34)
- **Bottleneck**: 1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1 (ResNet-50+)
- **Downsample**: Ajusta dimensiones cuando stride > 1

---

## üîß Paso 1: Configuraci√≥n del Entorno

Abre `starter/main.py` y ejecuta la primera secci√≥n para verificar las importaciones:

```python
import torch
import torch.nn as nn

# Verificar GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Dispositivo: {device}')
```

---

## üîß Paso 2: BasicBlock sin Skip Connection

Primero implementamos un bloque SIN conexi√≥n residual para ver la diferencia:

```python
class PlainBlock(nn.Module):
    """Bloque sin conexi√≥n residual."""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return self.relu(out)  # Sin skip connection
```

**Descomenta** la secci√≥n correspondiente en `starter/main.py`.

---

## üîß Paso 3: BasicBlock con Skip Connection

Ahora a√±adimos la conexi√≥n residual:

```python
class BasicBlock(nn.Module):
    """Bloque residual b√°sico (ResNet-18/34)."""
    expansion = 1
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
    
    def forward(self, x):
        identity = x
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity  # ¬°Skip connection!
        return self.relu(out)
```

**Descomenta** la secci√≥n correspondiente en `starter/main.py`.

---

## üîß Paso 4: Bottleneck Block

Implementamos el bloque m√°s eficiente para redes profundas:

```python
class Bottleneck(nn.Module):
    """Bloque bottleneck (ResNet-50/101/152)."""
    expansion = 4
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        # 1√ó1: Reducir canales
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 3√ó3: Procesamiento espacial
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 1√ó1: Expandir canales
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
    
    def forward(self, x):
        identity = x
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        return self.relu(out)
```

**Descomenta** la secci√≥n correspondiente en `starter/main.py`.

---

## üîß Paso 5: Comparar Par√°metros

Comparamos la eficiencia de cada bloque:

```python
def count_parameters(model):
    """Cuenta par√°metros entrenables."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# Crear bloques con 64 canales de entrada
plain = PlainBlock(64, 64)
basic = BasicBlock(64, 64)
bottleneck = Bottleneck(64, 64)

print(f'PlainBlock:  {count_parameters(plain):,} par√°metros')
print(f'BasicBlock:  {count_parameters(basic):,} par√°metros')
print(f'Bottleneck:  {count_parameters(bottleneck):,} par√°metros')
```

**Descomenta** la secci√≥n correspondiente en `starter/main.py`.

---

## üîß Paso 6: Verificar Flujo de Gradientes

Probamos que los gradientes fluyen correctamente:

```python
def test_gradient_flow(block, name):
    """Verifica que los gradientes fluyen a trav√©s del bloque."""
    x = torch.randn(1, 64, 32, 32, requires_grad=True)
    y = block(x)
    loss = y.sum()
    loss.backward()
    
    grad_norm = x.grad.norm().item()
    print(f'{name}: grad_norm = {grad_norm:.4f}')
    return grad_norm

# Probar cada bloque
test_gradient_flow(PlainBlock(64, 64), 'PlainBlock')
test_gradient_flow(BasicBlock(64, 64), 'BasicBlock')
```

**Descomenta** la secci√≥n correspondiente en `starter/main.py`.

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] PlainBlock implementado y funcionando
- [ ] BasicBlock con skip connection implementado
- [ ] Bottleneck block implementado
- [ ] Conteo de par√°metros correcto
- [ ] Gradientes fluyen a trav√©s de los bloques

---

## üìö Recursos

- [Deep Residual Learning (Paper)](https://arxiv.org/abs/1512.03385)
- [PyTorch ResNet Implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)
