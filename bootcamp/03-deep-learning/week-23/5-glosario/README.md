# üìñ Glosario - Semana 23: CNNs II

## B

### BasicBlock
Bloque residual b√°sico usado en ResNet-18 y ResNet-34. Consiste en dos convoluciones 3√ó3 con una skip connection.

```python
# Estructura: Conv3x3 ‚Üí BN ‚Üí ReLU ‚Üí Conv3x3 ‚Üí BN ‚Üí (+x) ‚Üí ReLU
```

### Backbone
Parte principal de una red neuronal que extrae caracter√≠sticas. En transfer learning, se refiere a las capas convolucionales preentrenadas.

### Bottleneck
Bloque residual eficiente usado en ResNet-50+. Usa convoluciones 1√ó1 para reducir y expandir canales.

```
1√ó1 (reducir) ‚Üí 3√ó3 (procesar) ‚Üí 1√ó1 (expandir)
```

---

## C

### Cosine Annealing
Estrategia de decaimiento del learning rate que sigue una curva coseno, permitiendo una reducci√≥n suave.

$$lr_t = lr_{min} + \frac{1}{2}(lr_{max} - lr_{min})(1 + \cos(\frac{t\pi}{T}))$$

---

## D

### Degradation Problem
Fen√≥meno donde redes m√°s profundas tienen **peor** rendimiento que redes menos profundas, incluso en el conjunto de entrenamiento. ResNet resuelve esto con skip connections.

### Discriminative Learning Rates
T√©cnica de fine-tuning donde diferentes capas usan diferentes learning rates. Capas profundas (cerca de entrada) usan LR m√°s bajo.

```python
# head: lr=1e-3, layer4: lr=1e-4, layer3: lr=1e-5, ...
```

### Downsample
Capa que ajusta las dimensiones de la skip connection cuando cambia el n√∫mero de canales o el tama√±o espacial.

---

## E

### Early Stopping
T√©cnica de regularizaci√≥n que detiene el entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar.

### Expansion
Factor por el cual el Bottleneck block expande los canales de salida. En ResNet, expansion=4.

### Exploding Gradients
Problema donde los gradientes crecen exponencialmente durante backpropagation, causando inestabilidad.

---

## F

### Feature Extraction
Estrategia de transfer learning donde se congela el backbone y solo se entrena el clasificador.

### Fine-tuning
Estrategia de transfer learning donde se entrenan algunas o todas las capas del modelo preentrenado con un learning rate bajo.

### Freeze/Unfreeze
Congelar (freeze) significa hacer que los pesos no se actualicen (`requires_grad=False`). Descongelar (unfreeze) permite que se entrenen.

---

## G

### Gradual Unfreezing
T√©cnica donde se descongelan capas progresivamente durante el entrenamiento, comenzando por las m√°s cercanas a la salida.

---

## I

### Identity Mapping
Funci√≥n identidad `f(x) = x`. Las skip connections permiten que la red aprenda esta funci√≥n f√°cilmente.

### ImageNet
Dataset de ~1.2 millones de im√°genes en 1000 categor√≠as. Base para la mayor√≠a de modelos preentrenados de visi√≥n.

---

## L

### Layer Groups
Agrupaci√≥n de capas para aplicar diferentes configuraciones (LR, freeze/unfreeze) durante fine-tuning.

### Learning Rate Warmup
T√©cnica donde el LR comienza bajo y aumenta gradualmente durante las primeras epochs para estabilizar el entrenamiento.

---

## P

### Pretrained Weights
Pesos de una red entrenada en un dataset grande (ej: ImageNet) que se reutilizan como punto de partida.

---

## R

### Residual Learning
Paradigma donde la red aprende la funci√≥n residual `F(x) = H(x) - x` en lugar de `H(x)` directamente, facilitando el aprendizaje de la identidad.

### ResNet (Residual Network)
Familia de arquitecturas que usan skip connections para permitir el entrenamiento de redes muy profundas (18, 34, 50, 101, 152 capas).

---

## S

### Skip Connection
Conexi√≥n que suma la entrada de un bloque a su salida: `y = F(x) + x`. Permite el flujo directo de gradientes.

### Stem
Primeras capas de una red (conv inicial, batch norm, pooling) antes de los bloques residuales.

---

## T

### Test Time Augmentation (TTA)
T√©cnica donde se aplican m√∫ltiples augmentations a una imagen durante inferencia y se promedian las predicciones.

### Transfer Learning
T√©cnica donde se reutiliza conocimiento de un modelo entrenado en un dataset grande para un nuevo problema relacionado.

---

## V

### Vanishing Gradients
Problema donde los gradientes se hacen exponencialmente peque√±os durante backpropagation, impidiendo que las primeras capas aprendan.

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial W_n} \cdot \prod_{i=1}^{n-1} \frac{\partial W_{i+1}}{\partial W_i} \approx 0$$

---

## W

### Warmup
Ver Learning Rate Warmup.

### Weight Decay
Regularizaci√≥n L2 que penaliza pesos grandes. Com√∫nmente usado junto con fine-tuning.

```python
optimizer = Adam(params, lr=1e-4, weight_decay=1e-4)
```
