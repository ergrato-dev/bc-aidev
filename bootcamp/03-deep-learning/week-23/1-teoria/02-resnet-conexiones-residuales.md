# ğŸ”— ResNet y Conexiones Residuales

## ğŸ¯ Objetivos

- Comprender la arquitectura ResNet en detalle
- Implementar bloques BasicBlock y Bottleneck
- Entender las variantes de ResNet (18, 34, 50, 101, 152)

---

## 1. La Idea Fundamental

### Aprendizaje Residual

En lugar de aprender la transformaciÃ³n directa $H(x)$, la red aprende el **residuo**:

$$F(x) = H(x) - x$$

Y la salida se calcula como:

$$y = F(x) + x$$

### Â¿Por QuÃ© "Residual"?

$F(x)$ representa lo que **falta agregar** a la entrada para obtener la salida deseada.

```
Si la transformaciÃ³n Ã³ptima es identidad:
  H(x) = x
  F(x) = H(x) - x = 0  â† Â¡FÃ¡cil de aprender!
```

---

## 2. BasicBlock (ResNet-18/34)

![Bloque Residual](../0-assets/01-bloque-residual.svg)

### Arquitectura

```
         x
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚Conv 3Ã—3â”‚     â”‚
â”‚  BN    â”‚     â”‚
â”‚ ReLU   â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚Conv 3Ã—3â”‚     â”‚
â”‚  BN    â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚         â”‚
    â–¼         â”‚
   (+)â—„â”€â”€â”€â”€â”€â”€â”€â”˜  Skip Connection
    â”‚
   ReLU
    â”‚
    â–¼
   out
```

### ImplementaciÃ³n PyTorch

```python
import torch.nn as nn

class BasicBlock(nn.Module):
    """Bloque residual bÃ¡sico para ResNet-18/34."""
    
    expansion = 1  # No cambia canales
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        
        # Primera convoluciÃ³n
        self.conv1 = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # Segunda convoluciÃ³n
        self.conv2 = nn.Conv2d(
            out_channels, out_channels,
            kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
    
    def forward(self, x):
        identity = x
        
        # Rama principal
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        # Skip connection
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity  # Â¡La magia!
        out = self.relu(out)
        
        return out
```

---

## 3. Bottleneck (ResNet-50/101/152)

![BasicBlock vs Bottleneck](../0-assets/02-basicblock-bottleneck.svg)

### Â¿Por QuÃ© Bottleneck?

Para redes muy profundas, BasicBlock es costoso. Bottleneck reduce cÃ³mputo:

```
BasicBlock: 3Ã—3Ã—64Ã—64 + 3Ã—3Ã—64Ã—64 = 73,728 params

Bottleneck: 1Ã—1Ã—256Ã—64 + 3Ã—3Ã—64Ã—64 + 1Ã—1Ã—64Ã—256 = 69,632 params
            (reduce)    (procesa)    (expande)
            
Â¡Menos parÃ¡metros con mÃ¡s profundidad!
```

### Arquitectura

```
           x (256 ch)
           â”‚
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
      â”‚         â”‚
      â–¼         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚
  â”‚Conv 1Ã—1â”‚     â”‚  â† Reduce: 256 â†’ 64
  â”‚  BN    â”‚     â”‚
  â”‚ ReLU   â”‚     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
  â”‚Conv 3Ã—3â”‚     â”‚  â† Procesa: 64 â†’ 64
  â”‚  BN    â”‚     â”‚
  â”‚ ReLU   â”‚     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
  â”‚Conv 1Ã—1â”‚     â”‚  â† Expande: 64 â†’ 256
  â”‚  BN    â”‚     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
      â”‚         â”‚
      â–¼         â”‚
     (+)â—„â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
    ReLU
      â”‚
      â–¼
     out (256 ch)
```

### ImplementaciÃ³n PyTorch

```python
class Bottleneck(nn.Module):
    """Bloque bottleneck para ResNet-50/101/152."""
    
    expansion = 4  # Salida = 4Ã— canales internos
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        
        # 1Ã—1 reduce
        self.conv1 = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 3Ã—3 procesa
        self.conv2 = nn.Conv2d(
            out_channels, out_channels,
            kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 1Ã—1 expande
        self.conv3 = nn.Conv2d(
            out_channels, out_channels * self.expansion,
            kernel_size=1, bias=False
        )
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
    
    def forward(self, x):
        identity = x
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        return out
```

---

## 4. Downsample: Cuando las Dimensiones No Coinciden

### El Problema

Skip connection suma $x + F(x)$. Pero si cambian dimensiones:

```
x:    [batch, 64, 56, 56]
F(x): [batch, 128, 28, 28]  â† stride=2

Â¡No se pueden sumar!
```

### La SoluciÃ³n

ProyecciÃ³n 1Ã—1 con stride:

```python
downsample = nn.Sequential(
    nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),
    nn.BatchNorm2d(128)
)

# Ahora:
# x:          [batch, 64, 56, 56]
# downsample: [batch, 128, 28, 28]  âœ“
# F(x):       [batch, 128, 28, 28]  âœ“
```

---

## 5. Variantes de ResNet

### ConfiguraciÃ³n por Variante

| Variante | Bloque | Capas por Stage | Total Capas | Params |
|----------|--------|-----------------|-------------|--------|
| ResNet-18 | BasicBlock | [2, 2, 2, 2] | 18 | 11.7M |
| ResNet-34 | BasicBlock | [3, 4, 6, 3] | 34 | 21.8M |
| ResNet-50 | Bottleneck | [3, 4, 6, 3] | 50 | 25.6M |
| ResNet-101 | Bottleneck | [3, 4, 23, 3] | 101 | 44.5M |
| ResNet-152 | Bottleneck | [3, 8, 36, 3] | 152 | 60.2M |

### Estructura General

```
Input (224Ã—224Ã—3)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv 7Ã—7, 64   â”‚  â† Stem
â”‚  MaxPool 3Ã—3    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (56Ã—56Ã—64)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1       â”‚  â† 64 canales
â”‚   N bloques     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (56Ã—56Ã—64)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 2       â”‚  â† 128 canales, stride=2
â”‚   N bloques     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (28Ã—28Ã—128)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 3       â”‚  â† 256 canales, stride=2
â”‚   N bloques     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (14Ã—14Ã—256)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 4       â”‚  â† 512 canales, stride=2
â”‚   N bloques     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (7Ã—7Ã—512)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Global AvgPool  â”‚
â”‚   FC â†’ 1000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
  Output
```

---

## 6. ResNet Completa

```python
class ResNet(nn.Module):
    """Arquitectura ResNet completa."""
    
    def __init__(self, block, layers, num_classes=1000):
        super().__init__()
        self.in_channels = 64
        
        # Stem
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Stages
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # Clasificador
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
    
    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion)
            )
        
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x


# Constructores
def resnet18(num_classes=1000):
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)

def resnet50(num_classes=1000):
    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)
```

---

## 7. Usar ResNet Preentrenada

```python
from torchvision import models

# Cargar con pesos de ImageNet
model = models.resnet50(weights='IMAGENET1K_V2')

# Ver arquitectura
print(model)

# Contar parÃ¡metros
total = sum(p.numel() for p in model.parameters())
print(f"ParÃ¡metros: {total:,}")  # ~25.6M
```

---

## âœ… Resumen

| Componente | FunciÃ³n |
|------------|---------|
| Skip Connection | Permite flujo de gradiente directo |
| BasicBlock | 2 convs 3Ã—3, para ResNet-18/34 |
| Bottleneck | 1Ã—1â†’3Ã—3â†’1Ã—1, para ResNet-50+ |
| Downsample | Ajusta dimensiones cuando hay stride |

---

## ğŸ”— NavegaciÃ³n

[â† Problema Profundidad](01-problema-profundidad.md) | [Siguiente: Transfer Learning â†’](03-transfer-learning.md)
