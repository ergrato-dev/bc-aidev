# ğŸ“‰ El Problema de la Profundidad en Redes Neuronales

## ğŸ¯ Objetivos

- Comprender por quÃ© las redes muy profundas fallan
- Entender vanishing/exploding gradients
- Conocer el problema de degradaciÃ³n

---

## 1. La Promesa de las Redes Profundas

### IntuiciÃ³n

MÃ¡s capas = mÃ¡s capacidad de aprender representaciones complejas.

```
Red Superficial (3 capas):
Input â†’ [Conv] â†’ [Conv] â†’ [Conv] â†’ Output
         â†“        â†“        â†“
      Bordes   Texturas  Formas

Red Profunda (20+ capas):
Input â†’ [...muchas capas...] â†’ Output
         â†“
      CaracterÃ­sticas muy abstractas
      (ojos, caras, objetos completos)
```

### El Problema Real

En la prÃ¡ctica, agregar mÃ¡s capas **no siempre mejora** el rendimiento.

```
Accuracy vs Profundidad (sin ResNet):

100% â”¤
 95% â”¤         â—â”€â”€â”€â—
 90% â”¤    â—â”€â”€â”€â—     â•²
 85% â”¤   â•±            â•²
 80% â”¤  â—              â—â”€â”€â”€â—
 75% â”¤ â•±                    â•²
 70% â”¼â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
     â””â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬
       8  16  20  32  56  110 152
                Capas
```

---

## 2. Vanishing Gradient

### Â¿QuÃ© es?

Durante backpropagation, los gradientes se multiplican capa por capa. Si son < 1, se vuelven exponencialmente pequeÃ±os.

### MatemÃ¡ticamente

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial a_n} \cdot \frac{\partial a_n}{\partial a_{n-1}} \cdot ... \cdot \frac{\partial a_2}{\partial W_1}$$

Si cada tÃ©rmino $|\frac{\partial a_i}{\partial a_{i-1}}| < 1$:

$$\text{gradiente} \approx (0.5)^{20} = 0.00000095$$

### VisualizaciÃ³n

```
Capa 20:  Gradiente = 1.0        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Capa 15:  Gradiente = 0.1        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Capa 10:  Gradiente = 0.01       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Capa 5:   Gradiente = 0.001      â–ˆâ–ˆ
Capa 1:   Gradiente = 0.0001     â–ª  (casi cero!)
```

### Consecuencias

- Las primeras capas **no aprenden**
- El modelo se comporta como una red superficial
- Entrenar se vuelve imposible

---

## 3. Exploding Gradient

### El Problema Opuesto

Si los gradientes son > 1, crecen exponencialmente.

$$\text{gradiente} \approx (2.0)^{20} = 1,048,576$$

### SÃ­ntomas

```python
# Durante entrenamiento verÃ¡s:
Epoch 1: Loss = 2.34
Epoch 2: Loss = 15.67
Epoch 3: Loss = nan  # Â¡ExplotÃ³!
```

### Soluciones Comunes

```python
# 1. Gradient Clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 2. InicializaciÃ³n cuidadosa (He, Xavier)
nn.init.kaiming_normal_(layer.weight, mode='fan_out')

# 3. Batch Normalization
nn.BatchNorm2d(num_features)
```

---

## 4. El Problema de DegradaciÃ³n

### Descubrimiento (He et al., 2015)

Incluso con BatchNorm y buena inicializaciÃ³n, redes mÃ¡s profundas tienen **peor** accuracy que redes menos profundas.

> "Esto no es overfitting (training error tambiÃ©n es peor)"

### Experimento Original

| Modelo | Train Error | Test Error |
|--------|-------------|------------|
| 20 capas | 5.5% | 8.2% |
| 56 capas | 8.3% | 9.6% |

**Â¿Por quÃ©?** La red de 56 capas deberÃ­a al menos igualar a la de 20 (podrÃ­a aprender identidad en capas extra).

### La Paradoja

```
Si una red de 20 capas es Ã³ptima:
- Red de 56 capas podrÃ­a copiar las 20 primeras
- Y hacer identidad en las 36 restantes: y = x
- Pero esto NO ocurre naturalmente
```

---

## 5. Por QuÃ© la Identidad es DifÃ­cil

### Aprender y = x

Parece simple, pero para una red neuronal es sorprendentemente difÃ­cil:

```python
# Una capa intenta aprender: y = x
# Tiene que aprender:
W = [[1, 0, 0, ...],
     [0, 1, 0, ...],
     [0, 0, 1, ...]]
b = [0, 0, 0, ...]

# Esto requiere ajuste preciso de muchos parÃ¡metros
```

### Con ReLU

```
y = ReLU(Wx + b)

Para que y = x:
- W debe ser identidad
- b debe ser 0
- Y x debe ser > 0 (por ReLU)

Â¡Muy restrictivo!
```

---

## 6. La SoluciÃ³n: Conexiones Residuales

### Idea Clave de ResNet

En lugar de aprender $H(x)$, aprende el **residuo** $F(x) = H(x) - x$

```
Tradicional:           Residual:
    x                      x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      â”‚              â”‚
    â–¼                      â–¼              â”‚
 â”Œâ”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”           â”‚
 â”‚  H  â”‚                â”‚  F  â”‚           â”‚
 â””â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”˜           â”‚
    â”‚                      â”‚              â”‚
    â–¼                      â–¼              â”‚
   H(x)                   F(x) + x â—„â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿Por QuÃ© Funciona?

**Si la transformaciÃ³n Ã³ptima es identidad:**

- Tradicional: Aprender $H(x) = x$ (difÃ­cil)
- Residual: Aprender $F(x) = 0$ (Â¡fÃ¡cil! solo poner pesos en 0)

### Gradientes

```
Tradicional:
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚H Ã— âˆ‚H/âˆ‚x    (puede â†’ 0)

Residual:
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚(F+x) Ã— (âˆ‚F/âˆ‚x + 1)
      = âˆ‚L/âˆ‚(F+x) Ã— âˆ‚F/âˆ‚x + âˆ‚L/âˆ‚(F+x)
                             â†‘
                    Â¡Siempre hay gradiente directo!
```

---

## 7. Comparativa: Con y Sin Residuales

### Sin Skip Connections (VGG-style)

```
Profundidad  | Train Acc | Test Acc
-------------|-----------|----------
    18       |   95.0%   |  92.0%
    34       |   93.5%   |  90.2%   â† Peor
    50       |   91.8%   |  88.5%   â† AÃºn peor
```

### Con Skip Connections (ResNet)

```
Profundidad  | Train Acc | Test Acc
-------------|-----------|----------
    18       |   95.5%   |  93.0%
    34       |   96.0%   |  94.5%   â† Mejor
    50       |   96.5%   |  95.2%   â† AÃºn mejor
   152       |   97.0%   |  95.8%   â† Â¡Funciona!
```

---

## 8. CÃ³digo: Verificar el Problema

```python
import torch
import torch.nn as nn

def check_gradient_flow(model, input_shape):
    """Verifica si hay vanishing gradient."""
    x = torch.randn(1, *input_shape, requires_grad=True)
    y = model(x)
    loss = y.sum()
    loss.backward()
    
    print("Gradientes por capa:")
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            status = "âœ“" if grad_norm > 1e-6 else "âœ— VANISHING"
            print(f"  {name}: {grad_norm:.2e} {status}")

# Ejemplo con red profunda sin residuales
class DeepCNN(nn.Module):
    def __init__(self, num_layers=20):
        super().__init__()
        layers = []
        for i in range(num_layers):
            in_ch = 3 if i == 0 else 64
            layers.append(nn.Conv2d(in_ch, 64, 3, padding=1))
            layers.append(nn.ReLU())
        self.features = nn.Sequential(*layers)
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.features(x)
        x = x.mean(dim=[2, 3])  # Global Average Pool
        return self.fc(x)

# Verificar
model = DeepCNN(num_layers=50)
check_gradient_flow(model, (3, 32, 32))
# VerÃ¡s gradientes muy pequeÃ±os en primeras capas
```

---

## âœ… Resumen

| Problema | Causa | SoluciÃ³n |
|----------|-------|----------|
| Vanishing Gradient | MultiplicaciÃ³n de gradientes < 1 | Skip connections, BatchNorm |
| Exploding Gradient | MultiplicaciÃ³n de gradientes > 1 | Gradient clipping, inicializaciÃ³n |
| DegradaciÃ³n | Dificultad para aprender identidad | Conexiones residuales |

**PrÃ³ximo**: ImplementaciÃ³n de ResNet y bloques residuales.

---

## ğŸ”— NavegaciÃ³n

[â† README](../README.md) | [Siguiente: ResNet â†’](02-resnet-conexiones-residuales.md)
