# ğŸ”„ Semana 24: Redes Neuronales Recurrentes (RNNs)

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar esta semana, serÃ¡s capaz de:

- âœ… Comprender la arquitectura de redes recurrentes
- âœ… Entender el problema del vanishing gradient en secuencias
- âœ… Implementar celdas LSTM y GRU desde cero
- âœ… Construir modelos para procesamiento de secuencias
- âœ… Aplicar RNNs a predicciÃ³n de series temporales
- âœ… Usar capas bidireccionales y stacked RNNs

---

## ğŸ“š Requisitos Previos

- Semana 19-21: Fundamentos de redes neuronales
- Semana 22-23: CNNs (conceptos de arquitecturas)
- Python con PyTorch
- Ãlgebra lineal (multiplicaciÃ³n de matrices)

---

## ğŸ—‚ï¸ Estructura de la Semana

```
week-24/
â”œâ”€â”€ README.md
â”œâ”€â”€ rubrica-evaluacion.md
â”œâ”€â”€ 0-assets/
â”‚   â”œâ”€â”€ 01-rnn-desplegada.svg
â”‚   â”œâ”€â”€ 02-lstm-celda.svg
â”‚   â”œâ”€â”€ 03-gru-celda.svg
â”‚   â””â”€â”€ 04-bidirectional-rnn.svg
â”œâ”€â”€ 1-teoria/
â”‚   â”œâ”€â”€ 01-introduccion-rnns.md
â”‚   â”œâ”€â”€ 02-problema-secuencias-largas.md
â”‚   â”œâ”€â”€ 03-lstm-memoria-largo-plazo.md
â”‚   â””â”€â”€ 04-gru-simplificacion.md
â”œâ”€â”€ 2-practicas/
â”‚   â”œâ”€â”€ ejercicio-01-rnn-basica/
â”‚   â”œâ”€â”€ ejercicio-02-lstm-gru/
â”‚   â””â”€â”€ ejercicio-03-series-temporales/
â”œâ”€â”€ 3-proyecto/
â”‚   â””â”€â”€ predictor-temperatura/
â”œâ”€â”€ 4-recursos/
â”‚   â””â”€â”€ README.md
â””â”€â”€ 5-glosario/
    â””â”€â”€ README.md
```

---

## ğŸ“ Contenidos

### ğŸ“– TeorÃ­a (1.5 horas)

| # | Tema | Archivo | DuraciÃ³n |
|---|------|---------|----------|
| 1 | IntroducciÃ³n a RNNs | [01-introduccion-rnns.md](1-teoria/01-introduccion-rnns.md) | 25 min |
| 2 | Problema de Secuencias Largas | [02-problema-secuencias-largas.md](1-teoria/02-problema-secuencias-largas.md) | 20 min |
| 3 | LSTM: Memoria a Largo Plazo | [03-lstm-memoria-largo-plazo.md](1-teoria/03-lstm-memoria-largo-plazo.md) | 25 min |
| 4 | GRU: SimplificaciÃ³n Efectiva | [04-gru-simplificacion.md](1-teoria/04-gru-simplificacion.md) | 20 min |

### ğŸ’» PrÃ¡cticas (2.5 horas)

| # | Ejercicio | Carpeta | DuraciÃ³n |
|---|-----------|---------|----------|
| 1 | RNN BÃ¡sica desde Cero | [ejercicio-01-rnn-basica/](2-practicas/ejercicio-01-rnn-basica/) | 45 min |
| 2 | LSTM y GRU en PyTorch | [ejercicio-02-lstm-gru/](2-practicas/ejercicio-02-lstm-gru/) | 50 min |
| 3 | Series Temporales | [ejercicio-03-series-temporales/](2-practicas/ejercicio-03-series-temporales/) | 55 min |

### ğŸ“¦ Proyecto (2 horas)

| Proyecto | DescripciÃ³n | Carpeta |
|----------|-------------|---------|
| Predictor de Temperatura | Predecir temperatura usando datos histÃ³ricos con LSTM | [predictor-temperatura/](3-proyecto/predictor-temperatura/) |

---

## â±ï¸ DistribuciÃ³n del Tiempo

```
Total: 6 horas

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“– TeorÃ­a      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  1.5h (25%)  â”‚
â”‚  ğŸ’» PrÃ¡cticas   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.5h (42%)  â”‚
â”‚  ğŸ“¦ Proyecto    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  2.0h (33%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Conceptos Clave

### RNN Vanilla
```
h_t = tanh(W_hh Â· h_{t-1} + W_xh Â· x_t + b)
```

### LSTM (4 gates)
- **Forget gate**: QuÃ© olvidar del estado de celda
- **Input gate**: QuÃ© nueva informaciÃ³n aÃ±adir
- **Cell state**: Memoria a largo plazo
- **Output gate**: QuÃ© parte del estado mostrar

### GRU (2 gates)
- **Reset gate**: CuÃ¡nto del pasado olvidar
- **Update gate**: Balance entre pasado y presente

---

## ğŸ“Œ Entregables

Al finalizar la semana debes entregar:

1. **Ejercicios completados** (2-practicas/)
   - [ ] ejercicio-01: RNN bÃ¡sica implementada
   - [ ] ejercicio-02: LSTM y GRU funcionando
   - [ ] ejercicio-03: PredicciÃ³n de serie temporal

2. **Proyecto semanal** (3-proyecto/)
   - [ ] Predictor de temperatura con LSTM
   - [ ] MAE < 2Â°C en predicciones
   - [ ] VisualizaciÃ³n de predicciones vs real

3. **AutoevaluaciÃ³n**
   - [ ] Completar checklist de verificaciÃ³n
   - [ ] Explicar diferencias entre RNN, LSTM, GRU

---

## ğŸ”— NavegaciÃ³n

| â¬…ï¸ Anterior | ğŸ  MÃ³dulo | Siguiente â¡ï¸ |
|-------------|-----------|--------------|
| [Semana 23: CNNs II](../week-23/) | [Deep Learning](../README.md) | [Semana 25: Transformers](../week-25/) |

---

## ğŸ’¡ Tips para esta Semana

> ğŸ¯ **Consejo**: Las RNNs procesan secuencias paso a paso. Visualiza mentalmente cÃ³mo la informaciÃ³n fluye a travÃ©s del tiempo para entender mejor la arquitectura.

- **Empieza simple**: Comprende RNN vanilla antes de LSTM/GRU
- **Dibuja los diagramas**: Las puertas de LSTM son mÃ¡s fÃ¡ciles de entender visualmente
- **Practica con secuencias cortas**: Antes de series temporales largas
- **Compara arquitecturas**: Entrena RNN, LSTM y GRU en el mismo problema

---

_Semana 24 de 36 | MÃ³dulo: Deep Learning | Bootcamp IA: Zero to Hero_
