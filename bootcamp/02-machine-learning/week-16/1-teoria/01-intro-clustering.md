# üìö Introducci√≥n al Clustering

## üéØ Objetivos

- Entender qu√© es el aprendizaje no supervisado
- Conocer las aplicaciones del clustering
- Diferenciar tipos de algoritmos de clustering

---

## 1. Aprendizaje Supervisado vs No Supervisado

### Supervisado
- **Datos**: Features (X) + Etiquetas (y)
- **Objetivo**: Predecir etiquetas para nuevos datos
- **Ejemplos**: Clasificaci√≥n, Regresi√≥n

### No Supervisado
- **Datos**: Solo features (X), sin etiquetas
- **Objetivo**: Encontrar estructura/patrones ocultos
- **Ejemplos**: Clustering, Reducci√≥n dimensional

```python
# Supervisado: tenemos las etiquetas
X_train, y_train  # y_train = [0, 1, 1, 0, ...]

# No supervisado: solo features
X_data  # Sin etiquetas, buscamos estructura
```

---

## 2. ¬øQu√© es Clustering?

**Clustering** es la tarea de agrupar objetos similares en grupos llamados **clusters**.

### Intuici√≥n
- Puntos dentro del mismo cluster son **similares** entre s√≠
- Puntos en diferentes clusters son **diferentes** entre s√≠

![Clustering Overview](../0-assets/01-clustering-overview.svg)

### Caracter√≠sticas
- No hay "respuesta correcta" predefinida
- Es exploratorio: descubrimos estructura
- La calidad depende de c√≥mo definimos "similitud"

---

## 3. Aplicaciones del Clustering

| Dominio | Aplicaci√≥n |
|---------|------------|
| **Marketing** | Segmentaci√≥n de clientes |
| **Biolog√≠a** | Agrupaci√≥n de genes, taxonom√≠a |
| **Documentos** | Organizaci√≥n de noticias, temas |
| **Im√°genes** | Compresi√≥n, segmentaci√≥n |
| **Anomal√≠as** | Detecci√≥n de fraude, outliers |
| **Redes** | Detecci√≥n de comunidades |

### Ejemplo: Segmentaci√≥n de Clientes

```python
# Datos de clientes (sin etiquetas)
# Features: edad, ingresos, gastos, frecuencia de compra

# Clustering encuentra grupos naturales:
# - Cluster 0: J√≥venes alto consumo digital
# - Cluster 1: Familias tradicionales
# - Cluster 2: Seniors bajo consumo
# - Cluster 3: Profesionales alto ingreso
```

---

## 4. Tipos de Algoritmos de Clustering

### 4.1 Basados en Partici√≥n
- Dividen datos en K grupos no superpuestos
- **Ejemplos**: K-Means, K-Medoids
- **Caracter√≠stica**: Requieren especificar K

### 4.2 Basados en Densidad
- Grupos = regiones de alta densidad
- **Ejemplos**: DBSCAN, OPTICS, HDBSCAN
- **Caracter√≠stica**: Detectan outliers, formas arbitrarias

### 4.3 Jer√°rquicos
- Crean jerarqu√≠a de clusters (dendrograma)
- **Ejemplos**: Aglomerativo, Divisivo
- **Caracter√≠stica**: No requieren K a priori

### 4.4 Basados en Modelo
- Asumen distribuci√≥n de probabilidad
- **Ejemplos**: Gaussian Mixture Models (GMM)
- **Caracter√≠stica**: Soft clustering (probabilidades)

---

## 5. Medidas de Distancia

El clustering depende de c√≥mo medimos "similitud" (distancia):

### Distancia Euclidiana
```python
import numpy as np

def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# Ejemplo
a = np.array([1, 2])
b = np.array([4, 6])
print(euclidean_distance(a, b))  # 5.0
```

### Otras Distancias
```python
from scipy.spatial.distance import cdist

# Manhattan (L1)
dist_manhattan = cdist(X, Y, metric='cityblock')

# Coseno (para texto, vectores dispersos)
dist_cosine = cdist(X, Y, metric='cosine')

# Correlaci√≥n
dist_corr = cdist(X, Y, metric='correlation')
```

### ‚ö†Ô∏è Importancia del Escalado

```python
from sklearn.preprocessing import StandardScaler

# SIEMPRE escalar antes de clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Sin escalar: features con mayor magnitud dominan
# Con escalar: todas las features contribuyen igual
```

---

## 6. Scikit-learn: Clustering API

```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering

# Todos siguen el mismo patr√≥n:
# 1. Crear modelo
model = KMeans(n_clusters=3, random_state=42)

# 2. Ajustar a los datos
model.fit(X_scaled)

# 3. Obtener etiquetas de cluster
labels = model.labels_

# O en un solo paso:
labels = model.fit_predict(X_scaled)
```

---

## 7. Visualizaci√≥n B√°sica

```python
import matplotlib.pyplot as plt

def plot_clusters(X, labels, title="Clusters"):
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.colorbar(scatter, label='Cluster')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.show()

# Uso
plot_clusters(X_scaled, labels, "K-Means Clustering")
```

---

## ‚úÖ Checklist de Aprendizaje

- [ ] Entiendo la diferencia entre supervisado y no supervisado
- [ ] Conozco qu√© es clustering y sus aplicaciones
- [ ] Identifico los tipos principales de algoritmos
- [ ] Comprendo la importancia del escalado
- [ ] S√© usar la API b√°sica de sklearn para clustering

---

## üîó Referencias

- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [Introduction to Clustering - Google ML](https://developers.google.com/machine-learning/clustering)
