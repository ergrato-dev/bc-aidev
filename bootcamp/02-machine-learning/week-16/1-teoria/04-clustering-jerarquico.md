# üìö Clustering Jer√°rquico

## üéØ Objetivos

- Entender el clustering jer√°rquico aglomerativo
- Interpretar dendrogramas
- Conocer los m√©todos de enlace (linkage)
- Implementar con scikit-learn y scipy

---

## 1. ¬øQu√© es Clustering Jer√°rquico?

Crea una **jerarqu√≠a de clusters** representada como un √°rbol (dendrograma).

![Dendrograma](../0-assets/04-dendrograma.svg)

### Dos Enfoques

| Tipo | Descripci√≥n |
|------|-------------|
| **Aglomerativo** (bottom-up) | Empieza con N clusters, fusiona hasta 1 |
| **Divisivo** (top-down) | Empieza con 1 cluster, divide hasta N |

> En la pr√°ctica, el **aglomerativo** es el m√°s usado.

---

## 2. Algoritmo Aglomerativo

### Pasos

1. Cada punto es un cluster individual
2. Calcular distancia entre todos los pares de clusters
3. Fusionar los 2 clusters m√°s cercanos
4. Repetir hasta tener 1 cluster
5. Cortar el dendrograma para obtener K clusters

### Complejidad
- Tiempo: O(n¬≥) para el b√°sico, O(n¬≤ log n) optimizado
- Espacio: O(n¬≤) para matriz de distancias

---

## 3. M√©todos de Enlace (Linkage)

Define c√≥mo medir la distancia entre clusters.

### Single Linkage (Minimum)
```
dist(A, B) = min{d(a, b) : a ‚àà A, b ‚àà B}
```
- Distancia entre los puntos **m√°s cercanos**
- Puede crear cadenas largas ("chaining effect")

### Complete Linkage (Maximum)
```
dist(A, B) = max{d(a, b) : a ‚àà A, b ‚àà B}
```
- Distancia entre los puntos **m√°s lejanos**
- Clusters m√°s compactos

### Average Linkage
```
dist(A, B) = mean{d(a, b) : a ‚àà A, b ‚àà B}
```
- **Promedio** de todas las distancias
- Balance entre single y complete

### Ward Linkage ‚≠ê (M√°s Usado)
```
Minimiza la varianza intra-cluster al fusionar
```
- Produce clusters de **tama√±o similar**
- Tiende a clusters **esf√©ricos y compactos**

```python
# Comparar linkages
linkages = ['single', 'complete', 'average', 'ward']
```

---

## 4. Dendrogramas con Scipy

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

# Generar datos
X, _ = make_blobs(n_samples=50, centers=3, cluster_std=1.0, random_state=42)
X_scaled = StandardScaler().fit_transform(X)

# Calcular linkage matrix
Z = linkage(X_scaled, method='ward')

# Visualizar dendrograma
plt.figure(figsize=(14, 6))
dendrogram(Z, 
           truncate_mode='lastp',  # Mostrar √∫ltimos p clusters
           p=12,                    # N√∫mero de hojas
           leaf_rotation=90,
           leaf_font_size=10,
           show_contracted=True)
plt.title('Dendrograma Jer√°rquico (Ward Linkage)')
plt.xlabel('√çndice de muestra o (tama√±o del cluster)')
plt.ylabel('Distancia')
plt.axhline(y=10, color='r', linestyle='--', label='Corte K=3')
plt.legend()
plt.show()
```

---

## 5. Obtener Clusters del Dendrograma

```python
from scipy.cluster.hierarchy import fcluster

# M√©todo 1: N√∫mero de clusters
labels_k = fcluster(Z, t=3, criterion='maxclust')  # K=3 clusters

# M√©todo 2: Distancia de corte
labels_d = fcluster(Z, t=10, criterion='distance')  # Cortar en distancia 10

# M√©todo 3: Inconsistency (autom√°tico)
labels_i = fcluster(Z, t=1.5, criterion='inconsistent')

print(f"Clusters (K=3): {np.unique(labels_k)}")
print(f"Tama√±os: {np.bincount(labels_k)}")
```

---

## 6. Clustering Jer√°rquico con Scikit-learn

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# Crear modelo
agg_clustering = AgglomerativeClustering(
    n_clusters=3,           # N√∫mero de clusters deseado
    metric='euclidean',     # M√©trica de distancia
    linkage='ward'          # M√©todo de enlace
)

# Ajustar y predecir
labels = agg_clustering.fit_predict(X_scaled)

# Visualizar
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=50)
plt.title(f'Clustering Jer√°rquico Aglomerativo (K=3, Ward)')
plt.colorbar(label='Cluster')
plt.show()
```

---

## 7. Comparar M√©todos de Linkage

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

linkages = ['ward', 'complete', 'average', 'single']

fig, axes = plt.subplots(1, 4, figsize=(16, 4))

for ax, linkage_method in zip(axes, linkages):
    # Ward requiere euclidean
    metric = 'euclidean' if linkage_method == 'ward' else 'euclidean'
    
    agg = AgglomerativeClustering(
        n_clusters=3, 
        linkage=linkage_method,
        metric=metric
    )
    labels = agg.fit_predict(X_scaled)
    
    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=30)
    ax.set_title(f'Linkage: {linkage_method}')

plt.suptitle('Comparaci√≥n de M√©todos de Enlace')
plt.tight_layout()
plt.show()
```

---

## 8. Determinar N√∫mero de Clusters

### M√©todo Visual: Dendrograma
```python
# Buscar el "salto" m√°s grande en distancias
# Cortar justo antes del salto grande
```

### M√©todo de Inconsistency
```python
from scipy.cluster.hierarchy import inconsistent

depth = 5  # Profundidad de an√°lisis
incons = inconsistent(Z, d=depth)
print("Inconsistency statistics:")
print(incons[-10:])  # √öltimas fusiones
```

### Usando Silhouette
```python
from sklearn.metrics import silhouette_score

silhouette_scores = []
K_range = range(2, 10)

for k in K_range:
    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = agg.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores.append(score)

plt.plot(K_range, silhouette_scores, 'bo-')
plt.xlabel('N√∫mero de Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette para Clustering Jer√°rquico')
plt.show()
```

---

## 9. Ventajas y Desventajas

### ‚úÖ Ventajas
- No requiere especificar K a priori
- Dendrograma visualiza estructura completa
- Flexible con diferentes linkages
- Produce clusters anidados

### ‚ùå Desventajas
- Costoso computacionalmente O(n¬≤) - O(n¬≥)
- Sensible a outliers (especialmente single)
- Decisiones de fusi√≥n son irreversibles
- No escala bien a datasets muy grandes

---

## 10. Ejemplo Completo

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Datos
X, y_true = make_blobs(n_samples=150, centers=4, cluster_std=0.8, random_state=42)
X_scaled = StandardScaler().fit_transform(X)

# Dendrograma
Z = linkage(X_scaled, method='ward')

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# 1. Dendrograma
dendrogram(Z, ax=axes[0], truncate_mode='lastp', p=20, 
           leaf_rotation=90, leaf_font_size=8)
axes[0].axhline(y=8, color='r', linestyle='--')
axes[0].set_title('Dendrograma (Ward)')

# 2. Datos originales
axes[1].scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.6)
axes[1].set_title('Datos Originales')

# 3. Clustering
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = agg.fit_predict(X_scaled)
axes[2].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')
axes[2].set_title(f'Aglomerativo K=4 (Silhouette: {silhouette_score(X_scaled, labels):.3f})')

plt.tight_layout()
plt.show()
```

---

## ‚úÖ Checklist de Aprendizaje

- [ ] Entiendo la diferencia entre aglomerativo y divisivo
- [ ] Puedo interpretar un dendrograma
- [ ] Conozco los 4 m√©todos de linkage principales
- [ ] S√© cu√°ndo usar ward vs otros linkages
- [ ] Puedo extraer clusters de un dendrograma

---

## üîó Referencias

- [AgglomerativeClustering - sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
- [Scipy Hierarchical Clustering](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)
