# √Årboles de Decisi√≥n: Fundamentos

## üéØ Objetivos

- Entender la estructura de un √°rbol de decisi√≥n
- Comprender c√≥mo se realizan las predicciones
- Conocer el algoritmo CART
- Identificar ventajas y limitaciones

## üìã Contenido

### 1. ¬øQu√© es un √Årbol de Decisi√≥n?

Un √°rbol de decisi√≥n es un modelo de ML que toma decisiones secuenciales basadas en preguntas sobre las features, similar a un diagrama de flujo.

![Estructura del √Årbol](../0-assets/01-arbol-decision-estructura.svg)

### 2. Componentes del √Årbol

| Componente         | Descripci√≥n                                     |
| ------------------ | ----------------------------------------------- |
| **Nodo Ra√≠z**      | Primera divisi√≥n, usa la feature m√°s importante |
| **Nodos Internos** | Divisiones intermedias                          |
| **Nodos Hoja**     | Predicci√≥n final (clase o valor)                |
| **Rama**           | Conexi√≥n entre nodos (condici√≥n)                |
| **Profundidad**    | N√∫mero de niveles del √°rbol                     |

### 3. Algoritmo CART

**CART** (Classification And Regression Trees) es el algoritmo que usa scikit-learn:

```python
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

# Clasificaci√≥n
clf = DecisionTreeClassifier(max_depth=5, random_state=42)
clf.fit(X_train, y_train)

# Regresi√≥n
reg = DecisionTreeRegressor(max_depth=5, random_state=42)
reg.fit(X_train, y_train)
```

### 4. ¬øC√≥mo Decide D√≥nde Dividir?

En cada nodo, el algoritmo:

1. **Eval√∫a todas las features** disponibles
2. **Prueba todos los posibles umbrales** (thresholds)
3. **Selecciona la divisi√≥n** que maximiza la "pureza" de los nodos hijos
4. **Repite recursivamente** hasta cumplir criterio de parada

```python
# Ejemplo de divisi√≥n
# Si feature "edad" con threshold 30:
# - Izquierda: muestras donde edad <= 30
# - Derecha: muestras donde edad > 30
```

### 5. Predicci√≥n en √Årboles

#### Clasificaci√≥n

```python
# Recorre el √°rbol hasta llegar a una hoja
# Retorna la clase mayoritaria en esa hoja
y_pred = clf.predict(X_test)

# Tambi√©n puede dar probabilidades
y_proba = clf.predict_proba(X_test)
# Devuelve proporci√≥n de cada clase en la hoja
```

#### Regresi√≥n

```python
# Retorna el valor promedio de las muestras en la hoja
y_pred = reg.predict(X_test)
```

### 6. Ventajas de los √Årboles

| Ventaja                     | Descripci√≥n                         |
| --------------------------- | ----------------------------------- |
| ‚úÖ **Interpretabilidad**    | F√°cil de visualizar y explicar      |
| ‚úÖ **No requiere escalado** | Funciona con datos sin normalizar   |
| ‚úÖ **Maneja mixtos**        | Features num√©ricas y categ√≥ricas    |
| ‚úÖ **No lineal**            | Captura relaciones complejas        |
| ‚úÖ **Feature importance**   | Indica qu√© features son importantes |

### 7. Limitaciones

| Limitaci√≥n                                     | Soluci√≥n                                |
| ---------------------------------------------- | --------------------------------------- |
| ‚ö†Ô∏è **Overfitting**                             | Limitar profundidad, poda               |
| ‚ö†Ô∏è **Inestabilidad**                           | Peque√±os cambios ‚Üí √°rbol diferente      |
| ‚ö†Ô∏è **Sesgo hacia features con muchos valores** | Usar Gini en lugar de Entropy           |
| ‚ö†Ô∏è **No extrapola**                            | Solo predice valores vistos en training |

### 8. Visualizaci√≥n del √Årbol

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Visualizar √°rbol
plt.figure(figsize=(20, 10))
plot_tree(
    clf,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.savefig('arbol_decision.png', dpi=150, bbox_inches='tight')
plt.show()
```

### 9. Ejemplo Completo

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# Dividir
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entrenar √°rbol (limitando profundidad)
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

# Evaluar
y_pred = tree.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

# Ver importancia de features
for name, imp in zip(iris.feature_names, tree.feature_importances_):
    print(f"{name}: {imp:.4f}")
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la estructura: ra√≠z, nodos internos, hojas
- [ ] S√© c√≥mo el √°rbol hace predicciones
- [ ] Conozco las ventajas de interpretabilidad
- [ ] Identifico el riesgo de overfitting
- [ ] Puedo visualizar un √°rbol con sklearn

---

## üìö Recursos

- [Decision Trees - sklearn](https://scikit-learn.org/stable/modules/tree.html)
- [Visualizing Decision Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)
