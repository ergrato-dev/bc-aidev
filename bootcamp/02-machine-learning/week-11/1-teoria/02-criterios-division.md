# Criterios de Divisi√≥n: Gini vs Entropy

## üéØ Objetivos

- Entender Gini Impurity y c√≥mo se calcula
- Comprender Entropy e Information Gain
- Comparar ambos criterios
- Saber cu√°ndo usar cada uno

## üìã Contenido

### 1. ¬øPor Qu√© Necesitamos Criterios?

El √°rbol debe decidir **qu√© feature y qu√© threshold** usar para dividir. Los criterios miden la **"pureza"** de los nodos resultantes.

- **Nodo puro**: Todas las muestras son de una sola clase
- **Nodo impuro**: Mezcla de clases

![Gini vs Entropy](../0-assets/02-gini-entropy.svg)

### 2. Gini Impurity

Mide la probabilidad de clasificar incorrectamente una muestra aleatoria.

#### F√≥rmula

$$Gini = 1 - \sum_{i=1}^{C} p_i^2$$

Donde $p_i$ es la proporci√≥n de muestras de clase $i$.

#### Propiedades

| Caracter√≠stica  | Valor         |
| --------------- | ------------- |
| Rango (binario) | [0, 0.5]      |
| Nodo puro       | 0             |
| M√°xima impureza | 0.5 (50/50)   |
| Complejidad     | O(C) - r√°pido |

#### Ejemplo de C√°lculo

```python
# Nodo con 60 muestras clase A, 40 muestras clase B
p_A = 60 / 100  # 0.6
p_B = 40 / 100  # 0.4

gini = 1 - (p_A**2 + p_B**2)
gini = 1 - (0.36 + 0.16)
gini = 0.48
```

### 3. Entropy (Entrop√≠a)

Basada en teor√≠a de la informaci√≥n, mide el desorden o incertidumbre.

#### F√≥rmula

$$Entropy = -\sum_{i=1}^{C} p_i \log_2(p_i)$$

#### Propiedades

| Caracter√≠stica       | Valor                |
| -------------------- | -------------------- |
| Rango (binario)      | [0, 1]               |
| Nodo puro            | 0                    |
| M√°xima incertidumbre | 1 (50/50)            |
| Complejidad          | O(C¬∑log) - m√°s lento |

#### Ejemplo de C√°lculo

```python
import numpy as np

# Mismo nodo: 60% A, 40% B
p_A, p_B = 0.6, 0.4

entropy = -(p_A * np.log2(p_A) + p_B * np.log2(p_B))
entropy = -(0.6 * (-0.737) + 0.4 * (-1.322))
entropy = 0.971
```

### 4. Information Gain

Mide **cu√°nto reduce la entrop√≠a** una divisi√≥n.

#### F√≥rmula

$$IG = Entropy(padre) - \sum_{j} \frac{n_j}{n} \cdot Entropy(hijo_j)$$

#### Ejemplo

```python
# Nodo padre: 100 muestras (60 A, 40 B)
entropy_padre = 0.971

# Divisi√≥n produce:
# - Hijo izquierdo: 50 muestras (45 A, 5 B)
# - Hijo derecho: 50 muestras (15 A, 35 B)

# Entrop√≠a hijos
p_A_izq, p_B_izq = 0.9, 0.1
entropy_izq = -(0.9 * np.log2(0.9) + 0.1 * np.log2(0.1))  # ‚âà 0.469

p_A_der, p_B_der = 0.3, 0.7
entropy_der = -(0.3 * np.log2(0.3) + 0.7 * np.log2(0.7))  # ‚âà 0.881

# Information Gain
ig = entropy_padre - (0.5 * entropy_izq + 0.5 * entropy_der)
ig = 0.971 - (0.5 * 0.469 + 0.5 * 0.881)
ig = 0.296
```

### 5. Comparaci√≥n Gini vs Entropy

| Aspecto             | Gini          | Entropy            |
| ------------------- | ------------- | ------------------ |
| **Velocidad**       | ‚úÖ M√°s r√°pido | M√°s lento (log)    |
| **Default sklearn** | ‚úÖ S√≠         | No                 |
| **Interpretaci√≥n**  | Prob. error   | Teor√≠a informaci√≥n |
| **Rango**           | [0, 0.5]      | [0, 1]             |
| **Resultados**      | Similares     | Similares          |

### 6. Uso en Scikit-learn

```python
from sklearn.tree import DecisionTreeClassifier

# Usando Gini (default)
tree_gini = DecisionTreeClassifier(criterion='gini')

# Usando Entropy
tree_entropy = DecisionTreeClassifier(criterion='entropy')

# Para regresi√≥n: MSE o MAE
from sklearn.tree import DecisionTreeRegressor

tree_mse = DecisionTreeRegressor(criterion='squared_error')  # default
tree_mae = DecisionTreeRegressor(criterion='absolute_error')
```

### 7. ¬øCu√°l Elegir?

En la pr√°ctica, **ambos dan resultados muy similares**.

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score

iris = load_iris()
X, y = iris.data, iris.target

# Comparar
for criterion in ['gini', 'entropy']:
    tree = DecisionTreeClassifier(criterion=criterion, max_depth=4, random_state=42)
    scores = cross_val_score(tree, X, y, cv=5)
    print(f"{criterion}: {scores.mean():.4f} ¬± {scores.std():.4f}")
```

**Recomendaci√≥n**: Usa Gini (default) a menos que tengas raz√≥n espec√≠fica para Entropy.

### 8. Visualizar Impureza

```python
import numpy as np
import matplotlib.pyplot as plt

p = np.linspace(0.01, 0.99, 100)

# Gini
gini = 1 - p**2 - (1-p)**2

# Entropy
entropy = -p * np.log2(p) - (1-p) * np.log2(1-p)

plt.figure(figsize=(10, 6))
plt.plot(p, gini, label='Gini', linewidth=2)
plt.plot(p, entropy, label='Entropy', linewidth=2)
plt.xlabel('P(clase = 1)')
plt.ylabel('Impureza')
plt.title('Gini vs Entropy')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('gini_vs_entropy.png', dpi=150)
plt.show()
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] S√© calcular Gini Impurity manualmente
- [ ] Entiendo Entropy y su base en teor√≠a de informaci√≥n
- [ ] Puedo calcular Information Gain
- [ ] Conozco las diferencias pr√°cticas entre Gini y Entropy
- [ ] S√© cambiar el criterio en sklearn

---

## üìö Recursos

- [Decision Tree - Mathematical Formulation](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation)
- [Gini vs Entropy - Towards Data Science](https://towardsdatascience.com/gini-impurity-vs-entropy-in-decision-trees-3a62ff79e917)
