# RegresiÃ³n LogÃ­stica

## ğŸ¯ Objetivos

- Entender por quÃ© regresiÃ³n lineal no sirve para clasificaciÃ³n
- Comprender la funciÃ³n sigmoide
- Implementar regresiÃ³n logÃ­stica para clasificaciÃ³n binaria
- Interpretar coeficientes como odds ratios

## ğŸ“– De RegresiÃ³n a ClasificaciÃ³n

A pesar de su nombre, la **regresiÃ³n logÃ­stica** es un algoritmo de **clasificaciÃ³n**, no de regresiÃ³n.

### Â¿Por quÃ© no usar regresiÃ³n lineal para clasificaciÃ³n?

Si usamos regresiÃ³n lineal para predecir clases (0 o 1):

- Puede predecir valores < 0 o > 1
- No tiene sentido probabilÃ­stico
- Sensible a outliers

**SoluciÃ³n**: Usar una funciÃ³n que mapee cualquier valor a un rango [0, 1].

## ğŸ“Š FunciÃ³n Sigmoide

La **funciÃ³n sigmoide** (o logÃ­stica) transforma cualquier nÃºmero real en un valor entre 0 y 1:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

![FunciÃ³n Sigmoide](../0-assets/04-sigmoide.svg)

### Propiedades

- **Rango**: $(0, 1)$ - interpreta como probabilidad
- **Centro**: $\sigma(0) = 0.5$
- **AsÃ­ntotas**: $\lim_{z \to -\infty} \sigma(z) = 0$, $\lim_{z \to +\infty} \sigma(z) = 1$
- **Derivada**: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

### En RegresiÃ³n LogÃ­stica

Combinamos la regresiÃ³n lineal con la sigmoide:

$$P(y=1|x) = \sigma(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)$$

$$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}}$$

## ğŸ¯ Decision Boundary

La **frontera de decisiÃ³n** es donde $P(y=1|x) = 0.5$:

$$\beta_0 + \beta_1 x_1 + ... + \beta_n x_n = 0$$

![Decision Boundary](../0-assets/05-decision-boundary.svg)

### Regla de ClasificaciÃ³n

```python
if P(y=1|x) >= 0.5:
    clase = 1
else:
    clase = 0
```

## ğŸ’» ImplementaciÃ³n con Scikit-learn

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import load_breast_cancer

# Cargar dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Crear y entrenar modelo
modelo = LogisticRegression(max_iter=1000)
modelo.fit(X_train, y_train)

# Predicciones
y_pred = modelo.predict(X_test)

# EvaluaciÃ³n
print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')
print('\nClassification Report:')
print(classification_report(y_test, y_pred, target_names=data.target_names))
```

## ğŸ“Š Probabilidades

RegresiÃ³n logÃ­stica no solo predice clases, sino **probabilidades**:

```python
# Probabilidades de cada clase
proba = modelo.predict_proba(X_test)

print('Primeras 5 predicciones:')
print('P(clase 0)  P(clase 1)  PredicciÃ³n  Real')
print('-' * 45)
for i in range(5):
    print(f'{proba[i][0]:.3f}       {proba[i][1]:.3f}        {y_pred[i]}           {y_test[i]}')
```

**Salida**:

```
P(clase 0)  P(clase 1)  PredicciÃ³n  Real
---------------------------------------------
0.002       0.998        1           1
0.987       0.013        0           0
0.001       0.999        1           1
...
```

## ğŸ” InterpretaciÃ³n de Coeficientes

### Odds y Log-Odds

La regresiÃ³n logÃ­stica modela el **log-odds** (logaritmo de las odds):

$$\log\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 x_1 + ...$$

### InterpretaciÃ³n de Î²â±¼

$\beta_j$ representa el cambio en **log-odds** por cada unidad de cambio en $x_j$.

Para interpretaciÃ³n mÃ¡s intuitiva, usamos el **odds ratio**:

$$OR_j = e^{\beta_j}$$

```python
import numpy as np

# Ver coeficientes para las primeras 5 features
print('Feature              Coef (Î²)    Odds Ratio')
print('-' * 50)
for name, coef in zip(data.feature_names[:5], modelo.coef_[0][:5]):
    odds_ratio = np.exp(coef)
    print(f'{name:20} {coef:8.3f}    {odds_ratio:.3f}')
```

### InterpretaciÃ³n del Odds Ratio

| Odds Ratio | InterpretaciÃ³n                        |
| ---------- | ------------------------------------- |
| OR = 1     | Sin efecto                            |
| OR > 1     | Aumenta la probabilidad de clase 1    |
| OR < 1     | Disminuye la probabilidad de clase 1  |
| OR = 2     | Duplica las odds de clase 1           |
| OR = 0.5   | Reduce a la mitad las odds de clase 1 |

## âš™ï¸ HiperparÃ¡metros Importantes

### ParÃ¡metro C (RegularizaciÃ³n)

`C` controla la regularizaciÃ³n (inverso de la fuerza):

```python
# C grande = menos regularizaciÃ³n
modelo_high_c = LogisticRegression(C=10, max_iter=1000)

# C pequeÃ±o = mÃ¡s regularizaciÃ³n
modelo_low_c = LogisticRegression(C=0.1, max_iter=1000)
```

### Solver

Diferentes algoritmos de optimizaciÃ³n:

```python
# Para datasets pequeÃ±os
LogisticRegression(solver='lbfgs')

# Para datasets grandes
LogisticRegression(solver='sag')

# Para regularizaciÃ³n L1 (Lasso)
LogisticRegression(solver='saga', penalty='l1')
```

## ğŸ“Š ClasificaciÃ³n Multiclase

RegresiÃ³n logÃ­stica puede extenderse a mÃºltiples clases:

```python
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target

modelo_multi = LogisticRegression(max_iter=1000, multi_class='multinomial')
modelo_multi.fit(X, y)

# Probabilidades para 3 clases
proba = modelo_multi.predict_proba(X[:3])
print('Probabilidades (setosa, versicolor, virginica):')
print(proba)
```

## ğŸ†š RegresiÃ³n Lineal vs LogÃ­stica

| Aspecto     | RegresiÃ³n Lineal | RegresiÃ³n LogÃ­stica |
| ----------- | ---------------- | ------------------- |
| **Tarea**   | RegresiÃ³n        | ClasificaciÃ³n       |
| **Output**  | Valor continuo   | Probabilidad [0,1]  |
| **Target**  | Continuo         | CategÃ³rico          |
| **FunciÃ³n** | Identidad        | Sigmoide            |
| **Costo**   | MSE              | Log-loss            |

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo por quÃ© usamos sigmoide para clasificaciÃ³n
- [ ] Puedo interpretar probabilidades y decision boundary
- [ ] SÃ© usar LogisticRegression de sklearn
- [ ] Puedo interpretar coeficientes como odds ratios
- [ ] Conozco la diferencia entre regresiÃ³n lineal y logÃ­stica

## ğŸ”— Recursos Adicionales

- [Sklearn LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)
