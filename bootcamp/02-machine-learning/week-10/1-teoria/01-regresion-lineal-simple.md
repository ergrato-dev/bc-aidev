# RegresiÃ³n Lineal Simple

## ğŸ¯ Objetivos

- Comprender quÃ© es la regresiÃ³n lineal simple
- Entender la ecuaciÃ³n de la recta y sus componentes
- Interpretar coeficientes (intercepto y pendiente)
- Conocer el mÃ©todo de mÃ­nimos cuadrados

## ğŸ“– Â¿QuÃ© es la RegresiÃ³n Lineal?

La **regresiÃ³n lineal** es el algoritmo de Machine Learning mÃ¡s fundamental. Modela la relaciÃ³n entre una variable dependiente (target) y una o mÃ¡s variables independientes (features) mediante una lÃ­nea recta.

### RegresiÃ³n Lineal Simple

Cuando tenemos **una sola feature**, hablamos de regresiÃ³n lineal simple:

$$\hat{y} = \beta_0 + \beta_1 x$$

Donde:

- $\hat{y}$: valor predicho
- $\beta_0$: intercepto (valor de y cuando x = 0)
- $\beta_1$: pendiente (cambio en y por cada unidad de x)
- $x$: feature de entrada

![RegresiÃ³n Lineal Simple](../0-assets/01-regresion-lineal.svg)

## ğŸ“Š InterpretaciÃ³n de Coeficientes

### Intercepto (Î²â‚€)

El **intercepto** representa el valor de $y$ cuando $x = 0$.

**Ejemplo**: Si predecimos el precio de una casa basÃ¡ndonos en metros cuadrados:

- $\beta_0 = 50,000$ significa que una casa de 0 mÂ² tendrÃ­a un precio base de $50,000
- Aunque no tiene sentido prÃ¡ctico, representa el "precio base"

### Pendiente (Î²â‚)

La **pendiente** indica cuÃ¡nto cambia $y$ por cada unidad de cambio en $x$.

**Ejemplo**:

- $\beta_1 = 1,500$ significa que por cada mÂ² adicional, el precio aumenta $1,500

```python
# Si Î²â‚€ = 50,000 y Î²â‚ = 1,500
# Casa de 100 mÂ²:
precio = 50000 + 1500 * 100  # = $200,000
```

## ğŸ”¢ MÃ©todo de MÃ­nimos Cuadrados (OLS)

### Â¿CÃ³mo encontramos la mejor lÃ­nea?

Buscamos los valores de $\beta_0$ y $\beta_1$ que **minimicen el error** entre los valores reales y predichos.

### FunciÃ³n de Costo (RSS - Residual Sum of Squares)

$$J(\beta_0, \beta_1) = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2$$

### SoluciÃ³n AnalÃ­tica

Para regresiÃ³n lineal simple, existe una soluciÃ³n exacta:

$$\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$

$$\beta_0 = \bar{y} - \beta_1\bar{x}$$

Donde $\bar{x}$ y $\bar{y}$ son las medias de x e y.

## ğŸ’» ImplementaciÃ³n con Scikit-learn

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# Datos de ejemplo
X = np.array([[100], [150], [200], [250], [300]])  # metros cuadrados
y = np.array([200000, 275000, 350000, 425000, 500000])  # precios

# Crear y entrenar el modelo
modelo = LinearRegression()
modelo.fit(X, y)

# Ver coeficientes
print(f'Intercepto (Î²â‚€): {modelo.intercept_}')
print(f'Pendiente (Î²â‚): {modelo.coef_[0]}')

# Predecir
casa_nueva = np.array([[175]])
precio_predicho = modelo.predict(casa_nueva)
print(f'Precio predicho para 175 mÂ²: ${precio_predicho[0]:,.0f}')
```

**Salida esperada**:

```
Intercepto (Î²â‚€): 50000.0
Pendiente (Î²â‚): 1500.0
Precio predicho para 175 mÂ²: $312,500
```

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

### RÂ² (Coeficiente de DeterminaciÃ³n)

Mide quÃ© proporciÃ³n de la varianza en $y$ es explicada por el modelo.

$$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2} = 1 - \frac{RSS}{TSS}$$

**InterpretaciÃ³n**:

- $R^2 = 1$: PredicciÃ³n perfecta
- $R^2 = 0$: El modelo no explica nada
- $R^2 = 0.85$: El modelo explica el 85% de la varianza

```python
from sklearn.metrics import r2_score

r2 = modelo.score(X, y)
# o
r2 = r2_score(y, modelo.predict(X))
print(f'RÂ²: {r2:.4f}')
```

### Error CuadrÃ¡tico Medio (MSE)

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

### RaÃ­z del Error CuadrÃ¡tico Medio (RMSE)

$$RMSE = \sqrt{MSE}$$

```python
from sklearn.metrics import mean_squared_error
import numpy as np

y_pred = modelo.predict(X)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)

print(f'MSE: {mse:,.0f}')
print(f'RMSE: {rmse:,.0f}')
```

### Error Absoluto Medio (MAE)

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y, y_pred)
print(f'MAE: {mae:,.0f}')
```

## âš ï¸ Supuestos de la RegresiÃ³n Lineal

Para que los resultados sean vÃ¡lidos, se asumen:

1. **Linealidad**: La relaciÃ³n entre X e y es lineal
2. **Independencia**: Los errores son independientes entre sÃ­
3. **Homocedasticidad**: Varianza constante de los errores
4. **Normalidad**: Los errores siguen distribuciÃ³n normal

## ğŸ¯ CuÃ¡ndo Usar RegresiÃ³n Lineal

âœ… **Usar cuando**:

- RelaciÃ³n aproximadamente lineal entre variables
- Variable target es continua
- Necesitas interpretabilidad

âŒ **No usar cuando**:

- RelaciÃ³n claramente no lineal
- Variable target es categÃ³rica (usar clasificaciÃ³n)
- Hay muchos outliers

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo la ecuaciÃ³n y = Î²â‚€ + Î²â‚x
- [ ] Puedo interpretar intercepto y pendiente
- [ ] SÃ© calcular e interpretar RÂ², MSE, RMSE, MAE
- [ ] Puedo implementar LinearRegression con sklearn
- [ ] Conozco los supuestos del modelo

## ğŸ”— Recursos Adicionales

- [Sklearn LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [StatQuest: Linear Regression](https://www.youtube.com/watch?v=7ArmBVF2dCs)
