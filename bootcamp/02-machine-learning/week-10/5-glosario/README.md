# ðŸ“– Glosario - Semana 10: RegresiÃ³n Lineal y LogÃ­stica

## A

### Accuracy (Exactitud)

ProporciÃ³n de predicciones correctas sobre el total. En clasificaciÃ³n: (TP + TN) / (TP + TN + FP + FN).

### Alpha (Î±)

HiperparÃ¡metro de regularizaciÃ³n en Ridge y Lasso. Valores mayores = mÃ¡s regularizaciÃ³n.

## B

### Bias (Sesgo)

En regresiÃ³n lineal, es el tÃ©rmino independiente (intercepto Î²â‚€). TambiÃ©n se refiere al error sistemÃ¡tico de un modelo.

### Binary Classification (ClasificaciÃ³n Binaria)

Problema de clasificaciÃ³n con solo dos clases posibles (0/1, SÃ­/No, Positivo/Negativo).

## C

### Coefficient (Coeficiente)

En regresiÃ³n lineal, los valores Î² que multiplican a cada feature. Representan el cambio en y por unidad de cambio en x.

### Confusion Matrix (Matriz de ConfusiÃ³n)

Tabla que muestra TP, TN, FP, FN para evaluar clasificaciÃ³n:

```
              Predicho
              0    1
Real    0   [TN] [FP]
        1   [FN] [TP]
```

### Cross-Validation (ValidaciÃ³n Cruzada)

TÃ©cnica para evaluar modelos dividiendo datos en k folds, entrenando en k-1 y validando en 1, rotando.

## D

### Decision Boundary (Frontera de DecisiÃ³n)

En regresiÃ³n logÃ­stica, la lÃ­nea/superficie donde P(y=1) = 0.5. Separa las clases.

## E

### Elastic Net

RegularizaciÃ³n que combina L1 (Lasso) y L2 (Ridge):
$$J = MSE + \alpha \cdot \rho \cdot ||w||_1 + \alpha \cdot (1-\rho) \cdot ||w||_2^2$$

## F

### F1-Score

Media armÃ³nica de precision y recall:
$$F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$$

### False Negative (FN)

PredicciÃ³n de clase 0 cuando la clase real es 1 (error tipo II).

### False Positive (FP)

PredicciÃ³n de clase 1 cuando la clase real es 0 (error tipo I).

### Feature (CaracterÃ­stica)

Variable independiente usada para hacer predicciones. En regresiÃ³n: las x.

### Feature Selection

Proceso de seleccionar las features mÃ¡s relevantes. Lasso lo hace automÃ¡ticamente.

## G

### Gradient Descent (Descenso de Gradiente)

Algoritmo de optimizaciÃ³n que minimiza la funciÃ³n de costo iterativamente siguiendo el gradiente negativo.

## H

### Hyperparameter (HiperparÃ¡metro)

ParÃ¡metro que no se aprende del datos sino que se define antes del entrenamiento (ej: Î± en Ridge).

## I

### Intercept (Intercepto)

TÃ©rmino independiente Î²â‚€ en regresiÃ³n lineal. Valor de y cuando todas las x son 0.

## L

### L1 Regularization (RegularizaciÃ³n L1)

PenalizaciÃ³n basada en la suma de valores absolutos de coeficientes. Usada en Lasso.
$$Penalty = \lambda \sum |w_i|$$

### L2 Regularization (RegularizaciÃ³n L2)

PenalizaciÃ³n basada en la suma de cuadrados de coeficientes. Usada en Ridge.
$$Penalty = \lambda \sum w_i^2$$

### Lasso (Least Absolute Shrinkage and Selection Operator)

RegresiÃ³n lineal con regularizaciÃ³n L1. Puede hacer coeficientes exactamente 0.

### Learning Rate (Tasa de Aprendizaje)

En gradient descent, el tamaÃ±o del paso en cada iteraciÃ³n.

### Linear Regression (RegresiÃ³n Lineal)

Modelo que asume relaciÃ³n lineal entre features y target:
$$y = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$$

### Log-Loss (Binary Cross-Entropy)

FunciÃ³n de costo para regresiÃ³n logÃ­stica:
$$J = -\frac{1}{n}\sum[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$$

### Logistic Regression (RegresiÃ³n LogÃ­stica)

Modelo de clasificaciÃ³n que predice probabilidades usando funciÃ³n sigmoide.

## M

### MAE (Mean Absolute Error)

Error absoluto promedio:
$$MAE = \frac{1}{n}\sum|y_i - \hat{y}_i|$$

### MSE (Mean Squared Error)

Error cuadrÃ¡tico medio:
$$MSE = \frac{1}{n}\sum(y_i - \hat{y}_i)^2$$

### Multicollinearity (Multicolinealidad)

CorrelaciÃ³n alta entre features. Causa coeficientes inestables en regresiÃ³n lineal.

### Multiple Regression (RegresiÃ³n MÃºltiple)

RegresiÃ³n lineal con mÃ¡s de una feature independiente.

## N

### Normalization (NormalizaciÃ³n)

Escalar datos a un rango especÃ­fico (tÃ­picamente [0,1]).

## O

### OLS (Ordinary Least Squares)

MÃ©todo de mÃ­nimos cuadrados ordinarios para ajustar regresiÃ³n lineal minimizando MSE.

### Overfitting (Sobreajuste)

Modelo que memoriza datos de entrenamiento y no generaliza bien a datos nuevos.

## P

### Precision (PrecisiÃ³n)

De las predicciones positivas, cuÃ¡ntas son correctas:
$$Precision = \frac{TP}{TP + FP}$$

### Prediction (PredicciÃ³n)

Valor estimado por el modelo (Å·).

### Probability (Probabilidad)

En regresiÃ³n logÃ­stica, P(y=1|x) devuelto por predict_proba().

## R

### RÂ² (R-squared, Coeficiente de DeterminaciÃ³n)

ProporciÃ³n de varianza explicada por el modelo:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

### Recall (Sensibilidad, TPR)

De los positivos reales, cuÃ¡ntos se detectaron:
$$Recall = \frac{TP}{TP + FN}$$

### Regularization (RegularizaciÃ³n)

TÃ©cnica para prevenir overfitting aÃ±adiendo penalizaciÃ³n a coeficientes grandes.

### Residual (Residuo)

Diferencia entre valor real y predicho: $e_i = y_i - \hat{y}_i$

### Ridge Regression

RegresiÃ³n lineal con regularizaciÃ³n L2. Reduce coeficientes pero no los hace 0.

### RMSE (Root Mean Squared Error)

RaÃ­z del error cuadrÃ¡tico medio:
$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$$

## S

### Sigmoid Function (FunciÃ³n Sigmoide)

FunciÃ³n que mapea cualquier valor a [0,1]:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

### Simple Linear Regression (RegresiÃ³n Lineal Simple)

RegresiÃ³n con una sola feature: $y = \beta_0 + \beta_1 x$

### Slope (Pendiente)

Coeficiente Î²â‚ en regresiÃ³n simple. Indica cambio en y por unidad de x.

### StandardScaler

TransformaciÃ³n que centra datos (media=0) y escala (desviaciÃ³n=1):
$$z = \frac{x - \mu}{\sigma}$$

## T

### Target (Variable Objetivo)

Variable que queremos predecir (y).

### Threshold (Umbral)

En clasificaciÃ³n, valor de probabilidad para decidir clase. Default: 0.5.

### True Negative (TN)

PredicciÃ³n correcta de clase 0.

### True Positive (TP)

PredicciÃ³n correcta de clase 1.

## U

### Underfitting (Subajuste)

Modelo demasiado simple que no captura patrones en los datos.

## V

### VIF (Variance Inflation Factor)

MÃ©trica para detectar multicolinealidad. VIF > 5 indica problemas:
$$VIF_i = \frac{1}{1 - R_i^2}$$

## W

### Weight (Peso)

SinÃ³nimo de coeficiente en el contexto de redes neuronales.

---

## ðŸ“Š FÃ³rmulas Clave Resumidas

| MÃ©trica   | FÃ³rmula                           |
| --------- | --------------------------------- |
| MSE       | $\frac{1}{n}\sum(y - \hat{y})^2$  |
| RMSE      | $\sqrt{MSE}$                      |
| MAE       | $\frac{1}{n}\sum\|y - \hat{y}\|$  |
| RÂ²        | $1 - \frac{SS_{res}}{SS_{tot}}$   |
| Sigmoid   | $\frac{1}{1 + e^{-z}}$            |
| Precision | $\frac{TP}{TP + FP}$              |
| Recall    | $\frac{TP}{TP + FN}$              |
| F1        | $\frac{2 \cdot P \cdot R}{P + R}$ |
