# ğŸ”® IntroducciÃ³n al Clustering

## ğŸ¯ Objetivos de Aprendizaje

- Comprender el paradigma del aprendizaje no supervisado
- Diferenciar clustering de clasificaciÃ³n
- Identificar aplicaciones reales del clustering
- Conocer los tipos principales de algoritmos de clustering

---

## ğŸ“‹ Â¿QuÃ© es el Aprendizaje No Supervisado?

### Supervisado vs No Supervisado

En **aprendizaje supervisado** tenemos:

- Datos de entrada (features) **X**
- Etiquetas conocidas (labels) **y**
- Objetivo: aprender la relaciÃ³n X â†’ y

En **aprendizaje no supervisado**:

- Solo tenemos datos de entrada **X**
- **No hay etiquetas**
- Objetivo: descubrir estructura oculta en los datos

```python
# Supervisado: tenemos etiquetas
X_train = [[1.2, 3.4], [2.1, 1.8], [5.2, 7.1]]
y_train = ['gato', 'gato', 'perro']  # â† etiquetas conocidas

# No supervisado: solo datos
X = [[1.2, 3.4], [2.1, 1.8], [5.2, 7.1], [4.8, 6.9]]
# No hay y - queremos descubrir grupos naturales
```

### Â¿QuÃ© es Clustering?

**Clustering** (agrupamiento) es la tarea de agrupar objetos similares en conjuntos llamados **clusters**, de manera que:

- Objetos **dentro** del mismo cluster sean **similares** entre sÃ­
- Objetos en **diferentes** clusters sean **distintos** entre sÃ­

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚    â—â—â—                    â–²â–²â–²                               â”‚
â”‚   â—â—â—â—â—                  â–²â–²â–²â–²â–²                â– â– â–            â”‚
â”‚    â—â—â—                    â–²â–²â–²                â– â– â– â– â–           â”‚
â”‚                                               â– â– â–            â”‚
â”‚  Cluster 1              Cluster 2          Cluster 3        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ Aplicaciones del Clustering

### 1. SegmentaciÃ³n de Clientes

```python
# Agrupar clientes por comportamiento de compra
customer_data = {
    'frecuencia_compras': [12, 2, 15, 1, 8],
    'gasto_promedio': [500, 50, 600, 30, 400],
    'antiguedad_meses': [24, 3, 36, 1, 18]
}
# Resultado: "VIP", "Ocasional", "Nuevo"
```

### 2. CompresiÃ³n de ImÃ¡genes

Reducir colores agrupando pÃ­xeles similares:

```python
from sklearn.cluster import KMeans
import numpy as np

# Imagen con millones de colores â†’ reducir a 16
pixels = image.reshape(-1, 3)  # RGB values
kmeans = KMeans(n_clusters=16)
compressed = kmeans.cluster_centers_[kmeans.predict(pixels)]
```

### 3. DetecciÃ³n de AnomalÃ­as

Identificar puntos que no pertenecen a ningÃºn grupo:

```python
from sklearn.cluster import DBSCAN

# Transacciones bancarias
transactions = load_transactions()
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(transactions)

# labels == -1 son potenciales fraudes
anomalies = transactions[labels == -1]
```

### 4. Otras Aplicaciones

| Dominio        | AplicaciÃ³n                                |
| -------------- | ----------------------------------------- |
| BiologÃ­a       | AgrupaciÃ³n de genes con expresiÃ³n similar |
| Redes Sociales | DetecciÃ³n de comunidades                  |
| Documentos     | AgrupaciÃ³n de artÃ­culos por tema          |
| AstronomÃ­a     | ClasificaciÃ³n de estrellas/galaxias       |
| Marketing      | SegmentaciÃ³n de mercado                   |

---

## ğŸ”¬ Tipos de Algoritmos de Clustering

### 1. Basados en ParticiÃ³n

Dividen los datos en K grupos no superpuestos.

```
Ejemplos: K-Means, K-Medoids
CaracterÃ­sticas:
- Requieren especificar K
- Clusters esfÃ©ricos
- RÃ¡pidos y escalables
```

### 2. Basados en Densidad

Definen clusters como regiones densas separadas por regiones de baja densidad.

```
Ejemplos: DBSCAN, OPTICS, HDBSCAN
CaracterÃ­sticas:
- No requieren especificar K
- Detectan formas arbitrarias
- Identifican outliers
```

### 3. JerÃ¡rquicos

Crean una jerarquÃ­a de clusters (Ã¡rbol).

```
Ejemplos: Agglomerative, Divisive
CaracterÃ­sticas:
- Producen dendrograma
- No requieren K inicial
- Costosos computacionalmente
```

### 4. Basados en Modelos

Asumen que los datos provienen de una distribuciÃ³n.

```
Ejemplos: Gaussian Mixture Models (GMM)
CaracterÃ­sticas:
- ProbabilÃ­sticos
- Clusters suaves (soft clustering)
- MÃ¡s flexibles pero complejos
```

### ComparaciÃ³n Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  K-Means           DBSCAN             JerÃ¡rquico             â”‚
â”‚                                                              â”‚
â”‚    â—‹â—‹â—‹               ~~~                  â”Œâ”€â”€â”¬â”€â”€â”            â”‚
â”‚   â—‹â—‹â—‹â—‹â—‹             ~~~~~                 â”‚  â”‚  â”‚            â”‚
â”‚    â—‹â—‹â—‹               ~~~                â”Œâ”€â”´â”€â”€â”´â”€â”€â”´â”€â”          â”‚
â”‚                    (formas             â”‚ â”‚ â”‚ â”‚ â”‚ â”‚          â”‚
â”‚  (esfÃ©ricos)      arbitrarias)        A B C D E F          â”‚
â”‚                                       (dendrograma)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Concepto de Distancia y Similitud

### Â¿Por quÃ© es importante?

El clustering agrupa por **similitud**, que se mide inversamente con **distancia**:

- Menor distancia â†’ Mayor similitud
- Mayor distancia â†’ Menor similitud

### Distancia Euclidiana

La mÃ¡s comÃºn, mide la lÃ­nea recta entre dos puntos:

$$d_{euclidean}(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

```python
import numpy as np

def euclidean_distance(x, y):
    """Calculate Euclidean distance between two points."""
    return np.sqrt(np.sum((x - y) ** 2))

# Ejemplo
a = np.array([1, 2])
b = np.array([4, 6])
print(euclidean_distance(a, b))  # 5.0
```

### Distancia Manhattan

Suma de diferencias absolutas (como caminar en una cuadrÃ­cula):

$$d_{manhattan}(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

```python
def manhattan_distance(x, y):
    """Calculate Manhattan distance between two points."""
    return np.sum(np.abs(x - y))

print(manhattan_distance(a, b))  # 7
```

### Similitud Coseno

Mide el Ã¡ngulo entre vectores (Ãºtil para texto):

$$sim_{cosine}(x, y) = \frac{x \cdot y}{||x|| \cdot ||y||}$$

```python
def cosine_similarity(x, y):
    """Calculate cosine similarity between two vectors."""
    dot_product = np.dot(x, y)
    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)
    return dot_product / (norm_x * norm_y)
```

### Â¿CuÃ¡l usar?

| Distancia  | CuÃ¡ndo usar                                    |
| ---------- | ---------------------------------------------- |
| Euclidiana | Datos numÃ©ricos continuos, escala similar      |
| Manhattan  | Datos con outliers, dimensiones independientes |
| Coseno     | Texto, datos de alta dimensionalidad           |

---

## âš ï¸ Preprocesamiento para Clustering

### NormalizaciÃ³n: Crucial para K-Means

K-Means usa distancias euclidianas, por lo que las features con mayor escala dominarÃ¡n:

```python
from sklearn.preprocessing import StandardScaler

# Sin normalizar: edad (0-100) domina sobre ingresos (0-1M)
X_raw = np.array([
    [25, 50000],
    [30, 80000],
    [65, 45000]
])

# Normalizar siempre
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
```

### ReducciÃ³n de Dimensionalidad

Para visualizar y mejorar clustering en alta dimensionalidad:

```python
from sklearn.decomposition import PCA

# Reducir a 2D para visualizaciÃ³n
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_scaled)

# O usar para mejorar el clustering
pca = PCA(n_components=0.95)  # Mantener 95% varianza
X_reduced = pca.fit_transform(X_scaled)
```

---

## ğŸ¯ El DesafÃ­o del Clustering

### No hay "respuesta correcta"

A diferencia de la clasificaciÃ³n supervisada:

- No tenemos etiquetas para validar
- MÃºltiples agrupaciones pueden ser vÃ¡lidas
- La interpretaciÃ³n requiere conocimiento del dominio

### Preguntas clave

1. **Â¿CuÃ¡ntos clusters?** - No siempre obvio
2. **Â¿QuÃ© algoritmo usar?** - Depende de los datos
3. **Â¿CÃ³mo evaluar calidad?** - MÃ©tricas intrÃ­nsecas vs extrÃ­nsecas
4. **Â¿Tienen sentido los clusters?** - ValidaciÃ³n con expertos

---

## ğŸ Clustering en Scikit-learn

### API Consistente

```python
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering

# Todos siguen el mismo patrÃ³n
model = KMeans(n_clusters=3)
labels = model.fit_predict(X)

# O en dos pasos
model.fit(X)
labels = model.labels_
```

### Atributos Comunes

```python
# DespuÃ©s de fit()
model.labels_          # Etiqueta de cluster para cada punto
model.cluster_centers_ # Centroides (K-Means)
model.inertia_         # Inercia (K-Means)
```

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo la diferencia entre aprendizaje supervisado y no supervisado
- [ ] Puedo explicar quÃ© es clustering y para quÃ© sirve
- [ ] Conozco los tipos principales de algoritmos de clustering
- [ ] Comprendo el concepto de distancia y similitud
- [ ] SÃ© por quÃ© es importante normalizar antes de clustering

---

## ğŸ“š Recursos Adicionales

- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [Comparison of Clustering Algorithms](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)
- [The Elements of Statistical Learning - Chapter 14](https://web.stanford.edu/~hastie/ElemStatLearn/)

---

## ğŸ”— NavegaciÃ³n

| â¬…ï¸ Anterior                          | ğŸ  Inicio                 | Siguiente â¡ï¸            |
| ------------------------------------ | ------------------------- | ----------------------- |
| [Semana 12](../../week-12/README.md) | [Semana 13](../README.md) | [K-Means](02-kmeans.md) |
