# üå≥ Clustering Jer√°rquico

## üéØ Objetivos de Aprendizaje

- Comprender el clustering aglomerativo y divisivo
- Construir e interpretar dendrogramas
- Dominar los diferentes m√©todos de enlace (linkage)
- Aplicar estrategias de corte para obtener clusters
- Comparar clustering jer√°rquico con otros m√©todos

---

## üìã ¬øQu√© es el Clustering Jer√°rquico?

El clustering jer√°rquico construye una **jerarqu√≠a de clusters** representada como un √°rbol (dendrograma). No requiere especificar el n√∫mero de clusters de antemano.

### Dos Enfoques

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ  AGLOMERATIVO (Bottom-Up)      DIVISIVO (Top-Down)         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îê                        ‚îå‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ      ‚îå‚îÄ‚î¥‚îÄ‚î¨‚îÄ‚îò                      ‚îå‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îê                ‚îÇ
‚îÇ    ‚îå‚îÄ‚î¥‚îÄ‚îê ‚îÇ                      ‚îå‚îÄ‚î¥‚îÄ‚îê   ‚îå‚îÄ‚î¥‚îÄ‚îê              ‚îÇ
‚îÇ    A B C D                      A B C   D E F              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Empieza con n clusters        Empieza con 1 cluster       ‚îÇ
‚îÇ  Fusiona los m√°s cercanos      Divide recursivamente       ‚îÇ
‚îÇ  M√ÅS COM√öN                     Menos eficiente             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

![Clustering Jer√°rquico](../0-assets/03-clustering-jerarquico.svg)

---

## üîÑ Algoritmo Aglomerativo

### Pasos del Algoritmo

1. Cada punto es un cluster individual (n clusters)
2. Calcular matriz de distancias entre todos los clusters
3. Fusionar los dos clusters m√°s cercanos
4. Actualizar matriz de distancias
5. Repetir 3-4 hasta tener un solo cluster

### Implementaci√≥n Conceptual

```python
import numpy as np
from scipy.spatial.distance import pdist, squareform

def agglomerative_clustering_concept(X: np.ndarray) -> list:
    """
    Conceptual implementation of agglomerative clustering.

    Args:
        X: Data matrix of shape (n_samples, n_features)

    Returns:
        merge_history: List of merges [(cluster_i, cluster_j, distance), ...]
    """
    n_samples = X.shape[0]

    # Each point starts as its own cluster
    clusters = {i: [i] for i in range(n_samples)}
    merge_history = []

    # Compute initial pairwise distances
    distances = squareform(pdist(X, metric='euclidean'))
    np.fill_diagonal(distances, np.inf)  # Ignore self-distances

    while len(clusters) > 1:
        # Find minimum distance
        min_dist = np.inf
        merge_i, merge_j = None, None

        cluster_keys = list(clusters.keys())
        for i in range(len(cluster_keys)):
            for j in range(i + 1, len(cluster_keys)):
                ci, cj = cluster_keys[i], cluster_keys[j]
                # Single linkage: minimum distance between any two points
                dist = min(distances[pi, pj]
                          for pi in clusters[ci]
                          for pj in clusters[cj])
                if dist < min_dist:
                    min_dist = dist
                    merge_i, merge_j = ci, cj

        # Merge clusters
        new_cluster_id = max(clusters.keys()) + 1
        clusters[new_cluster_id] = clusters[merge_i] + clusters[merge_j]
        del clusters[merge_i]
        del clusters[merge_j]

        merge_history.append((merge_i, merge_j, min_dist,
                             len(clusters[new_cluster_id])))

    return merge_history
```

---

## üîó M√©todos de Enlace (Linkage)

El **m√©todo de enlace** define c√≥mo medir la distancia entre clusters.

### Single Linkage (Enlace Simple)

Distancia **m√≠nima** entre cualquier par de puntos:

$$d(A, B) = \min_{a \in A, b \in B} d(a, b)$$

```python
# Caracter√≠sticas:
# ‚úì Puede detectar clusters alargados/irregulares
# ‚úó Sensible al "efecto cadena" (chaining)
# ‚úó Puede unir clusters que no deber√≠an unirse
```

### Complete Linkage (Enlace Completo)

Distancia **m√°xima** entre cualquier par de puntos:

$$d(A, B) = \max_{a \in A, b \in B} d(a, b)$$

```python
# Caracter√≠sticas:
# ‚úì Produce clusters compactos y esf√©ricos
# ‚úì Menos sensible a outliers que single
# ‚úó Puede romper clusters grandes
```

### Average Linkage (Enlace Promedio)

Distancia **promedio** entre todos los pares:

$$d(A, B) = \frac{1}{|A| \cdot |B|} \sum_{a \in A} \sum_{b \in B} d(a, b)$$

```python
# Caracter√≠sticas:
# ‚úì Balance entre single y complete
# ‚úì Menos sensible a outliers
# ‚úì Clusters de tama√±o similar
```

### Ward's Method (M√©todo de Ward)

Minimiza la **varianza** dentro del cluster fusionado:

$$d(A, B) = \sqrt{\frac{2|A||B|}{|A|+|B|}} \cdot ||c_A - c_B||$$

```python
# Caracter√≠sticas:
# ‚úì Produce clusters compactos y de tama√±o similar
# ‚úì M√°s usado en la pr√°ctica
# ‚úì Similar a K-Means en resultados
# ‚úó Asume clusters esf√©ricos
```

### Comparaci√≥n Visual

```
Single:     ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè   Cadenas largas

Complete:   ‚óè‚óè‚óè    ‚óè‚óè‚óè    ‚óè‚óè‚óè       Clusters compactos
            ‚óè‚óè‚óè    ‚óè‚óè‚óè    ‚óè‚óè‚óè

Ward:       ‚óã‚óã‚óã    ‚óã‚óã‚óã    ‚óã‚óã‚óã       Esf√©ricos, tama√±o similar
            ‚óã‚óã‚óã    ‚óã‚óã‚óã    ‚óã‚óã‚óã
```

---

## üìä Dendrogramas

### ¬øQu√© es un Dendrograma?

Un **dendrograma** es una representaci√≥n gr√°fica del clustering jer√°rquico:

- Eje Y: Distancia de fusi√≥n
- Eje X: Muestras individuales
- L√≠neas horizontales: Fusiones de clusters
- Altura de la l√≠nea: Distancia a la que se fusionaron

### Construcci√≥n con SciPy

```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

def create_dendrogram(X: np.ndarray, method: str = 'ward',
                      labels: list = None) -> None:
    """
    Create and display a dendrogram.

    Args:
        X: Data matrix
        method: Linkage method
        labels: Sample labels for x-axis
    """
    # Compute linkage matrix
    Z = linkage(X, method=method)

    # Plot dendrogram
    plt.figure(figsize=(14, 7))
    dendrogram(
        Z,
        labels=labels,
        leaf_rotation=90,
        leaf_font_size=10,
        color_threshold=0.7 * max(Z[:, 2])  # Color clusters
    )

    plt.title(f'Dendrograma (M√©todo: {method})', fontsize=14)
    plt.xlabel('Muestras', fontsize=12)
    plt.ylabel('Distancia', fontsize=12)
    plt.tight_layout()
    plt.show()

    return Z


# Ejemplo
from sklearn.datasets import make_blobs

X, _ = make_blobs(n_samples=30, centers=3, random_state=42)
Z = create_dendrogram(X, method='ward')
```

### Interpretaci√≥n del Dendrograma

```
Distancia
    ‚îÇ
 4  ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              ‚îÇ                 ‚îÇ
 3  ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ        ‚îÇ     ‚îÇ           ‚îÇ     ‚îÇ
 2  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ   ‚Üê Posible corte: 3 clusters
    ‚îÇ   ‚îÇ    ‚îÇ     ‚îÇ      ‚îÇ    ‚îÇ     ‚îÇ
 1  ‚îÇ ‚îå‚îÄ‚î§  ‚îå‚îÄ‚î§     ‚îÇ    ‚îå‚îÄ‚î§  ‚îå‚îÄ‚î§     ‚îÇ
    ‚îÇ ‚îÇ ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ    ‚îÇ ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ
    ‚îî‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ
      A B  C D     E    F G  H I     J

    Cluster 1     C2      Cluster 3
```

---

## ‚úÇÔ∏è Estrategias de Corte

### Corte por N√∫mero de Clusters

```python
from scipy.cluster.hierarchy import fcluster

# Obtener exactamente K clusters
K = 3
labels = fcluster(Z, t=K, criterion='maxclust')
print(f"Clusters: {labels}")
```

### Corte por Distancia

```python
# Cortar a una distancia espec√≠fica
distance_threshold = 5.0
labels = fcluster(Z, t=distance_threshold, criterion='distance')

n_clusters = len(set(labels))
print(f"Clusters encontrados: {n_clusters}")
```

### Corte Autom√°tico con Inconsistency

```python
# Corte basado en inconsistencia (detecta saltos inusuales)
from scipy.cluster.hierarchy import inconsistent

depth = 3  # Profundidad para calcular inconsistencia
incons = inconsistent(Z, d=depth)

# Cortar donde la inconsistencia excede un umbral
labels = fcluster(Z, t=1.5, criterion='inconsistent', depth=depth)
```

### M√©todo Visual: Buscar el Mayor Salto

```python
def find_optimal_cut(Z: np.ndarray) -> float:
    """
    Find optimal cut point by looking for largest gap in distances.

    Args:
        Z: Linkage matrix

    Returns:
        Optimal distance threshold
    """
    distances = Z[:, 2]
    gaps = np.diff(distances)
    max_gap_idx = np.argmax(gaps)

    # Cut between the two distances with largest gap
    optimal_threshold = (distances[max_gap_idx] + distances[max_gap_idx + 1]) / 2

    print(f"Mayor salto entre distancias: {gaps[max_gap_idx]:.4f}")
    print(f"Umbral √≥ptimo sugerido: {optimal_threshold:.4f}")

    return optimal_threshold


threshold = find_optimal_cut(Z)
labels = fcluster(Z, t=threshold, criterion='distance')
```

---

## üêç Clustering Jer√°rquico con Scikit-learn

### Uso B√°sico

```python
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

# Preparar datos
X = np.random.randn(100, 2)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Crear modelo
agg = AgglomerativeClustering(
    n_clusters=3,           # N√∫mero de clusters (o None)
    metric='euclidean',     # M√©trica de distancia
    linkage='ward'          # M√©todo de enlace
)

# Fit y obtener labels
labels = agg.fit_predict(X_scaled)
print(f"Labels: {labels}")
```

### Usando Threshold en lugar de n_clusters

```python
# Dejar que el algoritmo determine el n√∫mero de clusters
agg = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=1.5,  # Distancia de corte
    linkage='ward'
)
labels = agg.fit_predict(X_scaled)
n_clusters = len(set(labels))
print(f"Clusters encontrados: {n_clusters}")
```

### Atributos del Modelo

```python
# Despu√©s de fit()
print(f"Labels: {agg.labels_}")
print(f"N√∫mero de clusters: {agg.n_clusters_}")
print(f"N√∫mero de hojas: {agg.n_leaves_}")
print(f"N√∫mero de componentes: {agg.n_connected_components_}")

# Para obtener el dendrograma completo, usar scipy
from scipy.cluster.hierarchy import linkage, dendrogram
Z = linkage(X_scaled, method='ward')
```

---

## üîÑ Workflow Completo

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

def hierarchical_clustering_workflow(X: np.ndarray,
                                    method: str = 'ward') -> dict:
    """
    Complete hierarchical clustering workflow.

    Args:
        X: Data matrix
        method: Linkage method

    Returns:
        Dictionary with results
    """
    # 1. Normalizar datos
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 2. Calcular matriz de enlace
    Z = linkage(X_scaled, method=method)

    # 3. Crear dendrograma
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Dendrograma
    axes[0].set_title(f'Dendrograma ({method})', fontsize=14)
    dendrogram(Z, ax=axes[0], color_threshold=0.7 * max(Z[:, 2]))
    axes[0].set_xlabel('Muestras')
    axes[0].set_ylabel('Distancia')

    # 4. Encontrar K √≥ptimo con el m√©todo del codo
    last = Z[-10:, 2]
    last_rev = last[::-1]
    idxs = np.arange(1, len(last) + 1)

    axes[1].plot(idxs, last_rev, 'bo-')
    axes[1].set_title('M√©todo del Codo (√∫ltimas fusiones)', fontsize=14)
    axes[1].set_xlabel('N√∫mero de Clusters')
    axes[1].set_ylabel('Distancia de Fusi√≥n')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 5. Sugerir n√∫mero de clusters basado en mayor salto
    acceleration = np.diff(last_rev, 2)  # Segunda derivada
    k_optimal = acceleration.argmax() + 2  # +2 porque perdimos 2 puntos
    print(f"K √≥ptimo sugerido: {k_optimal}")

    # 6. Obtener labels finales
    labels = fcluster(Z, t=k_optimal, criterion='maxclust')

    return {
        'linkage_matrix': Z,
        'labels': labels,
        'k_optimal': k_optimal,
        'scaler': scaler
    }


# Ejemplo
X, y_true = make_blobs(n_samples=150, centers=4, random_state=42)
results = hierarchical_clustering_workflow(X, method='ward')
```

---

## ‚öñÔ∏è Comparaci√≥n con Otros M√©todos

### Jer√°rquico vs K-Means

| Aspecto              | K-Means | Jer√°rquico      |
| -------------------- | ------- | --------------- |
| K requerido          | S√≠      | Opcional        |
| Complejidad temporal | O(nKt)  | O(n¬≤) o O(n¬≥)   |
| Complejidad espacial | O(n)    | O(n¬≤)           |
| Dendrograma          | No      | S√≠              |
| Determin√≠stico       | No      | S√≠              |
| Escala               | Grande  | Peque√±o-Mediano |

### Jer√°rquico vs DBSCAN

| Aspecto        | DBSCAN     | Jer√°rquico         |
| -------------- | ---------- | ------------------ |
| K requerido    | No         | Opcional           |
| Outliers       | Detecta    | No                 |
| Forma clusters | Arbitraria | Depende de linkage |
| Visualizaci√≥n  | Scatter    | Dendrograma        |

### ¬øCu√°ndo usar Jer√°rquico?

```python
# Usar clustering jer√°rquico cuando:
# ‚úì Dataset peque√±o-mediano (< 10,000 muestras)
# ‚úì Necesitas explorar diferentes n√∫meros de clusters
# ‚úì Quieres visualizar la estructura jer√°rquica
# ‚úì Tienes datos taxon√≥micos/jer√°rquicos
# ‚úì No sabes cu√°ntos clusters esperar

# NO usar cuando:
# ‚úó Dataset muy grande (> 50,000 muestras)
# ‚úó Necesitas clustering en tiempo real
# ‚úó Los clusters tienen formas muy irregulares
```

---

## ‚ö†Ô∏è Limitaciones

### Complejidad Computacional

```python
# O(n¬≤) en espacio y O(n¬≥) en tiempo (sin optimizaciones)
# Para n = 10,000:
#   - Memoria: ~800 MB (matriz de distancias)
#   - Tiempo: Minutos a horas

# Soluciones para datos grandes:
# 1. Muestreo
sample_indices = np.random.choice(len(X), size=5000, replace=False)
X_sample = X[sample_indices]

# 2. Mini-batch clustering previo
from sklearn.cluster import MiniBatchKMeans
mbk = MiniBatchKMeans(n_clusters=100)
X_reduced = mbk.fit_transform(X)  # Cluster centroids
```

### No es Reversible

Una vez fusionados, los clusters no pueden separarse en pasos posteriores:

```python
# Si una fusi√≥n temprana es incorrecta, el error se propaga
# Soluci√≥n: probar diferentes m√©todos de linkage
for method in ['single', 'complete', 'average', 'ward']:
    Z = linkage(X_scaled, method=method)
    # Comparar dendrogramas
```

---

## üìä Ejemplo Completo: Taxonom√≠a de Productos

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Simular caracter√≠sticas de productos
np.random.seed(42)
products = {
    'producto': [f'P{i}' for i in range(20)],
    'precio': [100, 120, 95, 500, 480, 520, 50, 45, 55, 800,
               105, 115, 490, 510, 48, 52, 790, 810, 110, 495],
    'peso_kg': [0.5, 0.6, 0.4, 2.0, 1.8, 2.2, 0.3, 0.25, 0.35, 5.0,
                0.55, 0.45, 1.9, 2.1, 0.28, 0.32, 4.8, 5.2, 0.52, 2.0],
    'categoria_manual': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D',
                         'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'A', 'B']
}
df = pd.DataFrame(products)

# Preparar datos
X = df[['precio', 'peso_kg']].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Clustering jer√°rquico
Z = linkage(X_scaled, method='ward')

# Visualizar
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Dendrograma
dendrogram(Z, labels=df['producto'].values, ax=axes[0],
           leaf_rotation=90, leaf_font_size=10)
axes[0].set_title('Taxonom√≠a de Productos', fontsize=14)
axes[0].set_ylabel('Distancia')

# Cortar en 4 clusters
from scipy.cluster.hierarchy import fcluster
df['cluster'] = fcluster(Z, t=4, criterion='maxclust')

# Scatter plot
colors = {1: 'red', 2: 'blue', 3: 'green', 4: 'orange'}
for cluster in df['cluster'].unique():
    subset = df[df['cluster'] == cluster]
    axes[1].scatter(subset['precio'], subset['peso_kg'],
                   c=colors[cluster], label=f'Cluster {cluster}',
                   s=100, alpha=0.7)
    for _, row in subset.iterrows():
        axes[1].annotate(row['producto'], (row['precio'], row['peso_kg']),
                        fontsize=8)

axes[1].set_xlabel('Precio')
axes[1].set_ylabel('Peso (kg)')
axes[1].set_title('Productos por Cluster')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# An√°lisis de clusters
print("\nüì¶ An√°lisis de Clusters:")
print(df.groupby('cluster').agg({
    'precio': ['mean', 'std'],
    'peso_kg': ['mean', 'std'],
    'producto': 'count'
}).round(2))
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la diferencia entre aglomerativo y divisivo
- [ ] Puedo construir e interpretar un dendrograma
- [ ] Conozco los m√©todos de linkage y cu√°ndo usar cada uno
- [ ] S√© aplicar diferentes estrategias de corte
- [ ] Comprendo las limitaciones del clustering jer√°rquico

---

## üìö Recursos Adicionales

- [SciPy Hierarchical Clustering](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)
- [Scikit-learn AgglomerativeClustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)
- [Understanding Dendrograms](https://www.displayr.com/what-is-dendrogram/)

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior            | üè† Inicio                 | Siguiente ‚û°Ô∏è                                                 |
| ---------------------- | ------------------------- | ------------------------------------------------------------ |
| [DBSCAN](03-dbscan.md) | [Semana 13](../README.md) | [Ejercicio 01](../2-practicas/ejercicio-01-kmeans/README.md) |
