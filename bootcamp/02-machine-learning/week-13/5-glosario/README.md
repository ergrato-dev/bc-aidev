# üìñ Glosario - Semana 13: Clustering

T√©rminos clave ordenados alfab√©ticamente.

---

## A

### Agglomerative Clustering

**Clustering aglomerativo** - M√©todo jer√°rquico bottom-up que comienza con cada punto como su propio cluster y los va fusionando iterativamente seg√∫n su proximidad.

```python
from sklearn.cluster import AgglomerativeClustering
clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = clustering.fit_predict(X)
```

### Adjusted Rand Index (ARI)

**√çndice de Rand Ajustado** - M√©trica externa que mide la similitud entre clustering predicho y etiquetas reales, ajustada por azar. Rango: [-1, 1], mayor es mejor.

$$ARI = \frac{RI - E[RI]}{max(RI) - E[RI]}$$

---

## B

### Border Point

**Punto frontera** - En DBSCAN, punto que no es core pero est√° dentro del radio epsilon de un punto core. Pertenece al cluster pero no lo extiende.

---

## C

### Calinski-Harabasz Index

**√çndice de Calinski-Harabasz** - Tambi√©n llamado Variance Ratio Criterion. Mide la relaci√≥n entre dispersi√≥n inter-cluster e intra-cluster. Mayor es mejor.

$$CH = \frac{SS_B / (k-1)}{SS_W / (n-k)}$$

### Centroid

**Centroide** - Punto central de un cluster, calculado como la media de todos los puntos del cluster. Usado en K-Means.

```python
centroid = X[labels == cluster_id].mean(axis=0)
```

### Cluster

**Grupo/Conglomerado** - Conjunto de puntos que son m√°s similares entre s√≠ que con puntos de otros grupos.

### Complete Linkage

**Enlace completo** - M√©todo de linkage que define la distancia entre clusters como la m√°xima distancia entre cualquier par de puntos. Produce clusters compactos.

### Core Point

**Punto n√∫cleo** - En DBSCAN, punto que tiene al menos `min_samples` vecinos dentro del radio `epsilon`. Puede expandir el cluster.

---

## D

### Davies-Bouldin Index

**√çndice de Davies-Bouldin** - M√©trica interna que mide la "compacidad" promedio de clusters relativa a la separaci√≥n entre ellos. Menor es mejor.

$$DB = \frac{1}{k}\sum_{i=1}^{k} max_{j \neq i} \frac{s_i + s_j}{d_{ij}}$$

### DBSCAN

**Density-Based Spatial Clustering of Applications with Noise** - Algoritmo que agrupa puntos densamente conectados y marca como ruido los puntos en regiones de baja densidad.

```python
from sklearn.cluster import DBSCAN
clustering = DBSCAN(eps=0.5, min_samples=5)
labels = clustering.fit_predict(X)
```

### Dendrogram

**Dendrograma** - Diagrama de √°rbol que muestra la secuencia de fusiones (o divisiones) en clustering jer√°rquico. El eje Y muestra la distancia de fusi√≥n.

### Density-Based Clustering

**Clustering basado en densidad** - Familia de algoritmos (DBSCAN, OPTICS, HDBSCAN) que identifican clusters como regiones de alta densidad separadas por regiones de baja densidad.

### Divisive Clustering

**Clustering divisivo** - M√©todo jer√°rquico top-down que comienza con todos los puntos en un cluster y los va dividiendo. Opuesto al aglomerativo.

---

## E

### Elbow Method

**M√©todo del codo** - T√©cnica para elegir el n√∫mero √≥ptimo de clusters (K) graficando la inercia vs K y buscando el "codo" donde la mejora se estabiliza.

### Epsilon (Œµ)

**Radio de vecindad** - En DBSCAN, distancia m√°xima para considerar dos puntos como vecinos. Par√°metro cr√≠tico del algoritmo.

### Euclidean Distance

**Distancia euclidiana** - Medida de distancia m√°s com√∫n, la "l√≠nea recta" entre dos puntos.

$$d(x,y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

---

## H

### Hierarchical Clustering

**Clustering jer√°rquico** - Familia de algoritmos que construyen una jerarqu√≠a de clusters, visualizable como dendrograma. Puede ser aglomerativo o divisivo.

### Homogeneity

**Homogeneidad** - M√©trica externa que mide si cada cluster contiene solo miembros de una √∫nica clase. Rango [0,1].

---

## I

### Inertia

**Inercia** - Tambi√©n llamada WCSS (Within-Cluster Sum of Squares). Suma de distancias cuadradas de cada punto a su centroide. K-Means minimiza esta m√©trica.

$$Inertia = \sum_{i=1}^{n} ||x_i - c_{y_i}||^2$$

---

## K

### K-Distance Graph

**Gr√°fico de K-distancia** - T√©cnica para elegir epsilon en DBSCAN. Grafica la distancia al k-√©simo vecino m√°s cercano para cada punto, ordenados. El "codo" sugiere epsilon.

### K-Means

**K-Medias** - Algoritmo de clustering que particiona datos en K clusters minimizando la suma de distancias cuadradas intra-cluster.

```python
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)
```

### K-Means++

**Inicializaci√≥n inteligente** - M√©todo de inicializaci√≥n para K-Means que selecciona centroides iniciales de forma que est√©n dispersos, mejorando convergencia.

---

## L

### Linkage

**Enlace/Uni√≥n** - Criterio para medir distancia entre clusters en clustering jer√°rquico. Tipos: single, complete, average, ward.

---

## M

### Manhattan Distance

**Distancia Manhattan** - Suma de diferencias absolutas en cada dimensi√≥n. Tambi√©n llamada distancia L1 o city-block.

$$d(x,y) = \sum_{i=1}^{n}|x_i - y_i|$$

### Min_samples

**M√≠nimo de muestras** - En DBSCAN, n√∫mero m√≠nimo de puntos requeridos en el vecindario epsilon para que un punto sea considerado core.

---

## N

### Noise Points

**Puntos de ruido** - En DBSCAN, puntos que no son core ni border. Se etiquetan como -1 y se consideran outliers.

### Normalized Mutual Information (NMI)

**Informaci√≥n mutua normalizada** - M√©trica externa que mide cu√°nta informaci√≥n comparten las etiquetas predichas y reales. Rango [0,1].

---

## P

### Partitional Clustering

**Clustering particional** - Algoritmos que dividen datos en K particiones disjuntas sin jerarqu√≠a. Ejemplo: K-Means.

---

## R

### RFM Analysis

**An√°lisis RFM** - T√©cnica de segmentaci√≥n de clientes basada en Recency (recencia), Frequency (frecuencia) y Monetary (valor monetario).

---

## S

### Silhouette Score

**Coeficiente de silueta** - M√©trica que mide qu√© tan similar es un punto a su cluster comparado con otros clusters. Rango [-1, 1], mayor es mejor.

$$s(i) = \frac{b(i) - a(i)}{max(a(i), b(i))}$$

Donde:

- $a(i)$ = distancia promedio a puntos del mismo cluster
- $b(i)$ = distancia promedio al cluster m√°s cercano

### Single Linkage

**Enlace simple** - M√©todo de linkage que define distancia entre clusters como la m√≠nima distancia entre cualquier par de puntos. Susceptible a "chain effect".

---

## U

### Unsupervised Learning

**Aprendizaje no supervisado** - Rama del ML donde no hay etiquetas de salida. El objetivo es descubrir estructura en los datos (clustering, reducci√≥n de dimensionalidad).

---

## V

### V-Measure

**Medida V** - Media arm√≥nica de homogeneity y completeness. Combina ambas m√©tricas externas.

$$V = \frac{2 \times homogeneity \times completeness}{homogeneity + completeness}$$

---

## W

### Ward's Method

**M√©todo de Ward** - M√©todo de linkage que minimiza la varianza total intra-cluster al fusionar. Tiende a crear clusters de tama√±o similar. Muy usado en pr√°ctica.

### WCSS

**Within-Cluster Sum of Squares** - Ver: Inertia

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Recursos                   | üè† Semana                  | Siguiente ‚û°Ô∏è              |
| ----------------------------- | -------------------------- | ------------------------- |
| [Recursos](../../4-recursos/) | [Week 13](../../README.md) | [Week 14](../../week-14/) |
