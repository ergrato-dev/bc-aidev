# üìâ Introducci√≥n a la Reducci√≥n de Dimensionalidad

## üéØ Objetivos

- Comprender qu√© es la reducci√≥n de dimensionalidad
- Entender la maldici√≥n de la dimensionalidad
- Conocer los tipos de t√©cnicas disponibles
- Saber cu√°ndo aplicar reducci√≥n dimensional

---

## üìö ¬øQu√© es la Reducci√≥n de Dimensionalidad?

La **reducci√≥n de dimensionalidad** es el proceso de transformar datos de un espacio de alta dimensi√≥n a uno de menor dimensi√≥n, preservando la informaci√≥n m√°s relevante.

### Ejemplo Conceptual

```
Datos originales: 1000 features
         ‚Üì
Reducci√≥n dimensional
         ‚Üì
Datos reducidos: 50 features (manteniendo 95% de informaci√≥n)
```

---

## üå™Ô∏è La Maldici√≥n de la Dimensionalidad

![Maldici√≥n de la Dimensionalidad](../0-assets/01-curse-dimensionality.svg)

### ¬øQu√© es?

El t√©rmino "curse of dimensionality" (Richard Bellman, 1961) describe los problemas que surgen cuando trabajamos con datos de alta dimensi√≥n.

### Problemas Principales

#### 1. Distancias se Vuelven Similares

```python
import numpy as np

def compare_distances(dims, n_points=100):
    """Demuestra c√≥mo las distancias convergen en alta dimensi√≥n."""
    np.random.seed(42)

    for d in dims:
        X = np.random.randn(n_points, d)

        # Calcular todas las distancias
        distances = []
        for i in range(n_points):
            for j in range(i+1, n_points):
                dist = np.linalg.norm(X[i] - X[j])
                distances.append(dist)

        distances = np.array(distances)
        ratio = distances.max() / distances.min()

        print(f"D={d:4d}: max/min ratio = {ratio:.2f}")

# compare_distances([2, 10, 100, 1000])
# D=   2: max/min ratio = 15.23
# D=  10: max/min ratio = 3.41
# D= 100: max/min ratio = 1.68
# D=1000: max/min ratio = 1.21  ‚Üê ¬°Casi iguales!
```

#### 2. Datos Requeridos Crecen Exponencialmente

Para mantener la misma densidad de datos:

| Dimensiones | Puntos necesarios |
| ----------- | ----------------- |
| 1D          | 10                |
| 2D          | 100               |
| 3D          | 1,000             |
| 10D         | 10,000,000,000    |

#### 3. Volumen se Concentra en Bordes

```python
import numpy as np

def volume_in_shell(d, shell_thickness=0.1):
    """Fracci√≥n del volumen en el 'shell' exterior de un hipercubo."""
    inner_ratio = (1 - shell_thickness) ** d
    return 1 - inner_ratio

# for d in [2, 10, 50, 100]:
#     print(f"D={d}: {volume_in_shell(d)*100:.1f}% del volumen est√° en el borde exterior")
# D=2: 19.0% del volumen est√° en el borde exterior
# D=10: 65.1% del volumen est√° en el borde exterior
# D=50: 99.5% del volumen est√° en el borde exterior
# D=100: 100.0% del volumen est√° en el borde exterior
```

---

## üéØ Motivaciones para Reducir Dimensiones

### 1. Visualizaci√≥n

```python
# De 784 dimensiones (MNIST) a 2D para visualizar
from sklearn.manifold import TSNE

# X_2d = TSNE(n_components=2).fit_transform(X_mnist)
# plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_mnist)
```

### 2. Eliminar Ruido

Features irrelevantes a√±aden ruido que afecta el modelo.

### 3. Reducir Overfitting

Menos features = menos riesgo de memorizar datos de entrenamiento.

### 4. Mejorar Rendimiento

Menos dimensiones = entrenamiento m√°s r√°pido.

### 5. Comprimir Datos

Almacenar y transmitir menos informaci√≥n.

---

## üóÇÔ∏è Tipos de T√©cnicas

### T√©cnicas Lineales

Asumen que los datos pueden proyectarse linealmente.

| T√©cnica             | Supervisada | Descripci√≥n                      |
| ------------------- | ----------- | -------------------------------- |
| **PCA**             | No          | Maximiza varianza                |
| **LDA**             | S√≠          | Maximiza separabilidad de clases |
| **Factor Analysis** | No          | Modela factores latentes         |

### T√©cnicas No Lineales

Capturan relaciones complejas.

| T√©cnica          | Descripci√≥n                         |
| ---------------- | ----------------------------------- |
| **t-SNE**        | Preserva estructura local           |
| **UMAP**         | Preserva local y global, m√°s r√°pido |
| **Isomap**       | Preserva distancias geod√©sicas      |
| **LLE**          | Locally Linear Embedding            |
| **Autoencoders** | Redes neuronales para reducci√≥n     |

---

## üìä Selecci√≥n de Features vs Extracci√≥n

### Selecci√≥n de Features

Selecciona un subconjunto de las features originales.

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)
# Mantiene 10 features originales
```

**Ventajas**: Interpretabilidad, features originales
**Desventajas**: Ignora combinaciones de features

### Extracci√≥n de Features (Reducci√≥n Dimensional)

Crea nuevas features como combinaciones de las originales.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=10)
X_extracted = pca.fit_transform(X)
# Crea 10 nuevas features (combinaciones)
```

**Ventajas**: Captura m√°s informaci√≥n
**Desventajas**: Features no interpretables directamente

---

## üîÑ Flujo de Trabajo T√≠pico

```
1. Datos originales (D dimensiones)
         ‚Üì
2. Preprocesamiento (escalar)
         ‚Üì
3. Reducci√≥n dimensional
         ‚Üì
4. Datos reducidos (K dimensiones, K << D)
         ‚Üì
5. Modelo de ML o visualizaci√≥n
```

### Ejemplo Completo

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Pipeline completo
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=50)),
    ('classifier', LogisticRegression())
])

# pipeline.fit(X_train, y_train)
# pipeline.score(X_test, y_test)
```

---

## üìè M√©tricas de Evaluaci√≥n

### Para PCA

- **Varianza explicada**: % de informaci√≥n retenida
- **Scree plot**: Gr√°fico de varianza por componente

### Para t-SNE/UMAP

- **Trustworthiness**: ¬øVecinos cercanos en embedding eran cercanos originalmente?
- **Continuity**: ¬øVecinos originales siguen cercanos?
- **Visual inspection**: ¬øClusters claros?

---

## ü§î ¬øCu√°ndo Usar Cada T√©cnica?

| Situaci√≥n                   | T√©cnica Recomendada |
| --------------------------- | ------------------- |
| Preprocesamiento para ML    | PCA                 |
| Visualizaci√≥n exploratoria  | t-SNE, UMAP         |
| Datos muy grandes           | PCA, UMAP           |
| Preservar estructura global | PCA, UMAP           |
| Revelar clusters            | t-SNE, UMAP         |
| Clasificaci√≥n supervisada   | LDA                 |
| Compresi√≥n de datos         | PCA                 |

---

## ‚úÖ Resumen

| Concepto                  | Descripci√≥n                            |
| ------------------------- | -------------------------------------- |
| Reducci√≥n dimensional     | Transformar a menor n√∫mero de features |
| Maldici√≥n dimensionalidad | Problemas en alta dimensi√≥n            |
| T√©cnicas lineales         | PCA, LDA (proyecciones lineales)       |
| T√©cnicas no lineales      | t-SNE, UMAP (relaciones complejas)     |
| Selecci√≥n vs Extracci√≥n   | Elegir vs crear features               |

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior                          | üè† Semana 17           | Siguiente ‚û°Ô∏è     |
| ------------------------------------ | ---------------------- | ---------------- |
| [Semana 16](../../week-16/README.md) | [README](../README.md) | [PCA](02-pca.md) |
