# üìñ Glosario - Semana 17: Reducci√≥n de Dimensionalidad

## A

### Autovalor (Eigenvalue)

Escalar $\lambda$ que satisface $Av = \lambda v$ para una matriz $A$ y vector $v$. En PCA, representa la varianza capturada por cada componente principal.

### Autovector (Eigenvector)

Vector $v$ que satisface $Av = \lambda v$. En PCA, los autovectores de la matriz de covarianza definen las direcciones de los componentes principales.

## B

### Biplot

Visualizaci√≥n que muestra simult√°neamente las observaciones proyectadas y las contribuciones de las variables originales en el espacio de componentes principales.

## C

### Componente Principal (Principal Component)

Nueva variable creada como combinaci√≥n lineal de las variables originales, orientada en la direcci√≥n de m√°xima varianza.

### Covarianza

Medida de c√≥mo dos variables var√≠an juntas. La matriz de covarianza es fundamental para calcular PCA.

$$\text{Cov}(X, Y) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$

### Curse of Dimensionality (Maldici√≥n de la Dimensionalidad)

Fen√≥meno donde los datos se vuelven escasos en espacios de alta dimensi√≥n, dificultando el an√°lisis y la clasificaci√≥n.

## D

### Distancia Euclidiana

Distancia en l√≠nea recta entre dos puntos:

$$d(p, q) = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}$$

## E

### Embedding

Representaci√≥n de datos de alta dimensi√≥n en un espacio de menor dimensi√≥n que preserva alguna estructura del espacio original.

### Explained Variance Ratio

Proporci√≥n de varianza total explicada por cada componente principal. Suma 1.0 para todos los componentes.

## F

### Feature Extraction

T√©cnica que transforma las caracter√≠sticas originales en nuevas caracter√≠sticas de menor dimensi√≥n (ej: PCA, t-SNE).

### Feature Selection

T√©cnica que selecciona un subconjunto de las caracter√≠sticas originales sin transformarlas.

## G

### Gradient Descent

Algoritmo de optimizaci√≥n usado por t-SNE y UMAP para minimizar la funci√≥n de costo y encontrar el embedding √≥ptimo.

## K

### KL Divergence (Kullback-Leibler Divergence)

Medida de qu√© tan diferente es una distribuci√≥n de probabilidad de otra. t-SNE minimiza la KL divergence entre las distribuciones de alta y baja dimensi√≥n.

$$KL(P||Q) = \sum_i P(i) \log\frac{P(i)}{Q(i)}$$

## L

### Lineal

PCA es una t√©cnica lineal porque los componentes son combinaciones lineales de las variables originales.

### Local vs Global Structure

- **Local**: Relaciones entre puntos cercanos (preservada por t-SNE)
- **Global**: Estructura general del dataset (mejor preservada por PCA y UMAP)

## M

### Manifold

Superficie de baja dimensi√≥n embebida en un espacio de alta dimensi√≥n. t-SNE y UMAP asumen que los datos yacen en un manifold.

### min_dist (UMAP)

Par√°metro que controla qu√© tan compactos son los clusters en el embedding. Valores bajos = clusters m√°s densos.

## N

### n_components

N√∫mero de dimensiones objetivo en la reducci√≥n dimensional.

### n_neighbors (UMAP)

N√∫mero de vecinos cercanos usados para construir el grafo de vecindad. Controla el balance entre estructura local y global.

### Non-linear

T√©cnicas como t-SNE y UMAP que pueden capturar relaciones no lineales entre los datos.

## O

### Out-of-sample

Capacidad de transformar nuevos datos no vistos durante el entrenamiento. PCA y UMAP lo soportan; t-SNE no.

## P

### PCA (Principal Component Analysis)

T√©cnica lineal que encuentra las direcciones de m√°xima varianza en los datos y proyecta sobre ellas.

### Perplexity

Par√°metro de t-SNE que controla el n√∫mero efectivo de vecinos. T√≠picamente entre 5 y 50. Afecta el balance entre estructura local y global.

### Preservaci√≥n de Distancias

Propiedad de mantener las relaciones de distancia del espacio original en el espacio reducido.

## R

### Reconstrucci√≥n

Proceso de transformar datos reducidos de vuelta al espacio original. En PCA: $X_{rec} = X_{pca} \cdot V^T + \mu$.

### Reducci√≥n de Dimensionalidad

Proceso de reducir el n√∫mero de caracter√≠sticas de un dataset mientras se preserva informaci√≥n relevante.

## S

### Scree Plot

Gr√°fico que muestra la varianza explicada por cada componente principal. Usado para decidir cu√°ntos componentes retener.

### SVD (Singular Value Decomposition)

Descomposici√≥n matricial usada internamente por PCA para mayor estabilidad num√©rica.

$$A = U\Sigma V^T$$

## T

### t-Distribution (Distribuci√≥n t de Student)

Distribuci√≥n de probabilidad usada en t-SNE para el espacio de baja dimensi√≥n. Tiene colas m√°s pesadas que la gaussiana, lo que ayuda a separar clusters.

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

T√©cnica no lineal para visualizaci√≥n que preserva estructura local de los datos.

### Trustworthiness

M√©trica que eval√∫a qu√© tan bien se preservan las vecindades locales en el embedding. Valor entre 0 y 1; m√°s alto es mejor.

## U

### UMAP (Uniform Manifold Approximation and Projection)

T√©cnica no lineal moderna que combina buena preservaci√≥n de estructura local y global, con mejor rendimiento que t-SNE.

## V

### Varianza

Medida de dispersi√≥n de los datos:

$$\text{Var}(X) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$

### Varianza Acumulada

Suma acumulativa de varianza explicada. Usada para determinar cu√°ntos componentes retener (ej: 95% de varianza).

---

## üìä Comparaci√≥n R√°pida

| T√©cnica   | Tipo      | Velocidad     | Nuevos Datos | Mejor Para                          |
| --------- | --------- | ------------- | ------------ | ----------------------------------- |
| **PCA**   | Lineal    | ‚ö° Muy r√°pido | ‚úÖ S√≠        | Preprocesamiento, interpretabilidad |
| **t-SNE** | No lineal | üê¢ Lento      | ‚ùå No        | Visualizaci√≥n 2D/3D                 |
| **UMAP**  | No lineal | ‚ö° R√°pido     | ‚úÖ S√≠        | Visualizaci√≥n + pipelines ML        |

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior                         | üè† Semana                 | Siguiente ‚û°Ô∏è                         |
| ----------------------------------- | ------------------------- | ------------------------------------ |
| [Recursos](../4-recursos/README.md) | [Semana 17](../README.md) | [Semana 18](../../week-18/README.md) |
