# Ejercicio 04: Evaluaci√≥n B√°sica de Modelos

## üéØ Objetivo

Aprender a evaluar modelos de clasificaci√≥n usando diferentes m√©tricas: accuracy, precision, recall, F1-score y matriz de confusi√≥n.

## üìã Descripci√≥n

La accuracy no siempre es suficiente para evaluar un modelo. En este ejercicio aprender√°s m√©tricas m√°s completas que te ayudar√°n a entender el rendimiento real de tu modelo.

## üìö Conceptos Clave

- **Accuracy**: Proporci√≥n de predicciones correctas
- **Precision**: De los que predije positivo, ¬øcu√°ntos eran realmente positivos?
- **Recall**: De los positivos reales, ¬øcu√°ntos encontr√©?
- **F1-Score**: Media arm√≥nica de precision y recall
- **Matriz de Confusi√≥n**: Tabla que muestra TP, TN, FP, FN

## üõ†Ô∏è Instrucciones

Abre `starter/main.py` y sigue los pasos descomentando el c√≥digo indicado.

### Paso 1: Entrenar un Modelo

Preparar datos y entrenar un clasificador.

### Paso 2: Matriz de Confusi√≥n

```python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
```

### Paso 3: M√©tricas de Clasificaci√≥n

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
```

### Paso 4: Classification Report

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

## ‚úÖ Resultado Esperado

- Matriz de confusi√≥n interpretable
- Accuracy, Precision, Recall y F1-Score calculados
- Classification report completo por clase

## üîó Recursos

- [Sklearn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
