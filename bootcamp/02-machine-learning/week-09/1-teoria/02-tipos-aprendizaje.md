# ğŸ“Š Tipos de Aprendizaje en Machine Learning

## ğŸ¯ Objetivos

- Diferenciar aprendizaje supervisado, no supervisado y por refuerzo
- Identificar problemas de clasificaciÃ³n vs regresiÃ³n
- Conocer algoritmos comunes de cada tipo
- Saber cuÃ¡ndo aplicar cada tipo de aprendizaje

---

## 1. Panorama General

![Tipos de Aprendizaje en ML](../0-assets/04-tipos-aprendizaje.svg)

---

## 2. Aprendizaje Supervisado

### DefiniciÃ³n

El modelo aprende de datos **etiquetados** (conocemos la respuesta correcta).

```
Datos de entrada (X) + Etiquetas (y) â†’ Modelo â†’ Predicciones
```

### AnalogÃ­a

Como un estudiante que aprende con ejercicios resueltos:

- Ve el problema (X)
- Ve la respuesta correcta (y)
- Aprende el patrÃ³n

### Tipos de Problemas Supervisados

#### A) ClasificaciÃ³n

Predecir una **categorÃ­a/clase** discreta.

```python
# Ejemplo: Â¿El email es spam o no?
from sklearn.tree import DecisionTreeClassifier

X = [[0, 0], [1, 1], [0, 1], [1, 0]]  # Features
y = ['A', 'B', 'A', 'B']               # Clases

modelo = DecisionTreeClassifier()
modelo.fit(X, y)
print(modelo.predict([[0.5, 0.5]]))  # Predice clase
```

**Ejemplos de clasificaciÃ³n:**

- Spam vs No spam (binaria)
- DiagnÃ³stico de enfermedad (binaria)
- Tipo de flor (multiclase)
- DÃ­gitos escritos 0-9 (multiclase)

**Algoritmos comunes:**

- RegresiÃ³n LogÃ­stica
- Ãrboles de DecisiÃ³n
- Random Forest
- SVM (Support Vector Machines)
- K-Nearest Neighbors (KNN)
- Naive Bayes

#### B) RegresiÃ³n

Predecir un **valor numÃ©rico** continuo.

```python
# Ejemplo: Â¿CuÃ¡l serÃ¡ el precio de la casa?
from sklearn.linear_model import LinearRegression

X = [[50], [80], [120], [150]]  # Metros cuadrados
y = [100000, 180000, 250000, 300000]  # Precios

modelo = LinearRegression()
modelo.fit(X, y)
print(modelo.predict([[100]]))  # Predice precio
```

**Ejemplos de regresiÃ³n:**

- Precio de casa
- Temperatura maÃ±ana
- Ventas del prÃ³ximo mes
- Edad de una persona

**Algoritmos comunes:**

- RegresiÃ³n Lineal
- RegresiÃ³n Polinomial
- Ridge / Lasso
- Random Forest Regressor
- Gradient Boosting

### Tabla Comparativa: ClasificaciÃ³n vs RegresiÃ³n

| Aspecto        | ClasificaciÃ³n              | RegresiÃ³n                   |
| -------------- | -------------------------- | --------------------------- |
| Salida         | CategorÃ­a discreta         | Valor continuo              |
| Ejemplo        | spam / no spam             | precio = 150,000            |
| MÃ©trica tÃ­pica | Accuracy, F1-Score         | MSE, RÂ²                     |
| FunciÃ³n        | `predict()` devuelve clase | `predict()` devuelve nÃºmero |

---

## 3. Aprendizaje No Supervisado

### DefiniciÃ³n

El modelo encuentra patrones en datos **sin etiquetas**.

```
Datos de entrada (X) â†’ Modelo â†’ Estructuras/Patrones
```

### AnalogÃ­a

Como organizar una biblioteca sin saber las categorÃ­as:

- Agrupa libros similares
- Descubre patrones por tu cuenta

### Tipos de Problemas No Supervisados

#### A) Clustering (Agrupamiento)

Agrupar datos similares en clusters.

```python
# Ejemplo: Segmentar clientes
from sklearn.cluster import KMeans

X = [[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]]

modelo = KMeans(n_clusters=2, random_state=42)
modelo.fit(X)
print(modelo.labels_)  # [0, 0, 1, 1, 0, 1]
print(modelo.cluster_centers_)  # Centros de clusters
```

**Ejemplos de clustering:**

- SegmentaciÃ³n de clientes
- AgrupaciÃ³n de documentos
- DetecciÃ³n de anomalÃ­as
- CompresiÃ³n de imÃ¡genes

**Algoritmos comunes:**

- K-Means
- DBSCAN
- Hierarchical Clustering
- Gaussian Mixture Models

#### B) ReducciÃ³n de Dimensionalidad

Reducir el nÃºmero de features manteniendo informaciÃ³n importante.

```python
# Ejemplo: Reducir de muchas dimensiones a 2
from sklearn.decomposition import PCA

X = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]  # 4D

pca = PCA(n_components=2)
X_reducido = pca.fit_transform(X)  # Ahora 2D
print(X_reducido.shape)  # (3, 2)
```

**Ejemplos de reducciÃ³n:**

- VisualizaciÃ³n de datos de alta dimensiÃ³n
- CompresiÃ³n de datos
- EliminaciÃ³n de ruido
- Preprocesamiento para otros algoritmos

**Algoritmos comunes:**

- PCA (Principal Component Analysis)
- t-SNE
- UMAP
- Autoencoders

---

## 4. Aprendizaje por Refuerzo

### DefiniciÃ³n

Un **agente** aprende a tomar **acciones** en un **ambiente** para maximizar una **recompensa**.

```
Agente â† Observa estado â†’ Ambiente
   â”‚                         â”‚
   â””â”€â”€ Toma acciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                         â”‚
   â””â”€â”€ Recibe recompensa â”€â”€â”€â”€â”˜
```

### AnalogÃ­a

Como entrenar a un perro:

- El perro (agente) hace algo
- Recibe premio o castigo (recompensa)
- Aprende quÃ© acciones son buenas

### Ejemplo Conceptual

```python
# PseudocÃ³digo de Q-Learning
estado = ambiente.reset()

for episodio in range(1000):
    accion = agente.elegir_accion(estado)
    nuevo_estado, recompensa, terminado = ambiente.step(accion)
    agente.aprender(estado, accion, recompensa, nuevo_estado)
    estado = nuevo_estado
```

**Ejemplos de refuerzo:**

- Juegos (AlphaGo, Atari)
- RobÃ³tica (caminar, manipular)
- Trading algorÃ­tmico
- Control de sistemas

**Algoritmos comunes:**

- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient
- Actor-Critic (A3C)

---

## 5. ComparaciÃ³n de los Tres Tipos

| Aspecto        | Supervisado              | No Supervisado     | Refuerzo             |
| -------------- | ------------------------ | ------------------ | -------------------- |
| **Datos**      | Etiquetados              | Sin etiquetar      | InteracciÃ³n          |
| **Objetivo**   | Predecir etiqueta        | Encontrar patrones | Maximizar recompensa |
| **Feedback**   | Directo (etiquetas)      | No hay             | Recompensas          |
| **Ejemplos**   | ClasificaciÃ³n, RegresiÃ³n | Clustering, PCA    | Juegos, RobÃ³tica     |
| **Dificultad** | â­â­                     | â­â­â­             | â­â­â­â­             |

---

## 6. Â¿CuÃ¡ndo Usar Cada Tipo?

### Usar Supervisado cuando:

- âœ… Tienes datos etiquetados
- âœ… Sabes quÃ© quieres predecir
- âœ… Tienes suficientes ejemplos de cada clase

```python
# Â¿Tengo etiquetas?
if tengo_labels:
    if label_es_categoria:
        usar = "ClasificaciÃ³n"
    else:
        usar = "RegresiÃ³n"
```

### Usar No Supervisado cuando:

- âœ… No tienes etiquetas
- âœ… Quieres descubrir estructura en los datos
- âœ… Necesitas agrupar o simplificar datos

```python
# Â¿QuÃ© quiero hacer?
if quiero_agrupar:
    usar = "Clustering"
elif quiero_reducir_dimensiones:
    usar = "PCA / t-SNE"
elif quiero_detectar_anomalias:
    usar = "Isolation Forest"
```

### Usar Refuerzo cuando:

- âœ… El problema involucra decisiones secuenciales
- âœ… Hay un ambiente con el que interactuar
- âœ… Puedes definir recompensas claras

---

## 7. Algoritmos por Tipo (Resumen Visual)

```
SUPERVISADO                 NO SUPERVISADO           REFUERZO
â”œâ”€â”€ ClasificaciÃ³n           â”œâ”€â”€ Clustering           â”œâ”€â”€ Q-Learning
â”‚   â”œâ”€â”€ Logistic Reg.       â”‚   â”œâ”€â”€ K-Means          â”œâ”€â”€ DQN
â”‚   â”œâ”€â”€ Decision Tree       â”‚   â”œâ”€â”€ DBSCAN           â”œâ”€â”€ Policy Gradient
â”‚   â”œâ”€â”€ Random Forest       â”‚   â””â”€â”€ Hierarchical     â””â”€â”€ Actor-Critic
â”‚   â”œâ”€â”€ SVM                 â”‚
â”‚   â”œâ”€â”€ KNN                 â””â”€â”€ ReducciÃ³n Dim.
â”‚   â””â”€â”€ Naive Bayes             â”œâ”€â”€ PCA
â”‚                               â”œâ”€â”€ t-SNE
â””â”€â”€ RegresiÃ³n                   â””â”€â”€ UMAP
    â”œâ”€â”€ Linear Reg.
    â”œâ”€â”€ Polynomial
    â”œâ”€â”€ Ridge/Lasso
    â””â”€â”€ Gradient Boosting
```

---

## 8. Ejemplo PrÃ¡ctico: Mismo Dataset, Diferente Enfoque

```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression

# Dataset de clientes
clientes = pd.DataFrame({
    'edad': [25, 35, 45, 20, 55, 30],
    'ingresos': [30000, 50000, 80000, 20000, 90000, 45000],
    'compro': [0, 1, 1, 0, 1, 0]  # Etiqueta
})

X = clientes[['edad', 'ingresos']]

# SUPERVISADO: Predecir si comprarÃ¡
y = clientes['compro']
modelo_sup = LogisticRegression()
modelo_sup.fit(X, y)
print('PredicciÃ³n supervisada:', modelo_sup.predict([[40, 60000]]))

# NO SUPERVISADO: Segmentar clientes (ignoramos 'compro')
modelo_nosup = KMeans(n_clusters=2, random_state=42)
modelo_nosup.fit(X)
print('Clusters:', modelo_nosup.labels_)
```

---

## âœ… Resumen

| Tipo               | Datos         | Objetivo             | Algoritmos               |
| ------------------ | ------------- | -------------------- | ------------------------ |
| **Supervisado**    | Etiquetados   | Predecir             | RegresiÃ³n, ClasificaciÃ³n |
| **No Supervisado** | Sin etiquetar | Descubrir patrones   | Clustering, PCA          |
| **Refuerzo**       | InteracciÃ³n   | Maximizar recompensa | Q-Learning, DQN          |

---

## ğŸ”— NavegaciÃ³n

| Anterior                       | Siguiente                                         |
| ------------------------------ | ------------------------------------------------- |
| [â† QuÃ© es ML](01-que-es-ml.md) | [Flujo de Proyecto ML â†’](03-flujo-proyecto-ml.md) |
