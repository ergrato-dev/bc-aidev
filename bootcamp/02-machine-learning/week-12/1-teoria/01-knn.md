# K-Nearest Neighbors (KNN)

## üéØ Objetivos

- Entender el algoritmo KNN y su funcionamiento
- Conocer las m√©tricas de distancia
- Elegir el valor √≥ptimo de k
- Implementar KNN con scikit-learn

## üìã Contenido

### 1. ¬øQu√© es KNN?

K-Nearest Neighbors es un algoritmo de **aprendizaje basado en instancias** (lazy learning):

- **No entrena** un modelo expl√≠cito
- **Guarda** todos los datos de entrenamiento
- **Predice** bas√°ndose en los k vecinos m√°s cercanos

![KNN Distancias](../0-assets/01-knn-distancias.svg)

### 2. Funcionamiento

#### Clasificaci√≥n

1. Calcular distancia del nuevo punto a todos los puntos de entrenamiento
2. Seleccionar los k puntos m√°s cercanos
3. **Votaci√≥n mayoritaria**: la clase m√°s com√∫n entre los k vecinos

#### Regresi√≥n

1. Mismos pasos 1-2
2. **Promedio**: el valor predicho es la media de los k vecinos

```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

# Clasificaci√≥n
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)

# Regresi√≥n
knn_reg = KNeighborsRegressor(n_neighbors=5)
knn_reg.fit(X_train, y_train)
y_pred = knn_reg.predict(X_test)
```

### 3. M√©tricas de Distancia

| Distancia      | F√≥rmula                       | Uso                  |
| -------------- | ----------------------------- | -------------------- |
| **Euclidiana** | $\sqrt{\sum(x_i - y_i)^2}$    | Default, uso general |
| **Manhattan**  | $\sum\|x_i - y_i\|$           | Alta dimensionalidad |
| **Minkowski**  | $(\sum\|x_i - y_i\|^p)^{1/p}$ | Generalizaci√≥n       |

```python
# Euclidiana (default, p=2)
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')

# Manhattan (p=1)
knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')

# Minkowski con p personalizado
knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=3)
```

### 4. Eligiendo el k √ìptimo

El valor de k afecta el **tradeoff bias-variance**:

| k peque√±o           | k grande         |
| ------------------- | ---------------- |
| Baja bias           | Alta bias        |
| Alta varianza       | Baja varianza    |
| Overfitting         | Underfitting     |
| Fronteras complejas | Fronteras suaves |

#### Encontrar k √≥ptimo

```python
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

k_range = range(1, 31)
k_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
    k_scores.append(scores.mean())

# Visualizar
plt.figure(figsize=(10, 6))
plt.plot(k_range, k_scores, 'b-o')
plt.xlabel('Valor de k')
plt.ylabel('Accuracy (CV)')
plt.title('Elecci√≥n de k √≥ptimo')
plt.grid(True, alpha=0.3)
plt.show()

# Mejor k
best_k = k_range[k_scores.index(max(k_scores))]
print(f"Mejor k: {best_k}")
```

### 5. Importancia de la Normalizaci√≥n

KNN es **sensible a la escala** de las features. Features con valores grandes dominar√°n la distancia.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# ‚ö†Ô∏è SIEMPRE normalizar antes de KNN
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(n_neighbors=5))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

### 6. Curse of Dimensionality

Con muchas features:

- Las distancias se vuelven **similares** entre todos los puntos
- KNN pierde efectividad
- Soluci√≥n: reducci√≥n de dimensionalidad (PCA) o selecci√≥n de features

### 7. Par√°metros Importantes

| Par√°metro     | Descripci√≥n                             | Default     |
| ------------- | --------------------------------------- | ----------- |
| `n_neighbors` | N√∫mero de vecinos (k)                   | 5           |
| `weights`     | 'uniform' o 'distance'                  | 'uniform'   |
| `metric`      | M√©trica de distancia                    | 'minkowski' |
| `p`           | Par√°metro para Minkowski                | 2           |
| `algorithm`   | 'auto', 'ball_tree', 'kd_tree', 'brute' | 'auto'      |

#### Ponderaci√≥n por distancia

```python
# Vecinos m√°s cercanos tienen m√°s peso
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```

### 8. Ventajas y Desventajas

#### ‚úÖ Ventajas

- Simple de entender e implementar
- No requiere entrenamiento (lazy)
- Naturalmente multiclase
- No asume distribuci√≥n de datos

#### ‚ùå Desventajas

- Lento en predicci√≥n (O(n) por cada predicci√≥n)
- Sensible a features irrelevantes
- Requiere normalizaci√≥n
- Mal rendimiento en alta dimensionalidad
- Guarda todo el dataset en memoria

### 9. Ejemplo Completo

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipeline con normalizaci√≥n
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier())
])

# GridSearch para encontrar mejor k
param_grid = {
    'knn__n_neighbors': range(1, 21),
    'knn__weights': ['uniform', 'distance']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score CV: {grid_search.best_score_:.4f}")

# Evaluar
y_pred = grid_search.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo c√≥mo funciona KNN para clasificaci√≥n y regresi√≥n
- [ ] Conozco las diferentes m√©tricas de distancia
- [ ] S√© c√≥mo elegir el valor √≥ptimo de k
- [ ] Comprendo la importancia de normalizar features
- [ ] Puedo implementar KNN con sklearn

---

## üìö Recursos

- [KNN - sklearn](https://scikit-learn.org/stable/modules/neighbors.html)
- [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
