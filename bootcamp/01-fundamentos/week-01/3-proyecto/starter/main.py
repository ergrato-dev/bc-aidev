# ============================================
# PROYECTO SEMANA 01: Calculadora de M√©tricas ML
# ============================================
# Implementa las funciones marcadas con TODO
# Ejecuta el programa para verificar tu soluci√≥n
# ============================================

# ============================================
# DATOS DE PRUEBA
# ============================================
# Simulamos predicciones de un modelo de clasificaci√≥n binaria
# 1 = Positivo (ej: tiene la enfermedad)
# 0 = Negativo (ej: no tiene la enfermedad)

# Valores reales (ground truth)
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
          0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
          1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
          0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
          1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
          0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
          1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
          0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
          1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
          0, 1, 0, 0, 1, 1, 0, 1, 1, 0]

# Predicciones del modelo
y_pred = [1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
          0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
          1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
          0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
          1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
          0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
          1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
          0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
          1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
          0, 1, 0, 0, 1, 1, 0, 1, 1, 0]


# ============================================
# FUNCI√ìN 1: Contar Matriz de Confusi√≥n
# ============================================
def count_confusion_matrix(y_true: list, y_pred: list) -> dict:
    """
    Cuenta los valores de la matriz de confusi√≥n.
    
    Args:
        y_true: Lista de valores reales (0 o 1)
        y_pred: Lista de predicciones (0 o 1)
    
    Returns:
        dict: Diccionario con TP, TN, FP, FN
        
    Ejemplo:
        Si y_true=[1,0,1,0] y y_pred=[1,0,0,1]
        - TP=1 (predicho 1, real 1)
        - TN=1 (predicho 0, real 0)
        - FP=1 (predicho 1, real 0)
        - FN=1 (predicho 0, real 1)
    """
    # TODO: Inicializar contadores
    tp = 0  # True Positives
    tn = 0  # True Negatives
    fp = 0  # False Positives
    fn = 0  # False Negatives
    
    # TODO: Recorrer ambas listas y contar cada caso
    # Hint: Usa zip(y_true, y_pred) para iterar ambas listas
    # Hint: if real == 1 and pred == 1: es un TP
    
    # TODO: Retornar diccionario con los conteos
    return {"TP": tp, "TN": tn, "FP": fp, "FN": fn}


# ============================================
# FUNCI√ìN 2: Calcular Accuracy
# ============================================
def calculate_accuracy(confusion: dict) -> float:
    """
    Calcula el accuracy (exactitud) del modelo.
    
    F√≥rmula: (TP + TN) / (TP + TN + FP + FN)
    
    Args:
        confusion: Diccionario con TP, TN, FP, FN
    
    Returns:
        float: Accuracy entre 0 y 1
    """
    # TODO: Extraer valores del diccionario
    # TODO: Aplicar la f√≥rmula
    # TODO: Retornar el resultado
    
    return 0.0  # Placeholder


# ============================================
# FUNCI√ìN 3: Calcular Precision
# ============================================
def calculate_precision(confusion: dict) -> float:
    """
    Calcula la precision del modelo.
    
    F√≥rmula: TP / (TP + FP)
    
    Precision responde: De todos los que predije positivos,
    ¬øcu√°ntos realmente lo eran?
    
    Args:
        confusion: Diccionario con TP, TN, FP, FN
    
    Returns:
        float: Precision entre 0 y 1
    """
    # TODO: Extraer TP y FP
    # TODO: Cuidado con divisi√≥n por cero (si TP + FP == 0)
    # TODO: Aplicar la f√≥rmula
    
    return 0.0  # Placeholder


# ============================================
# FUNCI√ìN 4: Calcular Recall
# ============================================
def calculate_recall(confusion: dict) -> float:
    """
    Calcula el recall (sensibilidad) del modelo.
    
    F√≥rmula: TP / (TP + FN)
    
    Recall responde: De todos los positivos reales,
    ¬øcu√°ntos detect√© correctamente?
    
    Args:
        confusion: Diccionario con TP, TN, FP, FN
    
    Returns:
        float: Recall entre 0 y 1
    """
    # TODO: Extraer TP y FN
    # TODO: Cuidado con divisi√≥n por cero
    # TODO: Aplicar la f√≥rmula
    
    return 0.0  # Placeholder


# ============================================
# FUNCI√ìN 5: Calcular F1-Score
# ============================================
def calculate_f1_score(precision: float, recall: float) -> float:
    """
    Calcula el F1-Score (media arm√≥nica de precision y recall).
    
    F√≥rmula: 2 * (precision * recall) / (precision + recall)
    
    Args:
        precision: Valor de precision
        recall: Valor de recall
    
    Returns:
        float: F1-Score entre 0 y 1
    """
    # TODO: Cuidado con divisi√≥n por cero
    # TODO: Aplicar la f√≥rmula
    
    return 0.0  # Placeholder


# ============================================
# FUNCI√ìN 6: Clasificar Modelo
# ============================================
def classify_model(accuracy: float) -> str:
    """
    Clasifica el modelo seg√∫n su accuracy.
    
    Clasificaci√≥n:
    - >= 0.90: "üåü Excelente"
    - >= 0.80: "‚úÖ Bueno"  
    - >= 0.70: "‚ö†Ô∏è Aceptable"
    - < 0.70:  "‚ùå Necesita mejora"
    
    Args:
        accuracy: Valor de accuracy
    
    Returns:
        str: Clasificaci√≥n del modelo
    """
    # TODO: Usar if/elif/else para clasificar
    
    return ""  # Placeholder


# ============================================
# FUNCI√ìN 7: Generar Reporte
# ============================================
def generate_report(metrics: dict, classification: str) -> None:
    """
    Genera e imprime un reporte de evaluaci√≥n.
    
    Args:
        metrics: Diccionario con accuracy, precision, recall, f1
        classification: Clasificaci√≥n del modelo
    """
    # TODO: Imprimir reporte formateado
    # Incluir:
    # - Accuracy como porcentaje
    # - Precision como porcentaje
    # - Recall como porcentaje
    # - F1-Score
    # - Clasificaci√≥n
    # - Recomendaci√≥n basada en la clasificaci√≥n
    
    print("TODO: Implementar reporte")


# ============================================
# PROGRAMA PRINCIPAL
# ============================================
def main():
    print("=" * 60)
    print("ü§ñ CALCULADORA DE M√âTRICAS ML")
    print("=" * 60)
    print()
    
    # Paso 1: Calcular matriz de confusi√≥n
    print("--- Matriz de Confusi√≥n ---")
    confusion = count_confusion_matrix(y_true, y_pred)
    print(f"TP (True Positives): {confusion['TP']}")
    print(f"TN (True Negatives): {confusion['TN']}")
    print(f"FP (False Positives): {confusion['FP']}")
    print(f"FN (False Negatives): {confusion['FN']}")
    print()
    
    # Paso 2: Calcular m√©tricas
    print("--- M√©tricas Calculadas ---")
    accuracy = calculate_accuracy(confusion)
    precision = calculate_precision(confusion)
    recall = calculate_recall(confusion)
    f1 = calculate_f1_score(precision, recall)
    
    print(f"Accuracy:  {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall:    {recall:.2f}")
    print(f"F1-Score:  {f1:.2f}")
    print()
    
    # Paso 3: Clasificar modelo
    print("--- Clasificaci√≥n del Modelo ---")
    classification = classify_model(accuracy)
    print(classification)
    print()
    
    # Paso 4: Generar reporte
    metrics = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }
    generate_report(metrics, classification)


# Ejecutar programa
if __name__ == "__main__":
    main()
