# üìñ Glosario - Semana 11: √Årboles de Decisi√≥n y Random Forest

## A

### Accuracy (Exactitud)

Proporci√≥n de predicciones correctas sobre el total de predicciones.
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

### Agregaci√≥n

Proceso de combinar predicciones de m√∫ltiples modelos. En Random Forest: **votaci√≥n** (clasificaci√≥n) o **promedio** (regresi√≥n).

## B

### Bagging (Bootstrap Aggregating)

T√©cnica de ensemble que:

1. Crea m√∫ltiples subconjuntos de datos mediante bootstrap sampling
2. Entrena un modelo en cada subconjunto
3. Agrega las predicciones

```python
from sklearn.ensemble import BaggingClassifier
```

### Bootstrap Sampling

Muestreo **con reemplazo** del dataset original. Cada muestra bootstrap tiene el mismo tama√±o que el original, pero algunas observaciones se repiten y otras no aparecen (~37% quedan fuera).

## C

### CART (Classification and Regression Trees)

Algoritmo usado por sklearn para construir √°rboles de decisi√≥n. Caracter√≠sticas:

- Divisiones binarias
- Usa Gini o Entropy para clasificaci√≥n
- Usa MSE o MAE para regresi√≥n

### ccp_alpha (Cost-Complexity Pruning)

Par√°metro de poda que penaliza la complejidad del √°rbol. Valores mayores = √°rboles m√°s simples.

```python
tree = DecisionTreeClassifier(ccp_alpha=0.01)
```

### Criterion (Criterio)

M√©trica usada para evaluar la calidad de una divisi√≥n:

- **Clasificaci√≥n**: `'gini'` (default), `'entropy'`
- **Regresi√≥n**: `'squared_error'` (default), `'absolute_error'`

## D

### Decision Boundary (Frontera de Decisi√≥n)

L√≠mite que separa las clases en el espacio de features. En √°rboles, son fronteras **paralelas a los ejes** (rectangulares).

### Decision Tree (√Årbol de Decisi√≥n)

Modelo que aprende reglas de decisi√≥n en forma de √°rbol:

- **Nodos internos**: condiciones (feature ‚â§ threshold)
- **Hojas**: predicciones

## E

### Ensemble (Conjunto)

Combinaci√≥n de m√∫ltiples modelos para mejorar predicciones. Tipos principales:

- **Bagging**: Random Forest
- **Boosting**: XGBoost, AdaBoost
- **Stacking**: Meta-aprendizaje

### Entropy (Entrop√≠a)

Medida de desorden o incertidumbre basada en teor√≠a de informaci√≥n.
$$Entropy = -\sum_{i=1}^{C} p_i \log_2(p_i)$$

| Valor | Significado                  |
| ----- | ---------------------------- |
| 0     | Nodo puro                    |
| 1     | M√°xima incertidumbre (50/50) |

## F

### Feature Importance (Importancia de Features)

Medida de cu√°nto contribuye cada feature a las predicciones. En Random Forest: suma ponderada de las reducciones de impureza.

```python
importance = model.feature_importances_
```

### Feature Subsampling

En Random Forest, solo se consideran `max_features` features aleatorias en cada divisi√≥n. Reduce correlaci√≥n entre √°rboles.

## G

### Gini Impurity (Impureza de Gini)

Medida de impureza que calcula la probabilidad de clasificar incorrectamente.
$$Gini = 1 - \sum_{i=1}^{C} p_i^2$$

| Valor | Significado                     |
| ----- | ------------------------------- |
| 0     | Nodo puro                       |
| 0.5   | M√°xima impureza (50/50 binario) |

### Greedy Algorithm (Algoritmo Codicioso)

Estrategia de CART que elige la **mejor divisi√≥n local** en cada paso, sin considerar divisiones futuras.

## H

### Hiperpar√°metro

Par√°metro que se configura antes del entrenamiento (no se aprende de los datos).

**Principales en Random Forest:**

- `n_estimators`: n√∫mero de √°rboles
- `max_depth`: profundidad m√°xima
- `min_samples_split`: m√≠nimo para dividir
- `max_features`: features por divisi√≥n

## I

### Information Gain (Ganancia de Informaci√≥n)

Reducci√≥n de entrop√≠a despu√©s de una divisi√≥n.
$$IG = Entropy(padre) - \sum \frac{n_j}{n} Entropy(hijo_j)$$

### Internal Node (Nodo Interno)

Nodo que contiene una condici√≥n de divisi√≥n (no es hoja).

## L

### Leaf Node (Hoja)

Nodo terminal que contiene la predicci√≥n final (clase mayoritaria o valor promedio).

## M

### max_depth

Profundidad m√°xima del √°rbol. Limitar previene overfitting.

```python
tree = DecisionTreeClassifier(max_depth=5)
```

### max_features

N√∫mero de features a considerar en cada divisi√≥n:

- `'sqrt'`: ‚àön_features (default clasificaci√≥n)
- `'log2'`: log‚ÇÇ(n_features)
- `None`: todos los features

### min_samples_leaf

N√∫mero m√≠nimo de muestras requeridas en un nodo hoja.

### min_samples_split

N√∫mero m√≠nimo de muestras para dividir un nodo interno.

## N

### n_estimators

N√∫mero de √°rboles en Random Forest. M√°s √°rboles = mejor rendimiento (hasta cierto punto) pero m√°s lento.

```python
rf = RandomForestClassifier(n_estimators=100)
```

### n_jobs

N√∫mero de cores para paralelizaci√≥n. `-1` usa todos los disponibles.

## O

### OOB (Out-of-Bag)

Muestras no usadas en un bootstrap particular (~37%). Permiten validaci√≥n interna.

### OOB Score

Estimaci√≥n del error de generalizaci√≥n usando muestras OOB. Similar a cross-validation pero "gratis".

```python
rf = RandomForestClassifier(oob_score=True)
rf.fit(X, y)
print(rf.oob_score_)
```

### Overfitting (Sobreajuste)

Cuando el modelo memoriza los datos de entrenamiento y no generaliza. Se√±ales:

- Train accuracy >> Test accuracy
- √Årbol muy profundo

## P

### Pruning (Poda)

T√©cnica para simplificar √°rboles y evitar overfitting:

- **Pre-pruning**: limitar durante construcci√≥n (`max_depth`, `min_samples_split`)
- **Post-pruning**: podar despu√©s de construir (`ccp_alpha`)

## R

### Random Forest

Ensemble de √°rboles de decisi√≥n que usa:

1. **Bagging**: bootstrap sampling
2. **Feature randomness**: subconjunto aleatorio de features por split
3. **Agregaci√≥n**: votaci√≥n o promedio

```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100)
```

### random_state

Semilla para reproducibilidad. Mismo valor = mismos resultados.

## S

### Split (Divisi√≥n)

Partici√≥n de un nodo en dos hijos basada en una condici√≥n (feature ‚â§ threshold).

## T

### Threshold (Umbral)

Valor de corte para una feature en una divisi√≥n. Ejemplo: "edad ‚â§ 30".

### Tree Depth (Profundidad)

N√∫mero de niveles desde la ra√≠z hasta la hoja m√°s profunda.

## V

### Variance (Varianza)

Sensibilidad del modelo a cambios en los datos de entrenamiento. √Årboles individuales tienen **alta varianza**; Random Forest la reduce.

### Voting (Votaci√≥n)

M√©todo de agregaci√≥n en clasificaci√≥n:

- **Hard voting**: clase m√°s votada
- **Soft voting**: promedio de probabilidades

---

## üìä Tabla Resumen: Hiperpar√°metros

| Par√°metro           | Efecto al aumentar      | Default |
| ------------------- | ----------------------- | ------- |
| `max_depth`         | ‚Üë Overfitting           | None    |
| `min_samples_split` | ‚Üì Overfitting           | 2       |
| `min_samples_leaf`  | ‚Üì Overfitting           | 1       |
| `n_estimators`      | ‚Üë Performance, ‚Üë Tiempo | 100     |
| `max_features`      | ‚Üë Correlaci√≥n √°rboles   | 'sqrt'  |
| `ccp_alpha`         | ‚Üì Complejidad           | 0.0     |

---

## üîó Referencias

- [sklearn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [sklearn Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)
- [Random Forests - Breiman 2001](https://link.springer.com/article/10.1023/A:1010933404324)
