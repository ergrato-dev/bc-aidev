# ğŸŒ² Semana 11: Ãrboles de DecisiÃ³n y Random Forest

## ğŸ“‹ DescripciÃ³n

Esta semana exploramos los **modelos basados en Ã¡rboles**, fundamentales en Machine Learning por su interpretabilidad y potencia. Comenzamos con Ã¡rboles de decisiÃ³n individuales y avanzamos hacia Random Forest, uno de los algoritmos mÃ¡s utilizados en la industria.

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar esta semana, serÃ¡s capaz de:

- âœ… Entender cÃ³mo funcionan los Ã¡rboles de decisiÃ³n (CART)
- âœ… Aplicar criterios de divisiÃ³n: Gini e Information Gain (Entropy)
- âœ… Controlar overfitting con poda y lÃ­mites de profundidad
- âœ… Implementar Random Forest para clasificaciÃ³n y regresiÃ³n
- âœ… Interpretar feature importance en modelos de ensamble
- âœ… Visualizar Ã¡rboles de decisiÃ³n con sklearn y graphviz
- âœ… Ajustar hiperparÃ¡metros clave (n_estimators, max_depth, etc.)

## ğŸ“š Requisitos Previos

- Semana 09: Fundamentos de ML
- Semana 10: RegresiÃ³n lineal y logÃ­stica
- Conocimiento de mÃ©tricas de evaluaciÃ³n (accuracy, precision, recall)

## ğŸ—‚ï¸ Estructura de la Semana

```
week-11/
â”œâ”€â”€ README.md
â”œâ”€â”€ rubrica-evaluacion.md
â”œâ”€â”€ 0-assets/                    # Diagramas SVG
â”œâ”€â”€ 1-teoria/
â”‚   â”œâ”€â”€ 01-arboles-decision.md   # Fundamentos CART
â”‚   â”œâ”€â”€ 02-criterios-division.md # Gini vs Entropy
â”‚   â”œâ”€â”€ 03-random-forest.md      # Bagging y ensambles
â”‚   â””â”€â”€ 04-hiperparametros.md    # Tuning y validaciÃ³n
â”œâ”€â”€ 2-practicas/
â”‚   â”œâ”€â”€ ejercicio-01-arbol-clasificacion/
â”‚   â”œâ”€â”€ ejercicio-02-arbol-regresion/
â”‚   â”œâ”€â”€ ejercicio-03-random-forest/
â”‚   â””â”€â”€ ejercicio-04-feature-importance/
â”œâ”€â”€ 3-proyecto/                  # ClasificaciÃ³n de especies (Iris/Wine)
â”œâ”€â”€ 4-recursos/
â”‚   â”œâ”€â”€ ebooks-free/
â”‚   â”œâ”€â”€ videografia/
â”‚   â””â”€â”€ webgrafia/
â””â”€â”€ 5-glosario/
```

## ğŸ“ Contenidos

### TeorÃ­a (1.5 horas)

| Archivo                  | Tema                                     | DuraciÃ³n |
| ------------------------ | ---------------------------------------- | -------- |
| 01-arboles-decision.md   | Estructura, nodos, hojas, predicciÃ³n     | 25 min   |
| 02-criterios-division.md | Gini Impurity, Entropy, Information Gain | 25 min   |
| 03-random-forest.md      | Bagging, OOB score, ensambles            | 25 min   |
| 04-hiperparametros.md    | max_depth, n_estimators, GridSearchCV    | 15 min   |

### PrÃ¡cticas (2.5 horas)

| Ejercicio    | DescripciÃ³n                           | DuraciÃ³n |
| ------------ | ------------------------------------- | -------- |
| ejercicio-01 | Ãrbol de clasificaciÃ³n (Iris dataset) | 35 min   |
| ejercicio-02 | Ãrbol de regresiÃ³n (precios)          | 35 min   |
| ejercicio-03 | Random Forest clasificaciÃ³n           | 40 min   |
| ejercicio-04 | Feature importance y selecciÃ³n        | 40 min   |

### Proyecto (2 horas)

**Clasificador de Vinos**: Construir un modelo Random Forest para clasificar tipos de vino usando el Wine dataset de sklearn. Objetivo: accuracy â‰¥ 0.92 en test.

## â±ï¸ DistribuciÃ³n del Tiempo (6 horas)

| Actividad | Tiempo    |
| --------- | --------- |
| TeorÃ­a    | 1.5 h     |
| PrÃ¡cticas | 2.5 h     |
| Proyecto  | 2.0 h     |
| **Total** | **6.0 h** |

## ğŸ“Œ Entregables

1. **Conocimiento ğŸ§ **: Cuestionario sobre Ã¡rboles y criterios de divisiÃ³n
2. **DesempeÃ±o ğŸ’ª**: 4 ejercicios prÃ¡cticos completados
3. **Producto ğŸ“¦**: Proyecto de clasificaciÃ³n con accuracy â‰¥ 0.92

## ğŸ”— NavegaciÃ³n

| â† Anterior                                   |           Inicio            |                                  Siguiente â†’ |
| :------------------------------------------- | :-------------------------: | -------------------------------------------: |
| [Semana 10: RegresiÃ³n](../week-10/README.md) | [Bootcamp](../../README.md) | [Semana 12: SVM y KNN](../week-12/README.md) |

---

## ğŸ’¡ Conceptos Clave

```
Ãrbol de DecisiÃ³n
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nodo RaÃ­z  â”‚ â† Primera divisiÃ³n (feature mÃ¡s importante)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚Nodo â”‚ â”‚Nodo â”‚ â† Nodos internos (mÃ¡s divisiones)
â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
   â”‚       â”‚
â”Œâ”€â”€â”´â”€â”€â” â”Œâ”€â”€â”´â”€â”€â”
â”‚Hoja â”‚ â”‚Hoja â”‚ â† PredicciÃ³n final
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

### FÃ³rmulas Principales

**Gini Impurity**:
$$Gini = 1 - \sum_{i=1}^{C} p_i^2$$

**Entropy**:
$$Entropy = -\sum_{i=1}^{C} p_i \log_2(p_i)$$

**Information Gain**:
$$IG = Entropy(parent) - \sum \frac{n_{child}}{n_{parent}} \cdot Entropy(child)$$

---

_Semana 11 de 36 | MÃ³dulo: Machine Learning_
