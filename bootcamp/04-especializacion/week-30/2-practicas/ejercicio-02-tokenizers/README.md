# ğŸ”¤ Ejercicio 02: Tokenizers de Hugging Face

## ğŸ¯ Objetivo

Dominar el uso de tokenizers: cargar, tokenizar, manejar padding y attention masks.

---

## ğŸ“‹ DescripciÃ³n

En este ejercicio aprenderÃ¡s a usar AutoTokenizer, entender el proceso de tokenizaciÃ³n, manejar padding y truncation, y trabajar con tokens especiales.

---

## ğŸ”§ Pasos del Ejercicio

### Paso 1: Cargar Tokenizer

Usar AutoTokenizer para cargar cualquier tokenizer:

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

**Abre `starter/main.py`** y descomenta la secciÃ³n correspondiente.

### Paso 2: TokenizaciÃ³n BÃ¡sica

Diferentes mÃ©todos de tokenizaciÃ³n:

```python
# Solo tokens (strings)
tokens = tokenizer.tokenize("Hello world")

# Tokens + IDs
ids = tokenizer.encode("Hello world")

# Encoding completo (dict)
encoding = tokenizer("Hello world")
```

### Paso 3: DecodificaciÃ³n

Convertir IDs de vuelta a texto:

```python
text = tokenizer.decode([101, 7592, 2088, 102])
```

### Paso 4: Padding y Truncation

Manejar secuencias de diferentes longitudes:

```python
encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)
```

### Paso 5: Attention Mask

Entender quÃ© tokens son reales vs padding:

```python
# attention_mask: 1 = token real, 0 = padding
print(encoding['attention_mask'])
```

### Paso 6: Tokens Especiales

Conocer los tokens especiales del modelo:

```python
print(tokenizer.cls_token)  # [CLS]
print(tokenizer.sep_token)  # [SEP]
print(tokenizer.pad_token)  # [PAD]
```

---

## ğŸ“ Estructura

```
ejercicio-02-tokenizers/
â”œâ”€â”€ README.md
â””â”€â”€ starter/
    â””â”€â”€ main.py
```

---

## â–¶ï¸ EjecuciÃ³n

```bash
cd bootcamp/04-especializacion/week-30/2-practicas/ejercicio-02-tokenizers
python starter/main.py
```

---

## âœ… Criterios de Ã‰xito

- [ ] Puedo cargar tokenizers con AutoTokenizer
- [ ] Entiendo tokenize, encode, decode
- [ ] SÃ© aplicar padding y truncation
- [ ] Comprendo attention_mask
- [ ] Conozco los tokens especiales

---

## ğŸ”— Recursos

- [Tokenizers Documentation](https://huggingface.co/docs/transformers/main_classes/tokenizer)
- [Preprocessing Data](https://huggingface.co/docs/transformers/preprocessing)
