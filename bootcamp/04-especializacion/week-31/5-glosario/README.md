# üìñ Glosario - Semana 31: LLMs

T√©rminos clave de Large Language Models, ordenados alfab√©ticamente.

---

## A

### Attention Mechanism
Mecanismo que permite al modelo enfocarse en partes relevantes del input. En Transformers, calcula pesos de atenci√≥n entre todos los tokens.

**F√≥rmula**:
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### Autoregressive
Tipo de modelo que genera tokens uno por uno, condicionando cada token en los anteriores. GPT es autoregresivo.

```
Input:  "The cat"
Output: "The cat sat" ‚Üí "The cat sat on" ‚Üí ...
```

---

## B

### BERT (Bidirectional Encoder Representations from Transformers)
Modelo encoder-only de Google que usa atenci√≥n bidireccional. Excelente para tareas de clasificaci√≥n y NLU.

### BPE (Byte Pair Encoding)
Algoritmo de tokenizaci√≥n que aprende subpalabras frecuentes del corpus.

```
"unhappiness" ‚Üí ["un", "happiness"] o ["un", "happ", "iness"]
```

---

## C

### Causal Attention
Atenci√≥n que solo mira tokens anteriores (izquierda). Usada en modelos de lenguaje generativos.

```
Posici√≥n 3 puede ver: [1, 2, 3]
Posici√≥n 3 NO ve: [4, 5, ...]
```

### Chain-of-Thought (CoT)
T√©cnica de prompting que hace al modelo razonar paso a paso antes de dar la respuesta final.

### Context Window
N√∫mero m√°ximo de tokens que el modelo puede procesar. GPT-4 tiene ~128K tokens.

---

## D

### Decoder-Only
Arquitectura que solo usa el decoder del Transformer. GPT, LLaMA, Mistral son decoder-only.

### Distillation (Destilaci√≥n)
T√©cnica para transferir conocimiento de un modelo grande (teacher) a uno peque√±o (student).

---

## E

### Embedding
Representaci√≥n vectorial densa de tokens. Cada token se mapea a un vector de dimensi√≥n fija.

```python
# Token "hello" ‚Üí vector de 768 dimensiones
embedding = [0.23, -0.15, 0.67, ..., 0.42]
```

### Encoder-Only
Arquitectura que solo usa el encoder. BERT, RoBERTa son encoder-only.

---

## F

### Few-Shot Learning
Capacidad del modelo para realizar tareas nuevas con pocos ejemplos en el prompt.

```
Ejemplo 1: X ‚Üí Y
Ejemplo 2: A ‚Üí B
Nuevo: C ‚Üí ?  (El modelo predice)
```

### Fine-tuning
Proceso de continuar entrenando un modelo pre-entrenado en datos espec√≠ficos de una tarea.

### FP16 (Float16)
Precisi√≥n de punto flotante de 16 bits. Reduce memoria y acelera entrenamiento.

---

## G

### GPT (Generative Pre-trained Transformer)
Familia de modelos de OpenAI basados en arquitectura decoder-only autoregresiva.

### Gradient Accumulation
T√©cnica para simular batches grandes acumulando gradientes de varios mini-batches.

---

## H

### Hallucination (Alucinaci√≥n)
Cuando el modelo genera informaci√≥n falsa o inventada que parece plausible.

### Hidden State
Representaci√≥n interna del modelo en cada capa. Contiene informaci√≥n sem√°ntica del input.

---

## I

### In-Context Learning
Capacidad de aprender de ejemplos proporcionados en el prompt sin actualizar pesos.

### Inference
Proceso de usar un modelo entrenado para generar predicciones/texto.

### Instruction Tuning
Fine-tuning espec√≠fico para seguir instrucciones en lenguaje natural.

---

## K

### KV Cache
Cache de Keys y Values calculados para tokens anteriores. Acelera generaci√≥n autoregresiva.

---

## L

### LLM (Large Language Model)
Modelo de lenguaje con billones de par√°metros entrenado en texto masivo. Ejemplos: GPT-4, LLaMA, Claude.

### LoRA (Low-Rank Adaptation)
T√©cnica de fine-tuning eficiente que solo entrena matrices de bajo rango a√±adidas al modelo.

**F√≥rmula**:
$$W' = W + BA$$

Donde $B \in \mathbb{R}^{d \times r}$ y $A \in \mathbb{R}^{r \times d}$ con $r \ll d$

---

## M

### MLM (Masked Language Modeling)
Objetivo de entrenamiento de BERT. Predecir tokens enmascarados en el input.

```
Input:  "The [MASK] sat on the mat"
Target: "cat"
```

### Multi-Head Attention
Atenci√≥n ejecutada en paralelo con diferentes proyecciones. Captura relaciones diversas.

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

---

## N

### Next Token Prediction
Objetivo de entrenamiento de GPT. Predecir el siguiente token dado el contexto.

---

## O

### Open-Weights
Modelos cuyo pesos est√°n disponibles p√∫blicamente. Ejemplo: LLaMA, Mistral.

---

## P

### PEFT (Parameter-Efficient Fine-Tuning)
Familia de t√©cnicas que ajustan solo un peque√±o porcentaje de par√°metros. Incluye LoRA, QLoRA, adapters.

### Perplexity
M√©trica de evaluaci√≥n de modelos de lenguaje. Menor perplexity = mejor modelo.

$$PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log P(x_i|x_{<i})\right)$$

### Pre-training
Fase inicial de entrenamiento en grandes cantidades de texto sin supervisi√≥n espec√≠fica.

### Prompt
Texto de entrada que se proporciona al modelo para generar una respuesta.

### Prompt Engineering
Arte de dise√±ar prompts efectivos para obtener mejores respuestas del modelo.

---

## Q

### QLoRA
LoRA combinado con cuantizaci√≥n a 4 bits. Permite fine-tuning con muy poca memoria.

### Quantization (Cuantizaci√≥n)
Reducir precisi√≥n de pesos (32-bit ‚Üí 8-bit ‚Üí 4-bit) para reducir memoria.

---

## R

### RLHF (Reinforcement Learning from Human Feedback)
T√©cnica de alineaci√≥n que usa feedback humano para mejorar respuestas del modelo.

### Role Prompting
T√©cnica que asigna un rol/persona espec√≠fica al modelo en el prompt.

```
"You are an expert Python programmer..."
```

---

## S

### Self-Attention
Atenci√≥n donde Query, Key, Value vienen de la misma secuencia. Base de Transformers.

### SFT (Supervised Fine-Tuning)
Fine-tuning supervisado con pares (input, output) etiquetados.

### System Prompt
Prompt especial que define el comportamiento general del asistente.

---

## T

### Temperature
Par√°metro que controla aleatoriedad en la generaci√≥n.
- Baja (0.1): Determinista
- Alta (1.0+): Creativo/aleatorio

$$P(x_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

### Token
Unidad b√°sica de texto que procesa el modelo. Puede ser palabra, subpalabra o car√°cter.

### Top-K Sampling
M√©todo que limita la generaci√≥n a los K tokens m√°s probables.

### Top-P (Nucleus) Sampling
M√©todo que selecciona tokens hasta acumular probabilidad P.

### Transformer
Arquitectura neural basada en atenci√≥n. Base de todos los LLMs modernos.

---

## Z

### Zero-Shot Learning
Capacidad del modelo para realizar tareas sin ejemplos previos en el prompt.

---

## üìä Comparativa de Arquitecturas

| Caracter√≠stica | GPT (Decoder) | BERT (Encoder) | T5 (Enc-Dec) |
|---------------|---------------|----------------|--------------|
| Atenci√≥n | Causal | Bidireccional | Ambas |
| Uso principal | Generaci√≥n | Clasificaci√≥n | Seq2Seq |
| Pre-training | Next token | MLM | Span corruption |
| Ejemplos | GPT-4, LLaMA | BERT, RoBERTa | T5, BART |

---

## üìà M√©tricas Comunes

| M√©trica | Uso | Mejor si... |
|---------|-----|-------------|
| Perplexity | Calidad del modelo | Menor |
| BLEU | Traducci√≥n/generaci√≥n | Mayor |
| ROUGE | Resumen | Mayor |
| Accuracy | Clasificaci√≥n | Mayor |

---

_Glosario Semana 31 - Bootcamp IA_
