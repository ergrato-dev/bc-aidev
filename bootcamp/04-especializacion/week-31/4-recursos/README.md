# üìö Recursos Adicionales - Semana 31

## üìñ ebooks-free

### LLMs y Transformers

1. **"Attention Is All You Need"** (Paper original)
   - [arXiv](https://arxiv.org/abs/1706.03762)
   - Fundamento de todos los LLMs modernos

2. **"Language Models are Few-Shot Learners"** (GPT-3)
   - [arXiv](https://arxiv.org/abs/2005.14165)
   - Introduce in-context learning

3. **"BERT: Pre-training of Deep Bidirectional Transformers"**
   - [arXiv](https://arxiv.org/abs/1810.04805)
   - Arquitectura encoder bidireccional

4. **"LoRA: Low-Rank Adaptation of Large Language Models"**
   - [arXiv](https://arxiv.org/abs/2106.09685)
   - Fine-tuning eficiente

5. **"The Illustrated Transformer"** - Jay Alammar
   - [jalammar.github.io](https://jalammar.github.io/illustrated-transformer/)
   - Mejor visualizaci√≥n de Transformers

---

## üé• videografia

### Cursos y Tutoriales

1. **"Let's Build GPT"** - Andrej Karpathy
   - [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)
   - Construir GPT desde cero
   - Duraci√≥n: ~2 horas

2. **"Intro to Large Language Models"** - Andrej Karpathy
   - [YouTube](https://www.youtube.com/watch?v=zjkBMFhNj_g)
   - Visi√≥n general de LLMs
   - Duraci√≥n: ~1 hora

3. **"Fine-tuning LLMs"** - Hugging Face
   - [YouTube](https://www.youtube.com/watch?v=eC6Hd1hFvos)
   - Tutorial pr√°ctico de fine-tuning
   - Duraci√≥n: ~30 minutos

4. **"Prompt Engineering"** - DeepLearning.AI
   - [Coursera](https://www.coursera.org/learn/prompt-engineering)
   - Curso completo de prompt engineering
   - Duraci√≥n: ~4 horas

5. **"State of GPT"** - Andrej Karpathy (Microsoft Build)
   - [YouTube](https://www.youtube.com/watch?v=bZQun8Y4L2A)
   - Estado actual de GPT y entrenamiento
   - Duraci√≥n: ~45 minutos

---

## üåê webgrafia

### Documentaci√≥n Oficial

- **OpenAI Documentation**: https://platform.openai.com/docs/
- **Hugging Face Transformers**: https://huggingface.co/docs/transformers/
- **PEFT Library**: https://huggingface.co/docs/peft/
- **TRL (Transformer Reinforcement Learning)**: https://huggingface.co/docs/trl/

### Gu√≠as y Tutoriales

- **Prompt Engineering Guide**: https://www.promptingguide.ai/
- **LLM Course**: https://github.com/mlabonne/llm-course
- **OpenAI Cookbook**: https://cookbook.openai.com/

### Modelos y Benchmarks

- **Hugging Face Model Hub**: https://huggingface.co/models
- **Open LLM Leaderboard**: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
- **LMSys Chatbot Arena**: https://chat.lmsys.org/

### Herramientas

- **Ollama** (LLMs locales): https://ollama.ai/
- **LangChain**: https://python.langchain.com/
- **LlamaIndex**: https://www.llamaindex.ai/
- **vLLM** (Serving eficiente): https://vllm.ai/

### Blogs y Art√≠culos

- **Lilian Weng's Blog**: https://lilianweng.github.io/
- **Sebastian Raschka**: https://sebastianraschka.com/blog/
- **Chip Huyen**: https://huyenchip.com/blog/

---

## üõ†Ô∏è Herramientas Recomendadas

### Para Experimentar

| Herramienta | Uso | Link |
|-------------|-----|------|
| Ollama | LLMs locales | https://ollama.ai/ |
| text-generation-webui | UI para LLMs | https://github.com/oobabooga/text-generation-webui |
| LM Studio | LLMs locales con UI | https://lmstudio.ai/ |

### Para Fine-tuning

| Herramienta | Uso | Link |
|-------------|-----|------|
| Axolotl | Fine-tuning simplificado | https://github.com/OpenAccess-AI-Collective/axolotl |
| Unsloth | Fine-tuning optimizado | https://github.com/unslothai/unsloth |
| LLaMA-Factory | UI para fine-tuning | https://github.com/hiyouga/LLaMA-Factory |

### Para Deployment

| Herramienta | Uso | Link |
|-------------|-----|------|
| vLLM | Serving de alto rendimiento | https://vllm.ai/ |
| TGI | Text Generation Inference | https://huggingface.co/docs/text-generation-inference |
| llama.cpp | Inferencia en CPU | https://github.com/ggerganov/llama.cpp |

---

## üìä Modelos Recomendados para Aprender

### Peque√±os (< 3B par√°metros)

- **GPT-2** (124M - 1.5B): Ideal para experimentar
- **Phi-2** (2.7B): Muy capaz para su tama√±o
- **Gemma-2B**: Modelo abierto de Google

### Medianos (3B - 13B)

- **LLaMA-2-7B**: Buen balance calidad/recursos
- **Mistral-7B**: Excelente rendimiento
- **Qwen-7B**: Multiling√ºe

### Para Fine-tuning

- **LLaMA-2-7B** + LoRA: Requiere ~16GB VRAM
- **Phi-2** + LoRA: Requiere ~8GB VRAM
- **TinyLlama** (1.1B): Puede correr en CPU

---

## üìù Datasets para Pr√°ctica

| Dataset | Uso | Link |
|---------|-----|------|
| Alpaca | Instruction following | https://huggingface.co/datasets/tatsu-lab/alpaca |
| Dolly | Instruction tuning | https://huggingface.co/datasets/databricks/databricks-dolly-15k |
| OpenAssistant | Conversaciones | https://huggingface.co/datasets/OpenAssistant/oasst1 |
| LIMA | Datos de alta calidad | https://huggingface.co/datasets/GAIR/lima |

---

## üí° Tips de Estudio

1. **Empieza con GPT-2**: Es peque√±o y f√°cil de experimentar
2. **Usa Ollama**: Para probar modelos sin c√≥digo complejo
3. **Practica prompts**: La mayor√≠a de tareas no requieren fine-tuning
4. **Lee papers gradualmente**: Empieza con blogs, luego papers
5. **√önete a comunidades**: Discord de Hugging Face, r/LocalLLaMA

---

_Recursos actualizados: Diciembre 2024_
