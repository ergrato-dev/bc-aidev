# ğŸ¤– IntroducciÃ³n a Large Language Models (LLMs)

## ğŸ¯ Objetivos de Aprendizaje

- Entender quÃ© son los LLMs y por quÃ© son revolucionarios
- Conocer la historia y evoluciÃ³n de los modelos de lenguaje
- Comprender el paradigma pre-training + fine-tuning
- Identificar casos de uso y limitaciones

---

## ğŸ“‹ Contenido

![Ecosistema de LLMs mostrando modelos propietarios, open-weights y herramientas](../0-assets/01-llm-landscape.svg)

### 1. Â¿QuÃ© son los LLMs?

Los **Large Language Models** son redes neuronales con miles de millones de parÃ¡metros, entrenadas en enormes cantidades de texto para comprender y generar lenguaje natural.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVOLUCIÃ“N DE LLMs                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2017: Transformer (Attention Is All You Need)              â”‚
â”‚  2018: GPT-1 (117M params) | BERT (340M params)             â”‚
â”‚  2019: GPT-2 (1.5B params)                                  â”‚
â”‚  2020: GPT-3 (175B params)                                  â”‚
â”‚  2022: ChatGPT | LLaMA                                      â”‚
â”‚  2023: GPT-4 | LLaMA 2 | Mistral                            â”‚
â”‚  2024: LLaMA 3 | Claude 3 | GPT-4o                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. CaracterÃ­sticas Clave

#### Escala Sin Precedentes

| Modelo | ParÃ¡metros | Tokens de Entrenamiento |
|--------|------------|-------------------------|
| BERT-base | 110M | 3.3B |
| GPT-2 | 1.5B | 40B |
| GPT-3 | 175B | 300B |
| LLaMA 2 | 7B - 70B | 2T |
| GPT-4 | ~1.7T (estimado) | 13T+ |

#### Capacidades Emergentes

A cierta escala, los modelos desarrollan capacidades que no fueron explÃ­citamente entrenadas:

- **Razonamiento**: Resolver problemas paso a paso
- **Few-shot learning**: Aprender de pocos ejemplos en el prompt
- **Code generation**: Escribir cÃ³digo funcional
- **TraducciÃ³n**: Sin entrenamiento especÃ­fico en traducciÃ³n

### 3. El Paradigma de Pre-training

Los LLMs siguen un proceso de dos fases:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASE 1: PRE-TRAINING                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Internet Text (TB)  â†’  [LLM Base]  â†’  Modelo Generalista   â”‚
â”‚   - Wikipedia                          - Conocimiento        â”‚
â”‚   - Libros                             - GramÃ¡tica           â”‚
â”‚   - CÃ³digo                             - Razonamiento        â”‚
â”‚   - Web crawl                          - MultilingÃ¼e         â”‚
â”‚                                                              â”‚
â”‚   Objetivo: Predecir el siguiente token                      â”‚
â”‚   Costo: $$$$ (millones de dÃ³lares)                          â”‚
â”‚   Tiempo: Semanas/meses en clusters de GPUs                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FASE 2: FINE-TUNING / ALIGNMENT              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Modelo Base  â†’  [Fine-tuning]  â†’  Modelo Especializado     â”‚
â”‚                                                              â”‚
â”‚   Opciones:                                                  â”‚
â”‚   1. Supervised Fine-tuning (SFT)                            â”‚
â”‚   2. RLHF (Reinforcement Learning from Human Feedback)       â”‚
â”‚   3. PEFT (Parameter-Efficient Fine-Tuning)                  â”‚
â”‚                                                              â”‚
â”‚   Costo: $ (horas de GPU)                                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Tipos de Modelos

#### Por Arquitectura

| Tipo | Arquitectura | Ejemplos | Uso Principal |
|------|--------------|----------|---------------|
| **Encoder-only** | BERT | BERT, RoBERTa, DeBERTa | ClasificaciÃ³n, NER |
| **Decoder-only** | GPT | GPT, LLaMA, Mistral | GeneraciÃ³n |
| **Encoder-Decoder** | T5 | T5, BART, Flan-T5 | Seq2Seq, traducciÃ³n |

#### Por Licencia

| CategorÃ­a | Ejemplos | CaracterÃ­sticas |
|-----------|----------|-----------------|
| **Propietarios** | GPT-4, Claude, Gemini | API de pago, mejor rendimiento |
| **Open-weights** | LLaMA 2/3, Mistral | Pesos pÃºblicos, restricciones de uso |
| **Open-source** | Falcon, BLOOM | Totalmente abiertos |

### 5. Capacidades y Limitaciones

#### âœ… Lo que los LLMs hacen bien

- **GeneraciÃ³n de texto** fluido y coherente
- **Seguir instrucciones** complejas
- **Resumir** y **reformular** contenido
- **TraducciÃ³n** entre idiomas
- **GeneraciÃ³n de cÃ³digo** en mÃºltiples lenguajes
- **Responder preguntas** basÃ¡ndose en contexto
- **Razonamiento** en cadena (Chain-of-Thought)

#### âŒ Limitaciones importantes

| LimitaciÃ³n | DescripciÃ³n | MitigaciÃ³n |
|------------|-------------|------------|
| **Alucinaciones** | Inventan informaciÃ³n falsa con confianza | RAG, verificaciÃ³n |
| **Conocimiento desactualizado** | No saben eventos recientes | RAG, fine-tuning |
| **MatemÃ¡ticas** | Errores en cÃ¡lculos complejos | Code interpreter |
| **Razonamiento lÃ³gico** | Fallan en problemas formales | CoT, verificaciÃ³n |
| **Contexto limitado** | No recuerdan conversaciones largas | Memoria externa |
| **Sesgos** | Reflejan sesgos de los datos | Alignment, filtros |

### 6. El Ecosistema de LLMs Abiertos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODELOS OPEN-WEIGHTS POPULARES                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Meta LLaMA 3                    Mistral AI                 â”‚
â”‚  â”œâ”€â”€ LLaMA 3-8B                  â”œâ”€â”€ Mistral-7B             â”‚
â”‚  â”œâ”€â”€ LLaMA 3-70B                 â”œâ”€â”€ Mixtral-8x7B (MoE)     â”‚
â”‚  â””â”€â”€ LLaMA 3.1-405B              â””â”€â”€ Mistral-Large          â”‚
â”‚                                                             â”‚
â”‚  Google                          Alibaba                    â”‚
â”‚  â”œâ”€â”€ Gemma-2B/7B                 â””â”€â”€ Qwen 2.5 (0.5B-72B)    â”‚
â”‚  â””â”€â”€ Flan-T5                                                â”‚
â”‚                                                             â”‚
â”‚  Microsoft                       01.AI                      â”‚
â”‚  â””â”€â”€ Phi-3 (3.8B)                â””â”€â”€ Yi (6B-34B)            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. CÃ³mo Usar LLMs

#### OpciÃ³n 1: APIs (MÃ¡s FÃ¡cil)

```python
# OpenAI API
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Explica quÃ© es un LLM"}]
)
```

#### OpciÃ³n 2: Hugging Face (Local)

```python
from transformers import pipeline

# Modelo pequeÃ±o que corre en CPU
generator = pipeline("text-generation", model="gpt2")
result = generator("The future of AI is", max_length=50)
```

#### OpciÃ³n 3: Ollama (Local, FÃ¡cil)

```bash
# Instalar modelo
ollama pull llama3

# Usar
ollama run llama3 "Explica quÃ© es machine learning"
```

### 8. Consideraciones Ã‰ticas

Los LLMs plantean importantes cuestiones Ã©ticas:

- **DesinformaciÃ³n**: Pueden generar fake news convincentes
- **Privacidad**: Pueden memorizar datos sensibles del entrenamiento
- **Sesgos**: Amplifican sesgos existentes en los datos
- **Impacto laboral**: Automatizan tareas de escritura
- **Medio ambiente**: El entrenamiento consume mucha energÃ­a

---

## ğŸ”‘ Conceptos Clave

| Concepto | DefiniciÃ³n |
|----------|------------|
| **LLM** | Modelo de lenguaje con miles de millones de parÃ¡metros |
| **Pre-training** | Entrenamiento inicial en grandes corpus de texto |
| **Fine-tuning** | AdaptaciÃ³n a tareas especÃ­ficas |
| **Emergent abilities** | Capacidades que aparecen solo a gran escala |
| **Hallucination** | GeneraciÃ³n de informaciÃ³n falsa pero plausible |
| **Context window** | Cantidad mÃ¡xima de tokens que el modelo puede procesar |

---

## ğŸ“Š Comparativa de Modelos Populares

| Modelo | TamaÃ±o | Contexto | Open | Ideal para |
|--------|--------|----------|------|------------|
| GPT-4o | ~1.7T | 128K | âŒ | ProducciÃ³n, calidad mÃ¡xima |
| Claude 3 | ? | 200K | âŒ | Documentos largos |
| LLaMA 3-8B | 8B | 8K | âœ… | Local, recursos limitados |
| Mistral-7B | 7B | 32K | âœ… | Balance rendimiento/costo |
| Phi-3-mini | 3.8B | 128K | âœ… | Edge, mÃ³viles |

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo la diferencia entre pre-training y fine-tuning
- [ ] Conozco las arquitecturas principales (encoder, decoder, encoder-decoder)
- [ ] SÃ© identificar capacidades y limitaciones de los LLMs
- [ ] Comprendo quÃ© son las alucinaciones y cÃ³mo mitigarlas
- [ ] Puedo elegir el modelo apropiado segÃºn el caso de uso

---

## ğŸ”— Recursos

- [State of GPT - Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
- [Hugging Face Model Hub](https://huggingface.co/models)
- [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Ollama](https://ollama.ai/)
