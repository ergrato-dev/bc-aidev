# ğŸ—ï¸ Arquitecturas: GPT vs BERT

## ğŸ¯ Objetivos de Aprendizaje

- Comprender la arquitectura Transformer en profundidad
- Diferenciar modelos autoregresivos (GPT) de bidireccionales (BERT)
- Entender los objetivos de pre-training de cada arquitectura
- Saber cuÃ¡ndo usar cada tipo de modelo

---

## ğŸ“‹ Contenido

![ComparaciÃ³n de arquitecturas GPT vs BERT mostrando diferencias en atenciÃ³n](../0-assets/02-gpt-architecture.svg)

### 1. Repaso del Transformer

El Transformer, introducido en 2017, es la base de todos los LLMs modernos.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFORMER ORIGINAL                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      ENCODER         â”‚           DECODER                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Self-Attention â”‚  â”‚  â”‚ Masked Self-   â”‚                  â”‚
â”‚  â”‚ (bidireccional)â”‚  â”‚  â”‚ Attention      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚          â†“           â”‚          â†“                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Feed Forward  â”‚  â”‚  â”‚Cross-Attention â”‚ â† Output Encoder â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚          â†“           â”‚          â†“                           â”‚
â”‚      (Ã—N capas)      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                      â”‚  â”‚  Feed Forward  â”‚                  â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                      â”‚          â†“                           â”‚
â”‚                      â”‚      (Ã—N capas)                      â”‚
â”‚                      â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                         â†‘
    Input (source)            Output (target)
```

### 2. Arquitectura GPT (Decoder-Only)

GPT usa **solo el Decoder** del Transformer, con atenciÃ³n **causal** (solo ve tokens anteriores).

#### Estructura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPT ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Input: "The cat sat on the"                               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚         Token Embeddings                â”‚               â”‚
â”‚   â”‚    + Positional Embeddings              â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚      Masked Self-Attention              â”‚  Ã—N layers    â”‚
â”‚   â”‚    (causal: solo ve pasado)             â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚         Feed Forward (MLP)              â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚    LM Head (vocab_size output)          â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   Output: Probabilidad para cada token del vocabulario      â”‚
â”‚           â†’ "mat" (siguiente token mÃ¡s probable)            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### AtenciÃ³n Causal

```python
# Ejemplo conceptual de mÃ¡scara causal
# El token en posiciÃ³n i solo puede atender a tokens 0..i

#           the  cat  sat  on  the
# the        âœ“    âœ—    âœ—    âœ—   âœ—
# cat        âœ“    âœ“    âœ—    âœ—   âœ—
# sat        âœ“    âœ“    âœ“    âœ—   âœ—
# on         âœ“    âœ“    âœ“    âœ“   âœ—
# the        âœ“    âœ“    âœ“    âœ“   âœ“
```

#### Objetivo de Pre-training: Causal Language Modeling

```python
# Predecir el siguiente token
input:  "The cat sat on the"
target: "cat sat on the mat"

# Loss: Cross-entropy entre predicciÃ³n y siguiente token real
```

#### CÃ³digo: Generar con GPT-2

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Cargar modelo y tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Tokenizar input
input_text = "The future of artificial intelligence is"
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# Generar
with torch.no_grad():
    output = model.generate(
        input_ids,
        max_length=50,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

# Decodificar
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)
```

### 3. Arquitectura BERT (Encoder-Only)

BERT usa **solo el Encoder**, con atenciÃ³n **bidireccional** (ve todo el contexto).

#### Estructura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BERT ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Input: "[CLS] The cat [MASK] on the mat [SEP]"            â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚         Token Embeddings                â”‚               â”‚
â”‚   â”‚    + Positional Embeddings              â”‚               â”‚
â”‚   â”‚    + Segment Embeddings                 â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚      Bidirectional Self-Attention       â”‚  Ã—N layers    â”‚
â”‚   â”‚       (ve todo el contexto)             â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚         Feed Forward (MLP)              â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â†“                                              â”‚
â”‚   Output: Representaciones contextualizadas                 â”‚
â”‚           [CLS] â†’ clasificaciÃ³n de secuencia                â”‚
â”‚           [MASK] â†’ predicciÃ³n de token: "sat"               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### AtenciÃ³n Bidireccional

```python
# Cada token puede atender a TODOS los demÃ¡s

#           [CLS] The  cat [MASK] on  the  mat [SEP]
# [CLS]       âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“
# The         âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“
# cat         âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“
# [MASK]      âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“    âœ“
# ...
```

#### Objetivos de Pre-training

**1. Masked Language Modeling (MLM)**
```python
# 15% de tokens se enmascaran
# El modelo debe predecirlos usando contexto bidireccional

input:  "The cat [MASK] on the mat"
target: "sat"
```

**2. Next Sentence Prediction (NSP)**
```python
# Predecir si sentence B sigue a sentence A

# Positivo (50%)
[CLS] The cat sat. [SEP] It was tired. [SEP] â†’ IsNext

# Negativo (50%)  
[CLS] The cat sat. [SEP] Paris is beautiful. [SEP] â†’ NotNext
```

#### CÃ³digo: ClasificaciÃ³n con BERT

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Cargar modelo fine-tuned para sentiment
model = BertForSequenceClassification.from_pretrained(
    'nlptown/bert-base-multilingual-uncased-sentiment'
)
tokenizer = BertTokenizer.from_pretrained(
    'nlptown/bert-base-multilingual-uncased-sentiment'
)

# Tokenizar
text = "This movie was absolutely fantastic!"
inputs = tokenizer(text, return_tensors='pt', truncation=True)

# Inferencia
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.softmax(outputs.logits, dim=-1)
    
# El modelo predice 1-5 estrellas
stars = predictions.argmax().item() + 1
print(f"Rating: {stars} estrellas")
```

### 4. Comparativa Detallada

| Aspecto | GPT (Decoder) | BERT (Encoder) |
|---------|---------------|----------------|
| **AtenciÃ³n** | Causal (unidireccional) | Bidireccional |
| **Pre-training** | Predecir siguiente token | Predecir tokens enmascarados |
| **Flujo** | Izquierda â†’ Derecha | Todo el contexto |
| **GeneraciÃ³n** | âœ… Excelente | âŒ No diseÃ±ado para esto |
| **ComprensiÃ³n** | Buena | âœ… Excelente |
| **ClasificaciÃ³n** | Posible (menos eficiente) | âœ… Ideal |
| **NER/Tagging** | Posible | âœ… Ideal |
| **QA Extractivo** | Posible | âœ… Ideal |
| **Resumen** | âœ… Ideal (generativo) | âŒ No adecuado |
| **TraducciÃ³n** | âœ… Posible | âŒ No adecuado |

### 5. Modelos Modernos de Cada Familia

#### Familia GPT (Decoder-only)

| Modelo | ParÃ¡metros | Contexto | CaracterÃ­sticas |
|--------|------------|----------|-----------------|
| GPT-2 | 1.5B | 1024 | Modelo base, open-source |
| GPT-3.5 | ~175B | 4K-16K | ChatGPT original |
| GPT-4 | ~1.7T | 128K | Multimodal |
| LLaMA 3 | 8B-70B | 8K | Open-weights, Meta |
| Mistral | 7B | 32K | Eficiente, open-weights |

#### Familia BERT (Encoder-only)

| Modelo | ParÃ¡metros | Mejoras sobre BERT |
|--------|------------|-------------------|
| RoBERTa | 355M | MÃ¡s datos, sin NSP |
| ALBERT | 12M-235M | FactorizaciÃ³n de parÃ¡metros |
| DeBERTa | 134M-1.5B | Disentangled attention |
| DistilBERT | 66M | DestilaciÃ³n, 60% mÃ¡s rÃ¡pido |

#### Familia T5 (Encoder-Decoder)

| Modelo | ParÃ¡metros | CaracterÃ­sticas |
|--------|------------|-----------------|
| T5 | 60M-11B | Text-to-text framework |
| BART | 139M-406M | Denoising autoencoder |
| Flan-T5 | 80M-11B | Instruction-tuned |

### 6. CuÃ¡ndo Usar Cada Uno

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GUÃA DE SELECCIÃ“N                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Â¿Necesitas GENERAR texto nuevo?                            â”‚
â”‚  â””â”€ SÃ â†’ GPT / LLaMA / Mistral                             â”‚
â”‚                                                             â”‚
â”‚  Â¿Necesitas CLASIFICAR o ETIQUETAR?                         â”‚
â”‚  â””â”€ SÃ â†’ BERT / RoBERTa / DeBERTa                          â”‚
â”‚                                                             â”‚
â”‚  Â¿Necesitas TRANSFORMAR texto (traducir, resumir)?          â”‚
â”‚  â””â”€ SÃ â†’ T5 / BART / Flan-T5                               â”‚
â”‚                                                             â”‚
â”‚  Â¿Necesitas un CHATBOT o ASISTENTE?                         â”‚
â”‚  â””â”€ SÃ â†’ GPT-4 / LLaMA + instruct / Mistral-Instruct       â”‚
â”‚                                                             â”‚
â”‚  Â¿Tienes RECURSOS LIMITADOS?                                â”‚
â”‚  â””â”€ SÃ â†’ DistilBERT / Phi-3 / Mistral-7B                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Ejemplo: Mismo Input, Diferente Uso

```python
text = "The movie was absolutely terrible and I hated every minute of it."

# Con BERT: ClasificaciÃ³n
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
result = classifier(text)
# â†’ {'label': 'NEGATIVE', 'score': 0.99}

# Con GPT: GeneraciÃ³n
generator = pipeline("text-generation", model="gpt2")
continuation = generator(text + " However,", max_length=50)
# â†’ "The movie was absolutely terrible... However, the cinematography was stunning..."
```

---

## ğŸ”‘ Conceptos Clave

| Concepto | GPT | BERT |
|----------|-----|------|
| **AtenciÃ³n** | Causal (masked) | Bidireccional |
| **Objetivo** | Next token prediction | Masked token prediction |
| **Token especial** | BOS, EOS | [CLS], [SEP], [MASK] |
| **Output** | Siguiente token | Representaciones |

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo la diferencia entre atenciÃ³n causal y bidireccional
- [ ] SÃ© cuÃ¡ndo usar modelos encoder vs decoder
- [ ] Comprendo MLM vs CLM como objetivos de pre-training
- [ ] Puedo elegir el modelo correcto para una tarea especÃ­fica
- [ ] Conozco modelos populares de cada familia

---

## ğŸ”— Recursos

- [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)
