# ğŸ”§ Fine-tuning y PEFT

## ğŸ¯ Objetivos de Aprendizaje

- Entender cuÃ¡ndo y por quÃ© hacer fine-tuning
- Dominar tÃ©cnicas de Parameter-Efficient Fine-Tuning (PEFT)
- Implementar LoRA y QLoRA para entrenamiento eficiente
- Preparar datasets para fine-tuning

---

## ğŸ“‹ Contenido

![ComparaciÃ³n de Full Fine-tuning vs LoRA mostrando eficiencia de parÃ¡metros](../0-assets/04-fine-tuning.svg)

### 1. Â¿QuÃ© es Fine-tuning?

Fine-tuning es el proceso de adaptar un modelo pre-entrenado a una tarea especÃ­fica usando datos etiquetados.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PROCESO DE FINE-TUNING                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Modelo Pre-entrenado                                      â”‚
â”‚   (conocimiento general)                                    â”‚
â”‚            â”‚                                                â”‚
â”‚            â–¼                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚    Fine-tuning      â”‚ â† Dataset especÃ­fico              â”‚
â”‚   â”‚  (ajustar pesos)    â”‚   (cientos/miles de ejemplos)     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚            â”‚                                                â”‚
â”‚            â–¼                                                â”‚
â”‚   Modelo Especializado                                      â”‚
â”‚   (experto en tu tarea)                                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Â¿CuÃ¡ndo Hacer Fine-tuning?

| SituaciÃ³n | RecomendaciÃ³n |
|-----------|---------------|
| Tarea genÃ©rica (resumen, QA) | âŒ Usa modelo pre-entrenado |
| Formato de salida especÃ­fico | ğŸ¤” Prueba prompt engineering primero |
| Dominio especializado (legal, mÃ©dico) | âœ… Fine-tuning recomendado |
| Datos propietarios/privados | âœ… Fine-tuning necesario |
| MÃ¡xima calidad en tarea especÃ­fica | âœ… Fine-tuning vale la pena |
| Reducir latencia/costo | âœ… Fine-tune modelo pequeÃ±o |

### 3. Tipos de Fine-tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TIPOS DE FINE-TUNING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   FULL FINE-TUNING                                          â”‚
â”‚   â””â”€â”€ Actualiza TODOS los parÃ¡metros                        â”‚
â”‚       âœ… Mejor rendimiento                                  â”‚
â”‚       âŒ Requiere mucha GPU/memoria                         â”‚
â”‚       âŒ Riesgo de overfitting                              â”‚
â”‚       âŒ Modelo completo por tarea                          â”‚
â”‚                                                             â”‚
â”‚   PARAMETER-EFFICIENT (PEFT)                                â”‚
â”‚   â””â”€â”€ Actualiza POCOS parÃ¡metros                            â”‚
â”‚       âœ… Requiere menos recursos                            â”‚
â”‚       âœ… Menos overfitting                                  â”‚
â”‚       âœ… Adaptadores pequeÃ±os                               â”‚
â”‚       ğŸ¤” Rendimiento cercano a full                         â”‚
â”‚                                                             â”‚
â”‚   TÃ©cnicas PEFT:                                            â”‚
â”‚   â”œâ”€â”€ LoRA (Low-Rank Adaptation)                            â”‚
â”‚   â”œâ”€â”€ QLoRA (Quantized LoRA)                                â”‚
â”‚   â”œâ”€â”€ Prefix Tuning                                         â”‚
â”‚   â”œâ”€â”€ Prompt Tuning                                         â”‚
â”‚   â””â”€â”€ Adapters                                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. LoRA (Low-Rank Adaptation)

LoRA congela el modelo original y entrena matrices de bajo rango que se suman a los pesos.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LORA EXPLAINED                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Peso original W (congelado):  d Ã— k matriz                â”‚
â”‚                                                             â”‚
â”‚   En lugar de actualizar W directamente:                    â”‚
â”‚                                                             â”‚
â”‚   W' = W + Î”W                                               â”‚
â”‚                                                             â”‚
â”‚   LoRA descompone Î”W en dos matrices pequeÃ±as:              â”‚
â”‚                                                             â”‚
â”‚   Î”W = B Ã— A                                                â”‚
â”‚                                                             â”‚
â”‚   Donde:                                                    â”‚
â”‚   - A: r Ã— k  (r << d, tÃ­picamente r=8,16,32)               â”‚
â”‚   - B: d Ã— r                                                â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   â”‚  W  â”‚  +  â”‚  B  Ã—  A    â”‚  =  W'                        â”‚
â”‚   â”‚ dÃ—k â”‚     â”‚ dÃ—r   rÃ—k   â”‚                               â”‚
â”‚   â”‚FROZENâ”‚    â”‚ TRAINABLE   â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                             â”‚
â”‚   ParÃ¡metros originales: d Ã— k = 768 Ã— 768 = 590K           â”‚
â”‚   ParÃ¡metros LoRA (r=8): 768Ã—8 + 8Ã—768 = 12.3K (2%)         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ImplementaciÃ³n con PEFT

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# Cargar modelo base
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Configurar LoRA
lora_config = LoraConfig(
    r=16,                           # Rank de las matrices
    lora_alpha=32,                  # Scaling factor
    target_modules=[                # QuÃ© mÃ³dulos adaptar
        "q_proj", 
        "v_proj",
        "k_proj",
        "o_proj",
    ],
    lora_dropout=0.05,              # RegularizaciÃ³n
    bias="none",                    # No entrenar biases
    task_type=TaskType.CAUSAL_LM    # Tipo de tarea
)

# Aplicar LoRA
model = get_peft_model(model, lora_config)

# Ver parÃ¡metros entrenables
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622
```

### 5. QLoRA (Quantized LoRA)

QLoRA combina cuantizaciÃ³n a 4 bits con LoRA para reducir aÃºn mÃ¡s los requisitos de memoria.

```python
from transformers import BitsAndBytesConfig

# ConfiguraciÃ³n de cuantizaciÃ³n 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                        # Cargar en 4 bits
    bnb_4bit_quant_type="nf4",               # Tipo de cuantizaciÃ³n
    bnb_4bit_compute_dtype=torch.bfloat16,   # Dtype para cÃ³mputo
    bnb_4bit_use_double_quant=True           # Doble cuantizaciÃ³n
)

# Cargar modelo cuantizado
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# Preparar para entrenamiento con gradientes
from peft import prepare_model_for_kbit_training
model = prepare_model_for_kbit_training(model)

# Aplicar LoRA normalmente
model = get_peft_model(model, lora_config)
```

### 6. PreparaciÃ³n de Datos

#### Formato de Dataset

```python
# Formato tÃ­pico para instruction tuning
dataset = [
    {
        "instruction": "Resume el siguiente texto",
        "input": "El machine learning es una rama de la IA...",
        "output": "El ML es IA que aprende de datos automÃ¡ticamente."
    },
    {
        "instruction": "Traduce al inglÃ©s",
        "input": "Hola, Â¿cÃ³mo estÃ¡s?",
        "output": "Hello, how are you?"
    }
]
```

#### Template de Prompt

```python
def format_prompt(example: dict) -> str:
    """Formatea un ejemplo para entrenamiento."""
    if example.get("input"):
        return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
    else:
        return f"""### Instruction:
{example['instruction']}

### Response:
{example['output']}"""


def tokenize_function(example, tokenizer, max_length=512):
    """Tokeniza un ejemplo."""
    prompt = format_prompt(example)
    
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=max_length,
        padding="max_length",
    )
    
    # Para causal LM, labels = input_ids
    result["labels"] = result["input_ids"].copy()
    
    return result
```

### 7. Training Loop

```python
from transformers import TrainingArguments, Trainer
from datasets import Dataset

# Crear dataset de Hugging Face
train_data = Dataset.from_list(training_examples)
train_data = train_data.map(
    lambda x: tokenize_function(x, tokenizer),
    remove_columns=train_data.column_names
)

# Configurar entrenamiento
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
)

# Crear trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    tokenizer=tokenizer,
)

# Entrenar
trainer.train()

# Guardar adaptadores LoRA (solo ~30MB)
model.save_pretrained("./my-lora-adapter")
```

### 8. Inferencia con Modelo Fine-tuned

```python
from peft import PeftModel

# Cargar modelo base
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Cargar adaptadores LoRA
model = PeftModel.from_pretrained(base_model, "./my-lora-adapter")

# Generar
model.eval()
prompt = "### Instruction:\nResume el siguiente texto\n\n### Input:\n..."

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 9. Supervised Fine-Tuning (SFT) Simplificado

Con la librerÃ­a `trl` de Hugging Face:

```python
from trl import SFTTrainer
from datasets import load_dataset

# Cargar dataset (ej: Alpaca)
dataset = load_dataset("tatsu-lab/alpaca", split="train")

# Configurar SFT Trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",  # Campo con el texto formateado
    max_seq_length=512,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
)

trainer.train()
```

### 10. Mejores PrÃ¡cticas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MEJORES PRÃCTICAS FINE-TUNING                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“Š DATOS                                                   â”‚
â”‚  â”œâ”€â”€ MÃ­nimo 100-1000 ejemplos de calidad                    â”‚
â”‚  â”œâ”€â”€ Diversidad en los ejemplos                             â”‚
â”‚  â”œâ”€â”€ Limpiar datos de ruido                                 â”‚
â”‚  â””â”€â”€ Validar formato consistente                            â”‚
â”‚                                                             â”‚
â”‚  âš™ï¸ CONFIGURACIÃ“N                                            â”‚
â”‚  â”œâ”€â”€ Empezar con r=8-16 en LoRA                             â”‚
â”‚  â”œâ”€â”€ Learning rate bajo: 1e-4 a 2e-4                        â”‚
â”‚  â”œâ”€â”€ Usar warmup (3-5% de steps)                            â”‚
â”‚  â””â”€â”€ Gradient accumulation si batch pequeÃ±o                 â”‚
â”‚                                                             â”‚
â”‚  ğŸ” EVALUACIÃ“N                                               â”‚
â”‚  â”œâ”€â”€ Guardar checkpoints frecuentes                         â”‚
â”‚  â”œâ”€â”€ Evaluar en conjunto de validaciÃ³n                      â”‚
â”‚  â”œâ”€â”€ Probar con ejemplos manuales                           â”‚
â”‚  â””â”€â”€ Monitorear overfitting                                 â”‚
â”‚                                                             â”‚
â”‚  ğŸ’¾ RECURSOS                                                 â”‚
â”‚  â”œâ”€â”€ 7B modelo: ~16GB VRAM con QLoRA                        â”‚
â”‚  â”œâ”€â”€ 13B modelo: ~24GB VRAM con QLoRA                       â”‚
â”‚  â””â”€â”€ Usar gradient checkpointing si falta memoria           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11. Comparativa de MÃ©todos

| MÃ©todo | ParÃ¡metros Entrenados | Memoria | Rendimiento |
|--------|----------------------|---------|-------------|
| Full Fine-tuning | 100% | Alto | â­â­â­â­â­ |
| LoRA (r=16) | ~0.1% | Medio | â­â­â­â­ |
| QLoRA (4-bit) | ~0.1% | Bajo | â­â­â­â­ |
| Prompt Tuning | ~0.01% | Muy bajo | â­â­â­ |

---

## ğŸ”‘ Conceptos Clave

| Concepto | DescripciÃ³n |
|----------|-------------|
| **Fine-tuning** | Adaptar modelo pre-entrenado a tarea especÃ­fica |
| **PEFT** | TÃ©cnicas que actualizan pocos parÃ¡metros |
| **LoRA** | Matrices de bajo rango sumadas a pesos originales |
| **QLoRA** | LoRA + cuantizaciÃ³n 4-bit |
| **Rank (r)** | DimensiÃ³n de matrices LoRA, controla capacidad |
| **SFT** | Supervised Fine-Tuning con pares input-output |

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Entiendo cuÃ¡ndo hacer fine-tuning vs usar prompts
- [ ] SÃ© configurar LoRA con parÃ¡metros apropiados
- [ ] Puedo preparar un dataset en el formato correcto
- [ ] Conozco las mejores prÃ¡cticas de entrenamiento
- [ ] SÃ© cargar y usar un modelo fine-tuned

---

## ğŸ”— Recursos

- [PEFT Documentation](https://huggingface.co/docs/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [TRL Library](https://huggingface.co/docs/trl)
- [Alpaca Dataset](https://github.com/tatsu-lab/stanford_alpaca)
