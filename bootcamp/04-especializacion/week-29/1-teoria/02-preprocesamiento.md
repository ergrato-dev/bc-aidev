# üßπ Preprocesamiento de Texto

![Preprocesamiento](../0-assets/02-preprocessing-pipeline.svg)

## üéØ Objetivos

- Implementar t√©cnicas de limpieza de texto
- Aplicar normalizaci√≥n y estandarizaci√≥n
- Crear pipelines de preprocesamiento reutilizables

---

## üìã ¬øPor qu√© Preprocesar?

El texto crudo contiene mucho "ruido" que dificulta el procesamiento:

```python
# Texto crudo de redes sociales
raw_text = """
@usuario123 Esto es INCRE√çBLE!!! üòçüòçüòç
Visita https://ejemplo.com para m√°s info...
#python #datascience #ia
"""

# Despu√©s de preprocesar
clean_text = "esto es incre√≠ble visita para m√°s info python datascience ia"
```

---

## üîß T√©cnicas de Preprocesamiento

### 1. Conversi√≥n a Min√∫sculas

```python
text = "Python es GENIAL para NLP"
text_lower = text.lower()
# "python es genial para nlp"
```

**Cu√°ndo NO usar:**
- NER (nombres propios importan)
- Acr√≥nimos con significado (USA, FBI)

### 2. Eliminaci√≥n de Caracteres Especiales

```python
import re

def remove_special_chars(text: str) -> str:
    """Elimina caracteres especiales manteniendo espacios."""
    # Mantener solo letras, n√∫meros y espacios
    return re.sub(r'[^a-z√°√©√≠√≥√∫√±√º\s]', '', text.lower())

text = "¬°Hola! ¬øC√≥mo est√°s? üòä"
clean = remove_special_chars(text)
# "hola c√≥mo est√°s"
```

### 3. Eliminaci√≥n de URLs y Menciones

```python
def remove_urls(text: str) -> str:
    """Elimina URLs del texto."""
    return re.sub(r'https?://\S+|www\.\S+', '', text)

def remove_mentions(text: str) -> str:
    """Elimina menciones de redes sociales."""
    return re.sub(r'@\w+', '', text)

def remove_hashtags(text: str) -> str:
    """Elimina hashtags."""
    return re.sub(r'#\w+', '', text)
```

### 4. Eliminaci√≥n de N√∫meros

```python
def remove_numbers(text: str) -> str:
    """Elimina n√∫meros del texto."""
    return re.sub(r'\d+', '', text)

# O reemplazar por token especial
def replace_numbers(text: str) -> str:
    """Reemplaza n√∫meros por <NUM>."""
    return re.sub(r'\d+', '<NUM>', text)
```

### 5. Eliminaci√≥n de Espacios Extra

```python
def normalize_whitespace(text: str) -> str:
    """Normaliza espacios en blanco."""
    return ' '.join(text.split())

text = "Hola    mundo   cruel"
clean = normalize_whitespace(text)
# "Hola mundo cruel"
```

---

## üìù Normalizaci√≥n de Texto

### Eliminaci√≥n de Acentos (Opcional)

```python
import unicodedata

def remove_accents(text: str) -> str:
    """Elimina acentos del texto."""
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join(c for c in nfkd if not unicodedata.combining(c))

text = "canci√≥n ma√±ana ni√±o"
clean = remove_accents(text)
# "cancion manana nino"
```

**Nota:** Perder acentos puede cambiar significado (a√±o vs ano).

### Lematizaci√≥n vs Stemming

**Stemming** - Cortar sufijos (r√°pido, impreciso)
```python
from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer('spanish')
words = ["corriendo", "corr√≠", "correr", "corremos"]
stems = [stemmer.stem(w) for w in words]
# ["corr", "corr", "corr", "corr"]
```

**Lematizaci√≥n** - Forma base real (lento, preciso)
```python
import spacy

nlp = spacy.load('es_core_news_sm')
doc = nlp("Los gatos est√°n corriendo")
lemmas = [token.lemma_ for token in doc]
# ["el", "gato", "estar", "correr"]
```

---

## üõë Stopwords

Palabras muy frecuentes con poco valor sem√°ntico.

```python
from nltk.corpus import stopwords

stop_words = set(stopwords.words('spanish'))
# {'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', ...}

def remove_stopwords(text: str) -> str:
    """Elimina stopwords del texto."""
    words = text.split()
    return ' '.join(w for w in words if w.lower() not in stop_words)

text = "el gato de la casa est√° en el jard√≠n"
clean = remove_stopwords(text)
# "gato casa est√° jard√≠n"
```

**Cu√°ndo mantener stopwords:**
- An√°lisis de sentimiento ("no me gusta" vs "me gusta")
- Modelos de lenguaje
- Cuando el contexto importa

---

## üîÑ Pipeline Completo

```python
import re
from typing import Callable

def create_preprocessing_pipeline(
    lowercase: bool = True,
    remove_urls: bool = True,
    remove_mentions: bool = True,
    remove_special: bool = True,
    remove_numbers: bool = False,
    remove_stopwords: bool = False
) -> Callable[[str], str]:
    """
    Crea un pipeline de preprocesamiento configurable.
    
    Returns:
        Funci√≥n que preprocesa texto
    """
    def preprocess(text: str) -> str:
        if remove_urls:
            text = re.sub(r'https?://\S+', '', text)
        
        if remove_mentions:
            text = re.sub(r'@\w+', '', text)
        
        if lowercase:
            text = text.lower()
        
        if remove_special:
            text = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º0-9\s]', '', text)
        
        if remove_numbers:
            text = re.sub(r'\d+', '', text)
        
        # Normalizar espacios
        text = ' '.join(text.split())
        
        return text
    
    return preprocess

# Uso
preprocess = create_preprocessing_pipeline(
    lowercase=True,
    remove_urls=True,
    remove_special=True
)

raw = "@user Esto es GENIAL! https://t.co/xxx üéâ"
clean = preprocess(raw)
# "esto es genial"
```

---

## ‚ö†Ô∏è Consideraciones

### Preservar Informaci√≥n √ötil

```python
# A veces el "ruido" es informaci√≥n
"üòçüòçüòç"  # ‚Üí Sentimiento muy positivo
"!!!"     # ‚Üí √ânfasis
"jajaja"  # ‚Üí Humor/sarcasmo
```

### Idioma-Espec√≠fico

```python
# Espa√±ol: √±, acentos
# Alem√°n: √ü, umlauts
# Chino: sin espacios entre palabras
```

### Dominio-Espec√≠fico

```python
# M√©dico: mantener t√©rminos t√©cnicos
# Legal: preservar formato espec√≠fico
# Social media: emojis pueden ser importantes
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Puedo implementar limpieza b√°sica de texto
- [ ] Entiendo la diferencia entre stemming y lematizaci√≥n
- [ ] S√© cu√°ndo usar o no usar stopwords
- [ ] Puedo crear pipelines de preprocesamiento configurables

---

_Siguiente: [Tokenizaci√≥n](03-tokenizacion.md)_
