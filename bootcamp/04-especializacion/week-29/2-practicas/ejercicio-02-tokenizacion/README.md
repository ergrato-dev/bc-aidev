# âœ‚ï¸ Ejercicio 02: TokenizaciÃ³n

## ğŸ¯ Objetivo

Implementar diferentes estrategias de tokenizaciÃ³n y construir un vocabulario.

---

## ğŸ“‹ DescripciÃ³n

En este ejercicio aprenderÃ¡s a dividir texto en tokens usando diferentes estrategias, desde simples splits hasta tokenizadores de NLTK y spaCy.

---

## ğŸ”§ Pasos del Ejercicio

### Paso 1: TokenizaciÃ³n Simple

La forma mÃ¡s bÃ¡sica es dividir por espacios:

```python
text = "Hola mundo cruel"
tokens = text.split()
# ["Hola", "mundo", "cruel"]
```

**Abre `starter/main.py`** y descomenta la secciÃ³n correspondiente.

### Paso 2: TokenizaciÃ³n con Regex

Para manejar mejor la puntuaciÃ³n:

```python
import re
text = "Hola, Â¿cÃ³mo estÃ¡s?"
tokens = re.findall(r'\b\w+\b', text)
# ["Hola", "cÃ³mo", "estÃ¡s"]
```

### Paso 3: TokenizaciÃ³n con NLTK

NLTK ofrece tokenizadores mÃ¡s sofisticados:

```python
from nltk.tokenize import word_tokenize
tokens = word_tokenize("Hola, Â¿cÃ³mo estÃ¡s?")
# ["Hola", ",", "Â¿", "cÃ³mo", "estÃ¡s", "?"]
```

### Paso 4: TokenizaciÃ³n por Oraciones

Dividir texto en oraciones:

```python
from nltk.tokenize import sent_tokenize
text = "Hola. Â¿CÃ³mo estÃ¡s? Bien, gracias."
sentences = sent_tokenize(text)
# ["Hola.", "Â¿CÃ³mo estÃ¡s?", "Bien, gracias."]
```

### Paso 5: Construir Vocabulario

Crear un mapeo de tokens a Ã­ndices:

```python
from collections import Counter

def build_vocab(texts, min_freq=1):
    all_tokens = []
    for text in texts:
        all_tokens.extend(text.lower().split())
    
    counts = Counter(all_tokens)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    
    for token, count in counts.most_common():
        if count >= min_freq:
            vocab[token] = len(vocab)
    
    return vocab
```

### Paso 6: Codificar y Decodificar

Convertir entre texto e Ã­ndices:

```python
def encode(text, vocab):
    tokens = text.lower().split()
    return [vocab.get(t, vocab['<UNK>']) for t in tokens]

def decode(ids, vocab):
    id_to_token = {v: k for k, v in vocab.items()}
    return [id_to_token.get(i, '<UNK>') for i in ids]
```

---

## ğŸ“ Estructura

```
ejercicio-02-tokenizacion/
â”œâ”€â”€ README.md
â””â”€â”€ starter/
    â””â”€â”€ main.py
```

---

## â–¶ï¸ EjecuciÃ³n

```bash
cd bootcamp/04-especializacion/week-29/2-practicas/ejercicio-02-tokenizacion
python starter/main.py
```

---

## âœ… Criterios de Ã‰xito

- [ ] TokenizaciÃ³n simple con split funciona
- [ ] TokenizaciÃ³n con regex maneja puntuaciÃ³n
- [ ] Vocabulario se construye correctamente
- [ ] Encode/decode funcionan sin errores
- [ ] Tokens OOV se mapean a `<UNK>`

---

## ğŸ”— Recursos

- [NLTK Tokenizers](https://www.nltk.org/api/nltk.tokenize.html)
- [spaCy Tokenization](https://spacy.io/usage/linguistic-features#tokenization)
