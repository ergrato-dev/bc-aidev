# üìù Semana 29: Fundamentos de NLP

## üéØ Objetivos de Aprendizaje

Al finalizar esta semana, ser√°s capaz de:

- ‚úÖ Comprender los fundamentos del Procesamiento de Lenguaje Natural
- ‚úÖ Implementar t√©cnicas de preprocesamiento de texto
- ‚úÖ Entender y aplicar diferentes estrategias de tokenizaci√≥n
- ‚úÖ Trabajar con word embeddings (Word2Vec, GloVe)
- ‚úÖ Crear representaciones vectoriales de texto

---

## üìö Requisitos Previos

- M√≥dulo 3: Deep Learning completado
- Conocimiento de redes neuronales
- Python y NumPy

---

## üóÇÔ∏è Estructura de la Semana

```
week-29/
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ rubrica-evaluacion.md        # Criterios de evaluaci√≥n
‚îú‚îÄ‚îÄ 0-assets/                    # Diagramas y recursos
‚îú‚îÄ‚îÄ 1-teoria/                    # Material te√≥rico
‚îÇ   ‚îú‚îÄ‚îÄ 01-introduccion-nlp.md
‚îÇ   ‚îú‚îÄ‚îÄ 02-preprocesamiento.md
‚îÇ   ‚îú‚îÄ‚îÄ 03-tokenizacion.md
‚îÇ   ‚îî‚îÄ‚îÄ 04-word-embeddings.md
‚îú‚îÄ‚îÄ 2-practicas/                 # Ejercicios guiados
‚îÇ   ‚îú‚îÄ‚îÄ ejercicio-01-preprocesamiento/
‚îÇ   ‚îú‚îÄ‚îÄ ejercicio-02-tokenizacion/
‚îÇ   ‚îî‚îÄ‚îÄ ejercicio-03-embeddings/
‚îú‚îÄ‚îÄ 3-proyecto/                  # Proyecto semanal
‚îÇ   ‚îî‚îÄ‚îÄ buscador-semantico/
‚îú‚îÄ‚îÄ 4-recursos/                  # Material adicional
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ 5-glosario/                  # T√©rminos clave
    ‚îî‚îÄ‚îÄ README.md
```

---

## üìù Contenidos

### üìñ Teor√≠a (1.5 horas)

| # | Tema | Archivo | Duraci√≥n |
|---|------|---------|----------|
| 1 | Introducci√≥n a NLP | [01-introduccion-nlp.md](1-teoria/01-introduccion-nlp.md) | 20 min |
| 2 | Preprocesamiento de Texto | [02-preprocesamiento.md](1-teoria/02-preprocesamiento.md) | 25 min |
| 3 | Tokenizaci√≥n | [03-tokenizacion.md](1-teoria/03-tokenizacion.md) | 25 min |
| 4 | Word Embeddings | [04-word-embeddings.md](1-teoria/04-word-embeddings.md) | 20 min |

### üíª Pr√°cticas (2.5 horas)

| # | Ejercicio | Carpeta | Duraci√≥n |
|---|-----------|---------|----------|
| 1 | Preprocesamiento de Texto | [ejercicio-01-preprocesamiento/](2-practicas/ejercicio-01-preprocesamiento/) | 45 min |
| 2 | T√©cnicas de Tokenizaci√≥n | [ejercicio-02-tokenizacion/](2-practicas/ejercicio-02-tokenizacion/) | 45 min |
| 3 | Word Embeddings | [ejercicio-03-embeddings/](2-practicas/ejercicio-03-embeddings/) | 60 min |

### üì¶ Proyecto (2 horas)

| Proyecto | Descripci√≥n | Carpeta |
|----------|-------------|---------|
| Buscador Sem√°ntico | Sistema de b√∫squeda usando similaridad de embeddings | [buscador-semantico/](3-proyecto/buscador-semantico/) |

---

## ‚è±Ô∏è Distribuci√≥n del Tiempo

```
Total: 6 horas

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìñ Teor√≠a      ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  1.5h (25%)  ‚îÇ
‚îÇ  üíª Pr√°cticas   ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  2.5h (42%)  ‚îÇ
‚îÇ  üì¶ Proyecto    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ  2.0h (33%)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Sugerencia de Planificaci√≥n

| D√≠a | Actividad | Tiempo |
|-----|-----------|--------|
| D√≠a 1 | Teor√≠a: Intro + Preprocesamiento | 45 min |
| D√≠a 2 | Teor√≠a: Tokenizaci√≥n + Embeddings | 45 min |
| D√≠a 3 | Pr√°ctica 1 y 2 | 1.5h |
| D√≠a 4 | Pr√°ctica 3 | 1h |
| D√≠a 5 | Proyecto | 2h |

---

## üìå Entregables

### Ejercicios Completados
- [ ] Ejercicio 1: Pipeline de preprocesamiento funcionando
- [ ] Ejercicio 2: Diferentes tokenizadores implementados
- [ ] Ejercicio 3: Embeddings cargados y operaciones vectoriales

### Proyecto Semanal
- [ ] Buscador sem√°ntico funcionando
- [ ] B√∫squeda por similaridad coseno
- [ ] Al menos 3 consultas de ejemplo
- [ ] C√≥digo documentado

---

## üéØ Competencias a Desarrollar

### T√©cnicas
- Preprocesamiento de texto (limpieza, normalizaci√≥n)
- Tokenizaci√≥n (palabra, subpalabra, car√°cter)
- Representaciones vectoriales de texto
- Similaridad sem√°ntica

### Conceptuales
- Entender el pipeline de NLP
- Diferencias entre representaciones sparse y dense
- Trade-offs de diferentes tokenizadores

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior | üè† M√≥dulo | Siguiente ‚û°Ô∏è |
|-------------|-----------|--------------|
| [Semana 28](../../03-deep-learning/week-28/README.md) | [Especializaci√≥n](../README.md) | [Semana 30](../week-30/README.md) |

---

## üí° Tips para esta Semana

> üéØ **Consejo**: Los embeddings son la base de todo NLP moderno. Dedica tiempo a entender intuitivamente qu√© representan y c√≥mo se usan.

### Conceptos Clave
- **Tokenizaci√≥n**: Dividir texto en unidades procesables
- **Embeddings**: Vectores densos que capturan significado sem√°ntico
- **Similaridad**: Medir qu√© tan relacionados est√°n dos textos

### Errores Comunes
- ‚ùå No normalizar el texto antes de tokenizar
- ‚ùå Ignorar el manejo de palabras fuera de vocabulario (OOV)
- ‚ùå No entender la diferencia entre embeddings est√°ticos y contextuales

---

## üìö Recursos R√°pidos

- [NLTK Documentation](https://www.nltk.org/)
- [spaCy Documentation](https://spacy.io/)
- [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)
- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)

---

_Semana 29 de 36 | M√≥dulo: Especializaci√≥n | Bootcamp IA: Zero to Hero_
