# üìñ Glosario - Semana 29: NLP Fundamentos

T√©rminos clave de Procesamiento de Lenguaje Natural ordenados alfab√©ticamente.

---

## B

### BPE (Byte Pair Encoding)
Algoritmo de tokenizaci√≥n por subpalabras que fusiona iterativamente los pares de caracteres m√°s frecuentes. Usado en GPT-2, GPT-3, RoBERTa.

```python
# Ejemplo conceptual
"lowest" ‚Üí ["low", "est"]
"newer"  ‚Üí ["new", "er"]
```

### Bag of Words (BoW)
Representaci√≥n de texto que ignora el orden de las palabras, contando solo su frecuencia.

```python
"el gato come" ‚Üí {"el": 1, "gato": 1, "come": 1}
```

---

## C

### Corpus
Colecci√≥n de textos utilizada para entrenar o evaluar modelos de NLP.

### Cosine Similarity (Similaridad Coseno)
Medida de similaridad entre dos vectores basada en el √°ngulo entre ellos.

$$\text{cos}(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

```python
# Valores: 1 = id√©nticos, 0 = ortogonales, -1 = opuestos
similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
```

### CBOW (Continuous Bag of Words)
Arquitectura de Word2Vec que predice una palabra dado su contexto.

---

## D

### Dense Vector (Vector Denso)
Vector donde la mayor√≠a de valores son no-cero. Los word embeddings son vectores densos.

```python
# Dense: [0.2, -0.5, 0.8, 0.1, -0.3, ...]
# vs Sparse: [0, 0, 1, 0, 0, 0, 0, ...]
```

---

## E

### Embedding
Representaci√≥n vectorial densa de baja dimensionalidad que captura propiedades sem√°nticas.

### Embedding Dimension
N√∫mero de valores en un vector de embedding. Com√∫n: 50, 100, 300 dimensiones.

---

## G

### GloVe (Global Vectors)
Algoritmo de word embeddings que combina estad√≠sticas de co-ocurrencia global con predicci√≥n local. Desarrollado por Stanford.

---

## L

### Lemmatization (Lematizaci√≥n)
Reducir palabras a su forma base (lema) usando conocimiento ling√º√≠stico.

```python
"corriendo" ‚Üí "correr"
"mejores"   ‚Üí "bueno"
```

---

## N

### N-gram
Secuencia contigua de n elementos (palabras o caracteres) de un texto.

```python
# Texto: "el gato come"
# Unigrams (n=1): ["el", "gato", "come"]
# Bigrams (n=2): ["el gato", "gato come"]
# Trigrams (n=3): ["el gato come"]
```

### NLP (Natural Language Processing)
Campo de la IA que estudia la interacci√≥n entre computadoras y lenguaje humano.

### Normalization (Normalizaci√≥n)
Proceso de estandarizar texto (min√∫sculas, eliminar acentos, etc.).

---

## O

### OOV (Out of Vocabulary)
Palabras que no est√°n en el vocabulario del modelo. Se mapean t√≠picamente a un token especial `<UNK>`.

### One-Hot Encoding
Representaci√≥n sparse donde cada palabra es un vector con un solo 1.

```python
vocab = ["gato", "perro", "casa"]
"gato"  ‚Üí [1, 0, 0]
"perro" ‚Üí [0, 1, 0]
"casa"  ‚Üí [0, 0, 1]
```

---

## P

### Preprocessing (Preprocesamiento)
Limpieza y normalizaci√≥n de texto antes del an√°lisis: min√∫sculas, eliminar puntuaci√≥n, etc.

### POS Tagging (Part-of-Speech)
Etiquetar cada palabra con su categor√≠a gramatical (sustantivo, verbo, adjetivo, etc.).

---

## S

### Semantic Similarity (Similaridad Sem√°ntica)
Medida de cu√°n similares son dos textos en significado, no solo en palabras.

### Skip-gram
Arquitectura de Word2Vec que predice palabras del contexto dada una palabra central.

### Sparse Vector (Vector Disperso)
Vector donde la mayor√≠a de valores son cero. One-hot encoding produce vectores sparse.

### Stemming
Reducir palabras a su ra√≠z eliminando sufijos, sin considerar el contexto.

```python
"corriendo" ‚Üí "corr"
"corredor"  ‚Üí "corr"
```

### Stopwords (Palabras Vac√≠as)
Palabras muy frecuentes con poco valor sem√°ntico (el, la, de, que, etc.).

```python
from nltk.corpus import stopwords
spanish_stops = stopwords.words('spanish')
# ["de", "la", "que", "el", "en", ...]
```

---

## T

### TF-IDF (Term Frequency-Inverse Document Frequency)
Medida que pondera la importancia de una palabra en un documento relativo a un corpus.

$$\text{TF-IDF} = \text{TF}(t,d) \times \log\frac{N}{\text{DF}(t)}$$

### Token
Unidad b√°sica de texto despu√©s de tokenizaci√≥n (palabra, subpalabra, o car√°cter).

### Tokenization (Tokenizaci√≥n)
Proceso de dividir texto en unidades m√°s peque√±as (tokens).

```python
"Hola mundo" ‚Üí ["Hola", "mundo"]
```

---

## V

### Vocabulary (Vocabulario)
Conjunto de todos los tokens √∫nicos conocidos por un modelo.

```python
vocab = {"<PAD>": 0, "<UNK>": 1, "gato": 2, "perro": 3}
```

---

## W

### Word2Vec
Familia de modelos para generar word embeddings. Incluye Skip-gram y CBOW. Desarrollado por Google (2013).

### Word Embedding
Representaci√≥n vectorial densa de una palabra que captura su significado sem√°ntico.

### WordPiece
Algoritmo de tokenizaci√≥n por subpalabras usado en BERT. Similar a BPE pero usa likelihood.

```python
"unbelievable" ‚Üí ["un", "##believ", "##able"]
```

---

## üìä Resumen de Dimensiones T√≠picas

| Modelo | Dimensiones | Vocabulario |
|--------|-------------|-------------|
| Word2Vec (small) | 100 | ~3M |
| GloVe (6B) | 50, 100, 200, 300 | 400K |
| FastText | 300 | 2M |
| BERT tokenizer | - | 30K tokens |

---

## üîó Referencias

- [Stanford NLP Glossary](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/glossary)
