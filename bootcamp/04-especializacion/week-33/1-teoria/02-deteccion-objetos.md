# üéØ Detecci√≥n de Objetos

![M√©trica IoU](../0-assets/03-iou-metric.svg)

## üéØ Objetivos de Aprendizaje

- Entender c√≥mo funcionan los detectores de objetos
- Conocer las arquitecturas principales (R-CNN family, YOLO, SSD)
- Comprender anchor boxes y NMS
- Implementar detecci√≥n b√°sica con modelos pre-entrenados

---

## üìã Contenido

### 1. ¬øQu√© es Detecci√≥n de Objetos?

La **detecci√≥n de objetos** localiza y clasifica m√∫ltiples objetos en una imagen simult√°neamente.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                OUTPUT DE DETECCI√ìN                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ   Para cada objeto detectado:                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Clase: "perro"                       ‚îÇ                   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Bounding Box: [100, 50, 300, 250]    ‚îÇ                   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Confianza: 0.95                      ‚îÇ                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Formato bbox: [x_min, y_min, x_max, y_max]                    ‚îÇ
‚îÇ   o: [x_center, y_center, width, height]                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Familias de Detectores

#### 2.1 Two-Stage Detectors (R-CNN Family)

**Proceso en dos etapas:**
1. Proponer regiones candidatas (RPN)
2. Clasificar y refinar cada regi√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    R-CNN EVOLUTION                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  R-CNN (2014)                                                   ‚îÇ
‚îÇ    ‚Ä¢ Selective Search ‚Üí 2000 regiones                           ‚îÇ
‚îÇ    ‚Ä¢ CNN por cada regi√≥n                                        ‚îÇ
‚îÇ    ‚Ä¢ Muy lento (~47s/imagen)                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Fast R-CNN (2015)                                              ‚îÇ
‚îÇ    ‚Ä¢ Una pasada CNN para toda la imagen                         ‚îÇ
‚îÇ    ‚Ä¢ RoI Pooling para extraer features                          ‚îÇ
‚îÇ    ‚Ä¢ ~2s/imagen                                                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Faster R-CNN (2015)                                            ‚îÇ
‚îÇ    ‚Ä¢ Region Proposal Network (RPN) integrado                    ‚îÇ
‚îÇ    ‚Ä¢ End-to-end trainable                                       ‚îÇ
‚îÇ    ‚Ä¢ ~0.2s/imagen                                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 2.2 One-Stage Detectors (YOLO, SSD)

**Proceso en una etapa:**
- Predicci√≥n directa de boxes y clases
- M√°s r√°pido, ideal para tiempo real

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ONE-STAGE vs TWO-STAGE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ   Two-Stage (Faster R-CNN):                                     ‚îÇ
‚îÇ   Imagen ‚Üí Features ‚Üí RPN ‚Üí Proposals ‚Üí Classify ‚Üí Detections   ‚îÇ
‚îÇ                                ‚Üì                                ‚îÇ
‚îÇ                         Mayor precisi√≥n                         ‚îÇ
‚îÇ                         M√°s lento                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   One-Stage (YOLO):                                             ‚îÇ
‚îÇ   Imagen ‚Üí Features ‚Üí Grid Predictions ‚Üí NMS ‚Üí Detections       ‚îÇ
‚îÇ                                ‚Üì                                ‚îÇ
‚îÇ                         M√°s r√°pido                              ‚îÇ
‚îÇ                         Tiempo real                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3. Conceptos Clave

#### 3.1 Anchor Boxes

Los **anchor boxes** son cajas predefinidas de diferentes tama√±os y aspect ratios que sirven como referencia.

```python
# Definici√≥n de anchors (ejemplo)
anchor_sizes = [32, 64, 128, 256, 512]  # Tama√±os en p√≠xeles
aspect_ratios = [0.5, 1.0, 2.0]          # Alto/Ancho

# Para cada punto del feature map:
# - Se generan len(sizes) * len(ratios) anchors
# - El modelo predice offsets respecto a los anchors

# Ejemplo de anchors para un punto:
# [32x64, 32x32, 64x32,    # size=32, ratios=[0.5, 1.0, 2.0]
#  64x128, 64x64, 128x64,  # size=64
#  ...]
```

#### 3.2 Non-Maximum Suppression (NMS)

**NMS** elimina detecciones redundantes manteniendo solo la mejor.

```python
import numpy as np

def nms(
    boxes: np.ndarray, 
    scores: np.ndarray, 
    iou_threshold: float = 0.5
) -> list[int]:
    """
    Non-Maximum Suppression.
    
    Args:
        boxes: Array de shape (N, 4) con [x1, y1, x2, y2]
        scores: Array de shape (N,) con confianzas
        iou_threshold: Umbral de IoU para suprimir
        
    Returns:
        √çndices de boxes a mantener
    """
    # Ordenar por score (descendente)
    order = scores.argsort()[::-1]
    
    keep = []
    
    while order.size > 0:
        # Tomar el de mayor score
        i = order[0]
        keep.append(i)
        
        if order.size == 1:
            break
        
        # Calcular IoU con el resto
        ious = calculate_iou_batch(boxes[i], boxes[order[1:]])
        
        # Mantener los que tienen IoU bajo
        mask = ious <= iou_threshold
        order = order[1:][mask]
    
    return keep

# Ejemplo:
# Detecciones: 5 boxes para "perro" con scores [0.9, 0.85, 0.7, 0.6, 0.5]
# Si boxes 0, 1, 2 se solapan mucho ‚Üí NMS mantiene solo box 0
```

#### 3.3 Feature Pyramid Network (FPN)

**FPN** permite detectar objetos de diferentes tama√±os combinando features de m√∫ltiples escalas.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FEATURE PYRAMID NETWORK                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ   Bottom-up        Top-down + Lateral                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ   ‚îÇ C5  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ P5  ‚îÇ ‚Üí Objetos grandes                     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ      ‚îÇ      ‚Üó        ‚îÇ                                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îÇ      ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ   ‚îÇ C4  ‚îÇ‚îÄ‚îÄ‚îò      ‚îÇ P4  ‚îÇ ‚Üí Objetos medianos                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ      ‚îÇ      ‚Üó        ‚îÇ                                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îÇ      ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ   ‚îÇ C3  ‚îÇ‚îÄ‚îÄ‚îò      ‚îÇ P3  ‚îÇ ‚Üí Objetos peque√±os                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4. Detecci√≥n con PyTorch

```python
import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# COCO classes
COCO_CLASSES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle',
    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench',
    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',
    'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',
    # ... m√°s clases
]

def load_model():
    """Carga Faster R-CNN pre-entrenado."""
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    model.eval()
    return model

def detect_objects(
    model, 
    image_path: str, 
    threshold: float = 0.5
) -> dict:
    """
    Detecta objetos en una imagen.
    
    Returns:
        Dict con boxes, labels, scores
    """
    # Cargar y transformar imagen
    image = Image.open(image_path).convert('RGB')
    transform = torchvision.transforms.ToTensor()
    image_tensor = transform(image).unsqueeze(0)
    
    # Inferencia
    with torch.no_grad():
        predictions = model(image_tensor)[0]
    
    # Filtrar por confianza
    mask = predictions['scores'] > threshold
    
    return {
        'boxes': predictions['boxes'][mask].numpy(),
        'labels': predictions['labels'][mask].numpy(),
        'scores': predictions['scores'][mask].numpy()
    }

def visualize_detections(
    image_path: str,
    detections: dict,
    class_names: list = COCO_CLASSES
):
    """Visualiza detecciones en la imagen."""
    image = Image.open(image_path)
    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(image)
    
    colors = plt.cm.hsv(
        np.linspace(0, 1, len(class_names))
    )
    
    for box, label, score in zip(
        detections['boxes'],
        detections['labels'],
        detections['scores']
    ):
        x1, y1, x2, y2 = box
        
        # Dibujar box
        rect = patches.Rectangle(
            (x1, y1), x2 - x1, y2 - y1,
            linewidth=2,
            edgecolor=colors[label],
            facecolor='none'
        )
        ax.add_patch(rect)
        
        # A√±adir label
        ax.text(
            x1, y1 - 5,
            f'{class_names[label]}: {score:.2f}',
            color='white',
            fontsize=10,
            bbox=dict(boxstyle='round', facecolor=colors[label])
        )
    
    ax.axis('off')
    plt.tight_layout()
    plt.show()
```

### 5. Formato de Anotaciones

#### COCO Format

```json
{
    "images": [
        {
            "id": 1,
            "file_name": "image1.jpg",
            "width": 640,
            "height": 480
        }
    ],
    "annotations": [
        {
            "id": 1,
            "image_id": 1,
            "category_id": 1,
            "bbox": [100, 50, 200, 150],
            "area": 30000,
            "iscrowd": 0
        }
    ],
    "categories": [
        {"id": 1, "name": "person"},
        {"id": 2, "name": "car"}
    ]
}
```

#### YOLO Format

```
# Archivo: image1.txt
# class_id x_center y_center width height (normalizado 0-1)
0 0.5 0.4 0.3 0.5
1 0.7 0.6 0.2 0.3
```

### 6. Entrenamiento B√°sico

```python
import torch
from torch.utils.data import DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn

def train_detector(
    model,
    train_loader: DataLoader,
    num_epochs: int = 10,
    lr: float = 0.005
):
    """Entrena un detector de objetos."""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(
        params, lr=lr, momentum=0.9, weight_decay=0.0005
    )
    
    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=3, gamma=0.1
    )
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        
        for images, targets in train_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} 
                      for t in targets]
            
            # Forward
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            
            # Backward
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            
            epoch_loss += losses.item()
        
        scheduler.step()
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')
    
    return model
```

### 7. Evaluaci√≥n con mAP

```python
from torchvision.ops import box_iou

def evaluate_detections(
    predictions: list[dict],
    ground_truths: list[dict],
    iou_threshold: float = 0.5
) -> dict:
    """
    Eval√∫a detecciones contra ground truth.
    
    Returns:
        Dict con precision, recall, AP por clase
    """
    all_detections = []
    all_gt = []
    
    for pred, gt in zip(predictions, ground_truths):
        # Calcular IoU entre predicciones y GT
        if len(pred['boxes']) > 0 and len(gt['boxes']) > 0:
            ious = box_iou(
                torch.tensor(pred['boxes']),
                torch.tensor(gt['boxes'])
            )
            
            # Asignar detecciones a GT
            # ... l√≥gica de matching
    
    # Calcular AP
    # ... curva precision-recall
    
    return {
        'mAP': 0.0,  # Calcular
        'precision': 0.0,
        'recall': 0.0
    }
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo la diferencia entre two-stage y one-stage detectors
- [ ] Comprendo qu√© son anchor boxes y para qu√© sirven
- [ ] S√© aplicar NMS para eliminar detecciones redundantes
- [ ] Puedo usar un detector pre-entrenado para inferencia

---

## üîó Navegaci√≥n

| ‚¨ÖÔ∏è Anterior | üè† Inicio | Siguiente ‚û°Ô∏è |
|-------------|-----------|--------------|
| [01-introduccion-cv](01-introduccion-cv.md) | [README](../README.md) | [03-yolo-ultralytics](03-yolo-ultralytics.md) |
