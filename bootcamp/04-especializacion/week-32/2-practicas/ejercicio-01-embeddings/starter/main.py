"""
Ejercicio 01: Embeddings Sem√°nticos
===================================

Aprende a generar embeddings y calcular similitud sem√°ntica.

Instrucciones:
1. Lee cada secci√≥n y descomenta el c√≥digo
2. Ejecuta el script despu√©s de cada paso
3. Experimenta con diferentes textos
"""

import numpy as np

# ============================================
# PASO 1: Setup y Cargar Modelo
# ============================================
print("--- Paso 1: Setup ---")

# Descomenta las siguientes l√≠neas:
# from sentence_transformers import SentenceTransformer
# import warnings
# warnings.filterwarnings('ignore')
#
# # Cargar modelo de embeddings
# print('Cargando modelo all-MiniLM-L6-v2...')
# model = SentenceTransformer('all-MiniLM-L6-v2')
# print('‚úì Modelo cargado')
#
# # Generar un embedding simple
# text = "Python es un lenguaje de programaci√≥n"
# embedding = model.encode(text)
#
# print(f'\nTexto: "{text}"')
# print(f'Shape del embedding: {embedding.shape}')
# print(f'Tipo: {type(embedding)}')
# print(f'Primeros 5 valores: {embedding[:5]}')

print()


# ============================================
# PASO 2: Embeddings de M√∫ltiples Textos
# ============================================
print("--- Paso 2: M√∫ltiples Textos ---")

# Descomenta las siguientes l√≠neas:
# documents = [
#     "Python es ideal para data science",
#     "JavaScript domina el desarrollo web",
#     "SQL es esencial para bases de datos",
#     "Machine learning es una rama de la IA",
#     "Los gatos son mascotas populares"
# ]
#
# # Generar embeddings en batch (m√°s eficiente)
# embeddings = model.encode(documents, show_progress_bar=True)
#
# print(f'\nDocumentos: {len(documents)}')
# print(f'Shape de embeddings: {embeddings.shape}')
# # (5, 384) = 5 documentos, 384 dimensiones cada uno

print()


# ============================================
# PASO 3: Similitud Coseno
# ============================================
print("--- Paso 3: Similitud Coseno ---")

# Descomenta las siguientes l√≠neas:
# from numpy.linalg import norm
#
# def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
#     """
#     Calcula la similitud coseno entre dos vectores.
#
#     Valores:
#     - 1.0: Id√©nticos
#     - 0.0: Ortogonales (sin relaci√≥n)
#     - -1.0: Opuestos
#     """
#     return np.dot(a, b) / (norm(a) * norm(b))
#
# # Comparar documentos
# print('Comparando similitudes:')
#
# # Similar (ambos sobre programaci√≥n)
# sim_python_js = cosine_similarity(embeddings[0], embeddings[1])
# print(f'  Python ‚Üî JavaScript: {sim_python_js:.4f}')
#
# # Similar (data science relacionado con ML)
# sim_python_ml = cosine_similarity(embeddings[0], embeddings[3])
# print(f'  Python ‚Üî ML: {sim_python_ml:.4f}')
#
# # Diferente (programaci√≥n vs mascotas)
# sim_python_cats = cosine_similarity(embeddings[0], embeddings[4])
# print(f'  Python ‚Üî Gatos: {sim_python_cats:.4f}')

print()


# ============================================
# PASO 4: Matriz de Similitud
# ============================================
print("--- Paso 4: Matriz de Similitud ---")

# Descomenta las siguientes l√≠neas:
# def similarity_matrix(embeddings: np.ndarray) -> np.ndarray:
#     """Calcula matriz de similitud entre todos los embeddings."""
#     n = len(embeddings)
#     matrix = np.zeros((n, n))
#
#     for i in range(n):
#         for j in range(n):
#             matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])
#
#     return matrix
#
# # Calcular matriz
# sim_matrix = similarity_matrix(embeddings)
#
# print('Matriz de similitud:')
# print('     ', end='')
# for i in range(len(documents)):
#     print(f'D{i}    ', end='')
# print()
#
# for i, row in enumerate(sim_matrix):
#     print(f'D{i}  ', end='')
#     for val in row:
#         print(f'{val:.2f}  ', end='')
#     print()

print()


# ============================================
# PASO 5: B√∫squeda Sem√°ntica
# ============================================
print("--- Paso 5: B√∫squeda Sem√°ntica ---")

# Descomenta las siguientes l√≠neas:
# class SemanticSearch:
#     """Buscador sem√°ntico simple."""
#
#     def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
#         self.model = SentenceTransformer(model_name)
#         self.documents = []
#         self.embeddings = None
#
#     def index(self, documents: list[str]):
#         """Indexa documentos."""
#         self.documents = documents
#         self.embeddings = self.model.encode(documents)
#         print(f'‚úì Indexados {len(documents)} documentos')
#
#     def search(self, query: str, top_k: int = 3) -> list[tuple]:
#         """
#         Busca documentos similares a la query.
#
#         Returns:
#             Lista de (documento, score)
#         """
#         query_embedding = self.model.encode(query)
#
#         # Calcular similitudes
#         scores = []
#         for i, doc_emb in enumerate(self.embeddings):
#             score = cosine_similarity(query_embedding, doc_emb)
#             scores.append((self.documents[i], score))
#
#         # Ordenar por score (mayor primero)
#         scores.sort(key=lambda x: x[1], reverse=True)
#
#         return scores[:top_k]
#
# # Crear buscador
# searcher = SemanticSearch()
# searcher.index(documents)
#
# # Buscar
# query = "an√°lisis de datos"
# print(f'\nQuery: "{query}"')
# print('Resultados:')
#
# results = searcher.search(query, top_k=3)
# for doc, score in results:
#     print(f'  {score:.4f}: {doc}')

print()


# ============================================
# PASO 6: B√∫squeda con M√°s Documentos
# ============================================
print("--- Paso 6: Corpus Expandido ---")

# Descomenta las siguientes l√≠neas:
# # Corpus m√°s grande
# corpus = [
#     # Programaci√≥n
#     "Python es un lenguaje de programaci√≥n vers√°til",
#     "JavaScript permite crear aplicaciones web interactivas",
#     "Java es popular en desarrollo empresarial",
#     "C++ se usa para programaci√≥n de sistemas",
#     "Rust ofrece seguridad de memoria",
#
#     # Data Science
#     "Machine learning predice patrones en datos",
#     "Deep learning usa redes neuronales profundas",
#     "Pandas facilita el an√°lisis de datos en Python",
#     "NumPy es fundamental para c√°lculo num√©rico",
#     "Scikit-learn tiene algoritmos de ML",
#
#     # Bases de datos
#     "SQL es el lenguaje para bases de datos relacionales",
#     "MongoDB es una base de datos NoSQL",
#     "PostgreSQL es una BD relacional avanzada",
#     "Redis es una base de datos en memoria",
#
#     # Otros
#     "Los perros son las mascotas m√°s leales",
#     "El caf√© es una bebida estimulante",
#     "El f√∫tbol es el deporte m√°s popular del mundo"
# ]
#
# searcher2 = SemanticSearch()
# searcher2.index(corpus)
#
# # M√∫ltiples queries
# queries = [
#     "c√≥mo analizar datos",
#     "lenguaje para web",
#     "almacenar informaci√≥n",
#     "animales dom√©sticos"
# ]
#
# for query in queries:
#     print(f'\nüîç Query: "{query}"')
#     results = searcher2.search(query, top_k=2)
#     for doc, score in results:
#         print(f'   {score:.3f}: {doc[:50]}...')

print()


# ============================================
# PASO 7: Normalizaci√≥n de Embeddings
# ============================================
print("--- Paso 7: Normalizaci√≥n ---")

# Descomenta las siguientes l√≠neas:
# # Verificar si los embeddings est√°n normalizados
# embedding_test = model.encode("test")
# norma = np.linalg.norm(embedding_test)
# print(f'Norma del embedding: {norma:.4f}')
#
# if abs(norma - 1.0) < 0.01:
#     print('‚úì Embeddings normalizados (norma ‚âà 1)')
#     print('  ‚Üí Similitud coseno = producto punto')
# else:
#     print('Embeddings no normalizados')
#     print('  ‚Üí Usar cosine_similarity expl√≠cita')
#
# # Normalizar manualmente si es necesario
# def normalize(v: np.ndarray) -> np.ndarray:
#     """Normaliza un vector a norma 1."""
#     return v / np.linalg.norm(v)
#
# normalized = normalize(embedding_test)
# print(f'\nNorma despu√©s de normalizar: {np.linalg.norm(normalized):.4f}')

print()


# ============================================
# PASO 8: Diferentes Modelos
# ============================================
print("--- Paso 8: Comparar Modelos ---")

# Descomenta las siguientes l√≠neas:
# # Nota: Este paso es opcional, requiere descargar m√°s modelos
#
# models_info = {
#     'all-MiniLM-L6-v2': {'dim': 384, 'speed': 'R√°pido', 'quality': 'Buena'},
#     'all-mpnet-base-v2': {'dim': 768, 'speed': 'Medio', 'quality': 'Mejor'},
#     'paraphrase-MiniLM-L6-v2': {'dim': 384, 'speed': 'R√°pido', 'quality': 'Par√°frasis'},
# }
#
# print('Modelos de embedding populares:')
# print('-' * 50)
# for name, info in models_info.items():
#     print(f'{name}')
#     print(f'  Dimensiones: {info["dim"]}')
#     print(f'  Velocidad: {info["speed"]}')
#     print(f'  Calidad: {info["quality"]}')
#     print()
#
# # Para cargar otro modelo:
# # model_mpnet = SentenceTransformer('all-mpnet-base-v2')
# # embedding_mpnet = model_mpnet.encode("test")
# # print(f'all-mpnet-base-v2 shape: {embedding_mpnet.shape}')  # (768,)

print()


# ============================================
# PASO 9: Batch Processing Eficiente
# ============================================
print("--- Paso 9: Batch Processing ---")

# Descomenta las siguientes l√≠neas:
# import time
#
# # Crear corpus grande
# large_corpus = [f"Documento n√∫mero {i} sobre tema {i % 5}" for i in range(100)]
#
# # M√©todo lento: uno por uno
# start = time.time()
# slow_embeddings = [model.encode(doc) for doc in large_corpus]
# slow_time = time.time() - start
#
# # M√©todo r√°pido: batch
# start = time.time()
# fast_embeddings = model.encode(large_corpus, batch_size=32)
# fast_time = time.time() - start
#
# print(f'Tiempo uno por uno: {slow_time:.2f}s')
# print(f'Tiempo en batch: {fast_time:.2f}s')
# print(f'Speedup: {slow_time/fast_time:.1f}x')

print()


# ============================================
# PASO 10: Guardar y Cargar Embeddings
# ============================================
print("--- Paso 10: Persistencia ---")

# Descomenta las siguientes l√≠neas:
# # Los embeddings son costosos de calcular
# # Es buena pr√°ctica guardarlos para reutilizar
#
# # Guardar
# np.save('embeddings.npy', embeddings)
# print('‚úì Embeddings guardados en embeddings.npy')
#
# # Cargar
# loaded_embeddings = np.load('embeddings.npy')
# print(f'‚úì Embeddings cargados: {loaded_embeddings.shape}')
#
# # Verificar que son iguales
# are_equal = np.allclose(embeddings, loaded_embeddings)
# print(f'‚úì Verificaci√≥n: {"Iguales" if are_equal else "Diferentes"}')
#
# # Limpiar archivo de prueba
# import os
# os.remove('embeddings.npy')
# print('‚úì Archivo de prueba eliminado')

print()
print("=" * 50)
print("¬°Ejercicio completado!")
print("Ahora dominas embeddings sem√°nticos.")
