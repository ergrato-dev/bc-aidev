# üéØ Embeddings y Vectores

![Espacio de Embeddings](../0-assets/02-embeddings-space.svg)

## üéØ Objetivos de Aprendizaje

- Comprender qu√© son los embeddings sem√°nticos
- Entender c√≥mo funcionan los modelos de embedding
- Calcular similitud entre vectores
- Elegir el modelo de embedding adecuado

---

## üìã Contenido

### 1. ¬øQu√© son los Embeddings?

Los **embeddings** son representaciones vectoriales densas que capturan el significado sem√°ntico del texto.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DE TEXTO A VECTOR                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ   "El gato duerme"  ‚îÄ‚îÄ‚ñ∂  [0.23, -0.15, 0.67, ..., 0.42]        ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ Vector de 384-1536 dimensiones ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Textos similares ‚îÄ‚îÄ‚ñ∂ Vectores cercanos en el espacio          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   "El gato descansa" ‚âà "El gato duerme"                        ‚îÇ
‚îÇ   [0.21, -0.14, 0.65, ...]  ‚âà  [0.23, -0.15, 0.67, ...]        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Espacio Vectorial Sem√°ntico

Los embeddings organizan conceptos en un espacio donde:
- **Conceptos similares** ‚Üí vectores cercanos
- **Conceptos diferentes** ‚Üí vectores lejanos

```
                    Espacio 2D (simplificado)
                    
         animal ‚îÇ
                ‚îÇ    üê± gato
                ‚îÇ         üêï perro
                ‚îÇ
                ‚îÇ                    üöó carro
                ‚îÇ                         üöå bus
                ‚îÇ
         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                ‚îÇ                              veh√≠culo
                ‚îÇ
```

### 3. Modelos de Embedding Populares

| Modelo | Dimensiones | Contexto | Uso |
|--------|-------------|----------|-----|
| all-MiniLM-L6-v2 | 384 | 256 tokens | R√°pido, bueno para empezar |
| all-mpnet-base-v2 | 768 | 384 tokens | Balance calidad/velocidad |
| text-embedding-ada-002 | 1536 | 8191 tokens | OpenAI, alta calidad |
| text-embedding-3-small | 1536 | 8191 tokens | OpenAI, m√°s barato |
| bge-large-en-v1.5 | 1024 | 512 tokens | Open source, muy bueno |
| e5-large-v2 | 1024 | 512 tokens | Microsoft, excelente |

### 4. Generando Embeddings

```python
from sentence_transformers import SentenceTransformer

# Cargar modelo
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generar embedding de un texto
text = "Python es un lenguaje de programaci√≥n"
embedding = model.encode(text)

print(f"Shape: {embedding.shape}")  # (384,)
print(f"Tipo: {type(embedding)}")   # numpy.ndarray

# M√∫ltiples textos (m√°s eficiente)
texts = [
    "Python es un lenguaje de programaci√≥n",
    "JavaScript se usa para web",
    "Los gatos son mascotas populares"
]
embeddings = model.encode(texts)
print(f"Shape: {embeddings.shape}")  # (3, 384)
```

### 5. Similitud entre Vectores

#### Similitud Coseno

La m√©trica m√°s usada para comparar embeddings:

$$\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}$$

```python
import numpy as np
from numpy.linalg import norm

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Calcula similitud coseno entre dos vectores."""
    return np.dot(a, b) / (norm(a) * norm(b))

# Ejemplo
emb1 = model.encode("El perro corre en el parque")
emb2 = model.encode("El can juega en el jard√≠n")
emb3 = model.encode("La econom√≠a global crece")

print(f"Similar: {cosine_similarity(emb1, emb2):.3f}")  # ~0.75
print(f"Diferente: {cosine_similarity(emb1, emb3):.3f}")  # ~0.15
```

#### Distancia Euclidiana

Alternativa a similitud coseno:

$$\text{euclidean\_distance}(A, B) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}$$

```python
def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:
    """Calcula distancia euclidiana."""
    return np.sqrt(np.sum((a - b) ** 2))

# Menor distancia = m√°s similar
dist = euclidean_distance(emb1, emb2)
```

### 6. B√∫squeda Sem√°ntica

```python
class SemanticSearch:
    """B√∫squeda sem√°ntica simple."""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None
    
    def add_documents(self, documents: list[str]):
        """A√±ade documentos al √≠ndice."""
        self.documents = documents
        self.embeddings = self.model.encode(documents)
    
    def search(self, query: str, top_k: int = 3) -> list[tuple]:
        """Busca documentos similares a la query."""
        query_embedding = self.model.encode(query)
        
        # Calcular similitudes
        similarities = []
        for i, doc_emb in enumerate(self.embeddings):
            sim = cosine_similarity(query_embedding, doc_emb)
            similarities.append((i, sim, self.documents[i]))
        
        # Ordenar por similitud (mayor primero)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]


# Uso
searcher = SemanticSearch()
searcher.add_documents([
    "Python es ideal para data science",
    "JavaScript domina el desarrollo web",
    "SQL es esencial para bases de datos",
    "Docker facilita el despliegue de aplicaciones"
])

results = searcher.search("an√°lisis de datos", top_k=2)
for idx, score, doc in results:
    print(f"{score:.3f}: {doc}")
# 0.534: Python es ideal para data science
# 0.312: SQL es esencial para bases de datos
```

### 7. Consideraciones Importantes

#### Normalizaci√≥n

Muchos modelos retornan embeddings normalizados (norma = 1):

```python
# Verificar normalizaci√≥n
embedding = model.encode("test")
norma = np.linalg.norm(embedding)
print(f"Norma: {norma:.4f}")  # ~1.0 si normalizado
```

#### Batch Processing

Procesar en lotes es m√°s eficiente:

```python
# ‚ùå Lento
embeddings = [model.encode(doc) for doc in documents]

# ‚úÖ R√°pido
embeddings = model.encode(documents, batch_size=32, show_progress_bar=True)
```

#### Truncamiento

Los modelos tienen l√≠mite de tokens:

```python
# El modelo trunca autom√°ticamente textos largos
long_text = "..." * 10000
embedding = model.encode(long_text)  # Se trunca a max_seq_length
```

### 8. Embeddings para Queries vs Documentos

Algunos modelos usan prefijos diferentes:

```python
# Modelos E5 requieren prefijos
query = "query: ¬øQu√© es machine learning?"
document = "passage: Machine learning es una rama de la IA..."

# BGE tambi√©n usa instrucciones
query = "Represent this sentence for searching: ¬øQu√© es ML?"
```

---

## üîë Puntos Clave

1. **Embeddings** = representaci√≥n vectorial del significado
2. **Similitud coseno** es la m√©trica est√°ndar
3. **Batch processing** para eficiencia
4. Elegir modelo seg√∫n **calidad vs velocidad vs costo**

---

## üìö Recursos Adicionales

- [Sentence Transformers Docs](https://www.sbert.net/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Puedo generar embeddings con sentence-transformers
- [ ] Entiendo similitud coseno y distancia euclidiana
- [ ] S√© implementar b√∫squeda sem√°ntica b√°sica
- [ ] Conozco los modelos de embedding populares
