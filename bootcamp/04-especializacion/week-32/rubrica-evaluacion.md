# ğŸ“‹ RÃºbrica de EvaluaciÃ³n - Semana 32

## RAG - Retrieval Augmented Generation

---

## ğŸ“Š DistribuciÃ³n de PuntuaciÃ³n

| Componente | Peso | DescripciÃ³n |
|------------|------|-------------|
| ğŸ§  Conocimiento | 30% | ComprensiÃ³n teÃ³rica de RAG |
| ğŸ’ª DesempeÃ±o | 40% | Ejercicios prÃ¡cticos completados |
| ğŸ“¦ Producto | 30% | Proyecto Asistente de Documentos |

---

## ğŸ§  Conocimiento (30 puntos)

### Conceptos Evaluados

| Concepto | Puntos | Criterio |
|----------|--------|----------|
| Arquitectura RAG | 8 | Explica el flujo completo: retrieval â†’ augmentation â†’ generation |
| Embeddings | 7 | Comprende representaciÃ³n vectorial y similitud semÃ¡ntica |
| Vector Databases | 8 | Conoce operaciones: index, query, filter, update |
| OptimizaciÃ³n | 7 | Entiende chunking, reranking, hybrid search |

### Niveles de Logro

| Nivel | Puntos | DescripciÃ³n |
|-------|--------|-------------|
| Excelente | 27-30 | Explica RAG con profundidad y casos de uso |
| Bueno | 21-26 | Comprende todos los componentes |
| Suficiente | 15-20 | Entiende el flujo bÃ¡sico |
| Insuficiente | <15 | ConfusiÃ³n sobre conceptos clave |

---

## ğŸ’ª DesempeÃ±o (40 puntos)

### Ejercicio 1: Embeddings (12 puntos)

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| Generar embeddings | 4 | Usa sentence-transformers correctamente |
| Similitud coseno | 4 | Calcula similitud entre vectores |
| BÃºsqueda semÃ¡ntica | 4 | Encuentra documentos por query |

### Ejercicio 2: ChromaDB (14 puntos)

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| Crear colecciÃ³n | 3 | Configura ChromaDB correctamente |
| Insertar documentos | 4 | AÃ±ade docs con embeddings y metadata |
| Query semÃ¡ntico | 4 | Recupera documentos relevantes |
| Filtros metadata | 3 | Usa where clauses efectivamente |

### Ejercicio 3: Q&A Documents (14 puntos)

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| Cargar documentos | 3 | Procesa diferentes formatos |
| Chunking | 4 | Divide texto apropiadamente |
| Pipeline RAG | 4 | Integra retrieval + LLM |
| Respuestas coherentes | 3 | Genera respuestas basadas en contexto |

### Niveles de Logro

| Nivel | Puntos | DescripciÃ³n |
|-------|--------|-------------|
| Excelente | 36-40 | Todos los ejercicios completos y optimizados |
| Bueno | 28-35 | Funcionalidad correcta en todos |
| Suficiente | 20-27 | Ejercicios bÃ¡sicos funcionando |
| Insuficiente | <20 | Ejercicios incompletos o errÃ³neos |

---

## ğŸ“¦ Producto (30 puntos)

### Proyecto: Asistente de Documentos

#### Funcionalidad (15 puntos)

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| Carga de documentos | 4 | Soporta PDF, TXT, MD |
| IndexaciÃ³n | 4 | Embeddings + ChromaDB funcional |
| Q&A interactivo | 4 | Chat que responde preguntas |
| CitaciÃ³n de fuentes | 3 | Indica de dÃ³nde viene la info |

#### Calidad TÃ©cnica (10 puntos)

| Criterio | Puntos | DescripciÃ³n |
|----------|--------|-------------|
| CÃ³digo limpio | 3 | Organizado, legible, DRY |
| DocumentaciÃ³n | 3 | Docstrings, README claro |
| Manejo de errores | 2 | Try/except apropiado |
| Configurabilidad | 2 | ParÃ¡metros ajustables |

#### Bonus (5 puntos extra)

| Feature | Puntos | DescripciÃ³n |
|---------|--------|-------------|
| MÃºltiples colecciones | +1 | Separar docs por tema |
| Reranking | +2 | Implementar segundo paso de ranking |
| Persistencia | +1 | Guardar/cargar Ã­ndice |
| UI (Gradio/Streamlit) | +1 | Interfaz grÃ¡fica |

### Niveles de Logro

| Nivel | Puntos | DescripciÃ³n |
|-------|--------|-------------|
| Excelente | 27-30 | Sistema robusto con features adicionales |
| Bueno | 21-26 | Todas las funcionalidades core |
| Suficiente | 15-20 | Q&A bÃ¡sico funcionando |
| Insuficiente | <15 | Sistema no funcional |

---

## âœ… Checklist de VerificaciÃ³n

### Ejercicios

- [ ] Ejercicio 1: Embeddings calculados y comparados
- [ ] Ejercicio 2: ChromaDB operaciones CRUD funcionando
- [ ] Ejercicio 3: Pipeline Q&A genera respuestas

### Proyecto

- [ ] Carga al menos 3 documentos diferentes
- [ ] Responde preguntas sobre el contenido
- [ ] Indica la fuente de la informaciÃ³n
- [ ] CÃ³digo documentado con docstrings
- [ ] README con instrucciones de uso

### Conceptos

- [ ] Puedo explicar quÃ© problema resuelve RAG
- [ ] Entiendo cÃ³mo funcionan los embeddings semÃ¡nticos
- [ ] SÃ© las diferencias entre vector DBs
- [ ] Comprendo estrategias de chunking

---

## ğŸ“ Criterios de AprobaciÃ³n

| Requisito | MÃ­nimo |
|-----------|--------|
| Conocimiento | 15/30 (50%) |
| DesempeÃ±o | 20/40 (50%) |
| Producto | 15/30 (50%) |
| **Total** | **50/100 (50%)** |

**Nota**: Se requiere mÃ­nimo 50% en cada componente para aprobar.

---

## ğŸ¯ RetroalimentaciÃ³n

### Fortalezas Comunes
- ComprensiÃ³n del flujo RAG
- Uso efectivo de ChromaDB
- IntegraciÃ³n con LLMs

### Ãreas de Mejora Frecuentes
- Chunking muy grande o muy pequeÃ±o
- No filtrar resultados irrelevantes
- Prompts sin contexto estructurado
- Falta de manejo de errores

---

_RÃºbrica Semana 32 - Bootcamp IA: Zero to Hero_
