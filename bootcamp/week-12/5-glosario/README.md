# Glosario Semana 12: SVM, KNN y Naive Bayes

## A

### Accuracy

Proporci√≥n de predicciones correctas sobre el total. `(TP + TN) / (TP + TN + FP + FN)`.

### Alpha (Œ±)

En Naive Bayes, par√°metro de suavizado Laplace que evita probabilidades cero.

## B

### Bayes, Teorema de

F√≥rmula para calcular probabilidades condicionales: `P(A|B) = P(B|A) √ó P(A) / P(B)`.

### Bias-Variance Tradeoff

Balance entre error por simplificaci√≥n (bias) y error por sensibilidad a datos (variance).

### BernoulliNB

Variante de Naive Bayes para features binarias (presencia/ausencia).

## C

### C (par√°metro SVM)

Par√°metro de regularizaci√≥n. C alto = menos errores permitidos, C bajo = margen m√°s amplio.

### Conditional Independence

Asunci√≥n de Naive Bayes: features son independientes dado la clase.

### Cross-Validation

T√©cnica de evaluaci√≥n que divide datos en k partes para entrenar y validar m√∫ltiples veces.

## D

### Decision Boundary

Frontera que separa clases en el espacio de features.

### Distance Metric

Funci√≥n que mide similitud entre puntos. Ejemplos: Euclidiana, Manhattan.

## E

### Euclidean Distance

Distancia en l√≠nea recta: `‚àöŒ£(xi - yi)¬≤`.

### Evidence

En Bayes, probabilidad total de los datos observados P(X).

## F

### F1-Score

Media arm√≥nica de precision y recall: `2 √ó (P √ó R) / (P + R)`.

### Feature Space

Espacio multidimensional donde cada dimensi√≥n es una feature.

## G

### Gamma (Œ≥)

En SVM con kernel RBF, controla el alcance de influencia de cada punto.

### GaussianNB

Naive Bayes para features continuas que asume distribuci√≥n normal.

### GridSearchCV

B√∫squeda exhaustiva de hiperpar√°metros con validaci√≥n cruzada.

## H

### Hard Margin

SVM que no permite errores de clasificaci√≥n. Solo funciona con datos linealmente separables.

### Hiperpar√°metro

Par√°metro que se define antes del entrenamiento (k en KNN, C en SVM).

### Hyperplane

Superficie de decisi√≥n en SVM que separa las clases.

## I

### Instance-based Learning

Aprendizaje que guarda instancias de entrenamiento (como KNN). Tambi√©n llamado lazy learning.

## K

### K (en KNN)

N√∫mero de vecinos a considerar para la predicci√≥n.

### Kernel

Funci√≥n que transforma datos a un espacio de mayor dimensi√≥n en SVM.

### Kernel Trick

T√©cnica que permite calcular productos en espacio transformado sin transformar expl√≠citamente.

### KNN (K-Nearest Neighbors)

Algoritmo que clasifica bas√°ndose en los k vecinos m√°s cercanos.

## L

### Laplace Smoothing

T√©cnica para evitar probabilidades cero a√±adiendo Œ± a los conteos.

### Lazy Learning

Algoritmos que no construyen modelo expl√≠cito, solo guardan datos (KNN).

### Likelihood

Probabilidad de observar los datos dada una clase: P(X|y).

### Linear Kernel

Kernel SVM para datos linealmente separables: `K(x,y) = x¬∑y`.

## M

### Manhattan Distance

Suma de diferencias absolutas: `Œ£|xi - yi|`.

### Margin

En SVM, distancia entre el hiperplano y los puntos m√°s cercanos.

### Maximum Margin Classifier

SVM busca el hiperplano que maximiza el margen.

### Minkowski Distance

Generalizaci√≥n de distancias: `(Œ£|xi - yi|^p)^(1/p)`.

### MultinomialNB

Naive Bayes para conteos/frecuencias, com√∫n en clasificaci√≥n de texto.

## N

### Naive Bayes

Clasificador probabil√≠stico basado en Teorema de Bayes con asunci√≥n de independencia.

### n_neighbors

Par√°metro de KNN que especifica el n√∫mero de vecinos (k).

### Normalization

Escalar features a un rango com√∫n. Esencial para KNN y SVM.

## O

### Overfitting

Modelo que memoriza datos de entrenamiento pero no generaliza.

## P

### Pipeline

En sklearn, secuencia de transformaciones seguida de un estimador.

### Polynomial Kernel

Kernel SVM: `K(x,y) = (Œ≥x¬∑y + r)^d`.

### Posterior

Probabilidad de clase despu√©s de observar datos: P(y|X).

### Precision

Proporci√≥n de predicciones positivas correctas: `TP / (TP + FP)`.

### Prior

Probabilidad a priori de una clase: P(y).

## R

### RBF Kernel (Radial Basis Function)

Kernel gaussiano: `K(x,y) = exp(-Œ≥||x-y||¬≤)`. El m√°s usado en SVM.

### Recall

Proporci√≥n de positivos reales detectados: `TP / (TP + FN)`.

### Regularization

T√©cnicas para prevenir overfitting (par√°metro C en SVM).

## S

### Soft Margin

SVM que permite algunos errores de clasificaci√≥n.

### StandardScaler

Normaliza features a media 0 y varianza 1. Esencial antes de KNN/SVM.

### Support Vectors

Puntos de entrenamiento m√°s cercanos al hiperplano que definen el margen.

### SVC

Support Vector Classification en sklearn.

### SVR

Support Vector Regression en sklearn.

## T

### TF-IDF

Term Frequency-Inverse Document Frequency. Vectorizaci√≥n de texto.

## U

### Underfitting

Modelo demasiado simple que no captura patrones.

## V

### Vectorizer

En sklearn, transforma texto a vectores num√©ricos (CountVectorizer, TfidfVectorizer).

### Voting (en KNN)

Mecanismo de decisi√≥n por mayor√≠a entre los k vecinos.

## W

### Weights (en KNN)

'uniform' = todos los vecinos pesan igual, 'distance' = m√°s cercanos pesan m√°s.

---

## üìê F√≥rmulas Clave

### Distancias

- **Euclidiana**: $d(x,y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$
- **Manhattan**: $d(x,y) = \sum_{i=1}^{n}|x_i - y_i|$
- **Minkowski**: $d(x,y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$

### Teorema de Bayes

$$P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}$$

### Margen SVM

$$\text{margen} = \frac{2}{||w||}$$

### Kernels

- **Linear**: $K(x,y) = x \cdot y$
- **RBF**: $K(x,y) = e^{-\gamma||x-y||^2}$
- **Polynomial**: $K(x,y) = (\gamma x \cdot y + r)^d$
