# Support Vector Machines (SVM)

## üéØ Objetivos

- Comprender el concepto de hiperplano y margen m√°ximo
- Entender los vectores de soporte
- Conocer los diferentes kernels y cu√°ndo usarlos
- Implementar SVM con scikit-learn

## üìã Contenido

### 1. ¬øQu√© es SVM?

Support Vector Machine es un algoritmo que busca el **hiperplano √≥ptimo** que separa las clases maximizando el **margen**.

![SVM Hiperplano](../0-assets/02-svm-hiperplano.svg)

### 2. Conceptos Fundamentales

#### Hiperplano

Frontera de decisi√≥n definida por:

$$w \cdot x + b = 0$$

Donde:

- $w$: vector de pesos (normal al hiperplano)
- $b$: bias (intercepto)
- $x$: punto a clasificar

#### Margen

Distancia entre el hiperplano y los puntos m√°s cercanos de cada clase.

$$\text{margen} = \frac{2}{||w||}$$

**Objetivo**: Maximizar el margen ‚Üí Minimizar $||w||$

#### Vectores de Soporte

Puntos m√°s cercanos al hiperplano que **definen** el margen. Son los √∫nicos puntos que importan para el modelo.

### 3. SVM Lineal

```python
from sklearn.svm import SVC, LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# SVC con kernel lineal
svm_linear = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='linear'))
])

# LinearSVC (m√°s r√°pido para grandes datasets)
linear_svc = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', LinearSVC())
])

svm_linear.fit(X_train, y_train)
y_pred = svm_linear.predict(X_test)
```

### 4. Soft Margin: Par√°metro C

En datos no perfectamente separables, permitimos algunos errores.

| C peque√±o              | C grande              |
| ---------------------- | --------------------- |
| Margen amplio          | Margen estrecho       |
| M√°s errores permitidos | Menos errores         |
| Mejor generalizaci√≥n   | Riesgo de overfitting |

```python
# C bajo: m√°s regularizaci√≥n
svm_soft = SVC(kernel='linear', C=0.1)

# C alto: menos errores de clasificaci√≥n
svm_hard = SVC(kernel='linear', C=10)
```

### 5. Kernels: El Truco del Kernel

Para datos **no linealmente separables**, transformamos el espacio.

![SVM Kernels](../0-assets/03-svm-kernels.svg)

#### Tipos de Kernel

| Kernel         | F√≥rmula                                | Uso                        |
| -------------- | -------------------------------------- | -------------------------- |
| **Linear**     | $K(x,y) = x \cdot y$                   | Datos separables, texto    |
| **RBF**        | $K(x,y) = e^{-\gamma\|x-y\|^2}$        | Default, uso general       |
| **Polynomial** | $K(x,y) = (\gamma x \cdot y + r)^d$    | Interacciones features     |
| **Sigmoid**    | $K(x,y) = \tanh(\gamma x \cdot y + r)$ | Similar a redes neuronales |

```python
# RBF (default) - el m√°s usado
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')

# Polynomial
svm_poly = SVC(kernel='poly', degree=3, C=1.0)

# Linear
svm_linear = SVC(kernel='linear', C=1.0)
```

### 6. Par√°metro Gamma (Œ≥)

Controla el alcance de influencia de cada punto de entrenamiento.

| Œ≥ peque√±o        | Œ≥ grande            |
| ---------------- | ------------------- |
| Alcance amplio   | Alcance limitado    |
| Fronteras suaves | Fronteras complejas |
| Underfitting     | Overfitting         |

```python
# Gamma bajo
svm_low_gamma = SVC(kernel='rbf', gamma=0.01)

# Gamma alto
svm_high_gamma = SVC(kernel='rbf', gamma=10)

# Auto-calculado (recomendado)
svm_auto = SVC(kernel='rbf', gamma='scale')  # 1 / (n_features * X.var())
```

### 7. GridSearch para Hiperpar√°metros

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': ['scale', 'auto', 0.01, 0.1, 1],
    'svm__kernel': ['rbf', 'linear', 'poly']
}

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC())
])

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score: {grid_search.best_score_:.4f}")
```

### 8. SVM para Regresi√≥n (SVR)

```python
from sklearn.svm import SVR

svr = Pipeline([
    ('scaler', StandardScaler()),
    ('svr', SVR(kernel='rbf', C=1.0, epsilon=0.1))
])

svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)
```

### 9. Probabilidades

Por defecto SVM no da probabilidades. Se puede activar con `probability=True`:

```python
svm_proba = SVC(kernel='rbf', probability=True)
svm_proba.fit(X_train, y_train)

# Probabilidades
proba = svm_proba.predict_proba(X_test)
```

‚ö†Ô∏è **Nota**: Activar probabilidades hace el entrenamiento m√°s lento.

### 10. Ventajas y Desventajas

#### ‚úÖ Ventajas

- Efectivo en alta dimensionalidad
- Funciona bien con pocos datos
- Vers√°til con diferentes kernels
- Robusto al overfitting (especialmente con margen)
- Solo depende de vectores de soporte

#### ‚ùå Desventajas

- Lento en datasets grandes (O(n¬≤) a O(n¬≥))
- Sensible a la escala (requiere normalizaci√≥n)
- Dif√≠cil de interpretar
- No da probabilidades naturalmente
- Selecci√≥n de kernel y par√°metros puede ser compleja

### 11. Ejemplo Completo

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix

# Cargar datos
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC())
])

# GridSearch
param_grid = {
    'svm__C': [0.1, 1, 10],
    'svm__gamma': ['scale', 0.01, 0.1],
    'svm__kernel': ['rbf', 'linear']
}

grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train, y_train)

print(f"Mejores par√°metros: {grid.best_params_}")
print(f"Mejor score CV: {grid.best_score_:.4f}")

# Evaluar
y_pred = grid.predict(X_test)
print(f"\nTest Accuracy: {grid.score(X_test, y_test):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))
```

---

## ‚úÖ Checklist de Verificaci√≥n

- [ ] Entiendo el concepto de hiperplano y margen m√°ximo
- [ ] Conozco qu√© son los vectores de soporte
- [ ] S√© cu√°ndo usar cada tipo de kernel
- [ ] Comprendo los par√°metros C y gamma
- [ ] Puedo implementar y tunear SVM con sklearn

---

## üìö Recursos

- [SVM - sklearn](https://scikit-learn.org/stable/modules/svm.html)
- [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
- [Kernel Functions](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)
